"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[630],{15680:(e,n,o)=>{o.d(n,{xA:()=>u,yg:()=>y});var t=o(96540);function i(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function r(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),o.push.apply(o,t)}return o}function a(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?r(Object(o),!0).forEach(function(n){i(e,n,o[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))})}return e}function l(e,n){if(null==e)return{};var o,t,i=function(e,n){if(null==e)return{};var o,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)o=r[t],n.indexOf(o)>=0||(i[o]=e[o]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)o=r[t],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(i[o]=e[o])}return i}var s=t.createContext({}),c=function(e){var n=t.useContext(s),o=n;return e&&(o="function"==typeof e?e(n):a(a({},n),e)),o},u=function(e){var n=c(e.components);return t.createElement(s.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},p=t.forwardRef(function(e,n){var o=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=c(o),p=i,y=m["".concat(s,".").concat(p)]||m[p]||d[p]||r;return o?t.createElement(y,a(a({ref:n},u),{},{components:o})):t.createElement(y,a({ref:n},u))});function y(e,n){var o=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=o.length,a=new Array(r);a[0]=p;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:i,a[1]=l;for(var c=2;c<r;c++)a[c]=o[c];return t.createElement.apply(null,a)}return t.createElement.apply(null,o)}p.displayName="MDXCreateElement"},40603:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var t=o(58168),i=(o(96540),o(15680));const r={id:"module-1-ros2",title:"The Robotic Nervous System (ROS 2)",sidebar_label:"Module 1: ROS 2"},a="The Robotic Nervous System (ROS 2)",l={unversionedId:"module-1-ros2",id:"module-1-ros2",title:"The Robotic Nervous System (ROS 2)",description:"Introduction to ROS 2",source:"@site/docs/module-1-ros2.mdx",sourceDirName:".",slug:"/module-1-ros2",permalink:"/ai-book/docs/module-1-ros2",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/module-1-ros2.mdx",tags:[],version:"current",frontMatter:{id:"module-1-ros2",title:"The Robotic Nervous System (ROS 2)",sidebar_label:"Module 1: ROS 2"},sidebar:"tutorialSidebar",previous:{title:"Exercise Solutions",permalink:"/ai-book/docs/resources/exercise-solutions"},next:{title:"Module 2: Digital Twin",permalink:"/ai-book/docs/module-2-digital-twin"}},s={},c=[{value:"Introduction to ROS 2",id:"introduction-to-ros-2",level:2},{value:"ROS 2 Nodes, Topics, and Services",id:"ros-2-nodes-topics-and-services",level:2},{value:"Nodes",id:"nodes",level:3},{value:"Topics",id:"topics",level:3},{value:"Services",id:"services",level:3},{value:"Bridging Python Agents to ROS Controllers using rclpy",id:"bridging-python-agents-to-ros-controllers-using-rclpy",level:2},{value:"Understanding URDF for Humanoid Robots",id:"understanding-urdf-for-humanoid-robots",level:2},{value:"ROS 2 Humanoid Architecture Overview",id:"ros-2-humanoid-architecture-overview",level:2},{value:"Hardware Abstraction Layer",id:"hardware-abstraction-layer",level:3},{value:"Perception Layer",id:"perception-layer",level:3},{value:"Planning Layer",id:"planning-layer",level:3},{value:"Behavior Layer",id:"behavior-layer",level:3}],u={toc:c},m="wrapper";function d({components:e,...n}){return(0,i.yg)(m,(0,t.A)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"the-robotic-nervous-system-ros-2"},"The Robotic Nervous System (ROS 2)"),(0,i.yg)("h2",{id:"introduction-to-ros-2"},"Introduction to ROS 2"),(0,i.yg)("p",null,"Robot Operating System 2 (ROS 2) serves as the foundational middleware for modern humanoid robots and AI agents. Unlike its predecessor, ROS 2 provides enhanced reliability, scalability, and real-time capabilities essential for safety-critical applications like humanoid robotics. Think of ROS 2 as the nervous system of your robot - connecting sensors, actuators, and computational nodes in a coordinated fashion."),(0,i.yg)("p",null,"ROS 2 introduces improved communication protocols, better security features, and support for real-time systems. It enables distributed computing across multiple machines, allowing humanoid robots to process sensor data, execute control algorithms, and coordinate complex behaviors seamlessly."),(0,i.yg)("h2",{id:"ros-2-nodes-topics-and-services"},"ROS 2 Nodes, Topics, and Services"),(0,i.yg)("h3",{id:"nodes"},"Nodes"),(0,i.yg)("p",null,"Nodes are the fundamental building blocks of ROS 2 applications. Each node typically handles a specific task - such as sensor processing, motion planning, or actuator control. In humanoid robots, you might have nodes for:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Joint controller nodes"),(0,i.yg)("li",{parentName:"ul"},"Sensor processing nodes (IMU, cameras, LiDAR)"),(0,i.yg)("li",{parentName:"ul"},"Perception nodes (object detection, SLAM)"),(0,i.yg)("li",{parentName:"ul"},"Behavior nodes (walking, grasping, speech)")),(0,i.yg)("h3",{id:"topics"},"Topics"),(0,i.yg)("p",null,"Topics enable asynchronous communication between nodes through a publish-subscribe model. This decoupled architecture allows nodes to operate independently while sharing information. Common topics in humanoid robots include:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/joint_states"),": Current positions, velocities, and efforts of all joints"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/sensor_msgs/LaserScan"),": LiDAR data for navigation"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/image_raw"),": Camera feeds for computer vision"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/cmd_vel"),": Velocity commands for locomotion")),(0,i.yg)("h3",{id:"services"},"Services"),(0,i.yg)("p",null,"Services provide synchronous request-response communication for operations that require confirmation or return specific results. Examples in humanoid robotics:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/get_joint_state"),": Request current joint positions"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/set_trajectory"),": Send trajectory commands to controllers"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/save_map"),": Save a map in SLAM operations")),(0,i.yg)("h2",{id:"bridging-python-agents-to-ros-controllers-using-rclpy"},"Bridging Python Agents to ROS Controllers using rclpy"),(0,i.yg)("p",null,"Python agents often serve as high-level decision makers in robotic systems, while ROS controllers handle low-level actuator commands. The ",(0,i.yg)("inlineCode",{parentName:"p"},"rclpy")," library enables seamless integration between Python-based AI agents and ROS 2 systems."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n\nclass AIAgentNode(Node):\n    def __init__(self):\n        super().__init__(\"ai_agent_node\")\n\n        # Subscribe to sensor data\n        self.sensor_subscription = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10)\n\n        # Publisher for high-level commands\n        self.command_publisher = self.create_publisher(\n            String,\n            '/ai_commands',\n            10)\n\n        # Publisher for trajectory commands\n        self.trajectory_publisher = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory_controller/joint_trajectory',\n            10)\n\n    def joint_state_callback(self, msg):\n        # Process joint states and make AI decisions\n        self.get_logger().info(f\"Received joint states: {len(msg.position)} joints\")\n\n    def send_trajectory_command(self, joint_names, positions, velocities=None):\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = positions\n        if velocities:\n            point.velocities = velocities\n\n        # Set execution time\n        point.time_from_start.sec = 2\n        point.time_from_start.nanosec = 0\n\n        traj_msg.points.append(point)\n        self.trajectory_publisher.publish(traj_msg)\n")),(0,i.yg)("p",null,"This bridge allows sophisticated Python-based AI algorithms to interact with real-time ROS 2 controllers, enabling complex behaviors in humanoid robots."),(0,i.yg)("h2",{id:"understanding-urdf-for-humanoid-robots"},"Understanding URDF for Humanoid Robots"),(0,i.yg)("p",null,"Unified Robot Description Format (URDF) defines the physical and kinematic properties of robots. For humanoid robots, URDF describes:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Link geometry and mass properties"),(0,i.yg)("li",{parentName:"ul"},"Joint types and limits"),(0,i.yg)("li",{parentName:"ul"},"Kinematic chains (legs, arms, torso, head)"),(0,i.yg)("li",{parentName:"ul"},"Sensor placements"),(0,i.yg)("li",{parentName:"ul"},"Collision models")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<robot name="humanoid_robot">\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="5.0"/>\n      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Hip joint and link --\x3e\n  <joint name="hip_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_leg"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1.0"/>\n  </joint>\n\n  <link name="left_leg">\n    <visual>\n      <geometry>\n        <capsule length="0.5" radius="0.05"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <capsule length="0.5" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="2.0"/>\n      <inertia ixx="0.05" ixy="0.0" ixz="0.0" iyy="0.05" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n</robot>\n')),(0,i.yg)("p",null,"URDF enables simulation environments, kinematic solvers, and motion planning algorithms to understand the robot's physical structure."),(0,i.yg)("h2",{id:"ros-2-humanoid-architecture-overview"},"ROS 2 Humanoid Architecture Overview"),(0,i.yg)("p",null,"A typical ROS 2 architecture for humanoid robots includes multiple layers:"),(0,i.yg)("h3",{id:"hardware-abstraction-layer"},"Hardware Abstraction Layer"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Real-time controllers for joint servos"),(0,i.yg)("li",{parentName:"ul"},"Sensor drivers for IMUs, cameras, LiDAR"),(0,i.yg)("li",{parentName:"ul"},"Actuator interfaces")),(0,i.yg)("h3",{id:"perception-layer"},"Perception Layer"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Computer vision nodes"),(0,i.yg)("li",{parentName:"ul"},"SLAM and localization"),(0,i.yg)("li",{parentName:"ul"},"Object recognition"),(0,i.yg)("li",{parentName:"ul"},"Environment mapping")),(0,i.yg)("h3",{id:"planning-layer"},"Planning Layer"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Motion planners"),(0,i.yg)("li",{parentName:"ul"},"Trajectory generators"),(0,i.yg)("li",{parentName:"ul"},"Path planning algorithms"),(0,i.yg)("li",{parentName:"ul"},"Balance controllers")),(0,i.yg)("h3",{id:"behavior-layer"},"Behavior Layer"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"High-level AI agents"),(0,i.yg)("li",{parentName:"ul"},"State machines"),(0,i.yg)("li",{parentName:"ul"},"Task planners"),(0,i.yg)("li",{parentName:"ul"},"Learning algorithms")),(0,i.yg)("figure",null,(0,i.yg)("img",{src:"/img/ros2-humanoid-architecture.png",alt:"ROS 2 Humanoid Architecture Diagram"}),(0,i.yg)("figcaption",null,"Diagram showing the layered architecture of ROS 2 for humanoid robots")),(0,i.yg)("p",null,"This architecture enables modular development and easy integration of new capabilities while maintaining system stability and performance."))}d.isMDXComponent=!0}}]);