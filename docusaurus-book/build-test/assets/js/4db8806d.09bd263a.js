"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[820],{15680:(e,n,t)=>{t.d(n,{xA:()=>m,yg:()=>g});var o=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=o.createContext({}),p=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},m=function(e){var n=p(e.components);return o.createElement(l.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},d=o.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=p(t),d=i,g=c["".concat(l,".").concat(d)]||c[d]||u[d]||r;return t?o.createElement(g,a(a({ref:n},m),{},{components:t})):o.createElement(g,a({ref:n},m))});function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,a=new Array(r);a[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:i,a[1]=s;for(var p=2;p<r;p++)a[p]=t[p];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},86989:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var o=t(58168),i=(t(96540),t(15680));const r={sidebar_position:7,title:"Tutorials and Hands-on Guides"},a="Tutorials and Hands-on Guides",s={unversionedId:"tutorials",id:"tutorials",title:"Tutorials and Hands-on Guides",description:"Introduction to Tutorials",source:"@site/docs/tutorials.md",sourceDirName:".",slug:"/tutorials",permalink:"/ai-book/docs/tutorials",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/tutorials.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7,title:"Tutorials and Hands-on Guides"}},l={},p=[{value:"Introduction to Tutorials",id:"introduction-to-tutorials",level:2},{value:"Setting Up Your Development Environment",id:"setting-up-your-development-environment",level:2},{value:"Required Software and Tools",id:"required-software-and-tools",level:3},{value:"Robot Operating System (ROS)",id:"robot-operating-system-ros",level:4},{value:"Simulation Environment",id:"simulation-environment",level:4},{value:"Python Development",id:"python-development",level:4},{value:"Hardware Setup (Optional)",id:"hardware-setup-optional",level:3},{value:"Tutorial 1: Basic Robot Simulation",id:"tutorial-1-basic-robot-simulation",level:2},{value:"Creating Your First Robot Model",id:"creating-your-first-robot-model",level:3},{value:"URDF Robot Model",id:"urdf-robot-model",level:4},{value:"Launch File",id:"launch-file",level:4},{value:"Running the Simulation",id:"running-the-simulation",level:3},{value:"Controlling the Robot",id:"controlling-the-robot",level:3},{value:"Tutorial 2: Sensor Integration and Perception",id:"tutorial-2-sensor-integration-and-perception",level:2},{value:"Adding Sensors to Your Robot",id:"adding-sensors-to-your-robot",level:3},{value:"Processing Sensor Data",id:"processing-sensor-data",level:3},{value:"Tutorial 3: AI Integration with Reinforcement Learning",id:"tutorial-3-ai-integration-with-reinforcement-learning",level:2},{value:"Setting up a Simple RL Environment",id:"setting-up-a-simple-rl-environment",level:3},{value:"Tutorial 4: Humanoid Robot Simulation with PyBullet",id:"tutorial-4-humanoid-robot-simulation-with-pybullet",level:2},{value:"Setting up PyBullet Environment",id:"setting-up-pybullet-environment",level:3},{value:"Tutorial 5: Integrating LLMs with Physical Robots",id:"tutorial-5-integrating-llms-with-physical-robots",level:2},{value:"Using OpenAI API for Robot Commands",id:"using-openai-api-for-robot-commands",level:3},{value:"Best Practices and Troubleshooting",id:"best-practices-and-troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Development Tips",id:"development-tips",level:3},{value:"Exercise: Build Your Own Embodied AI System",id:"exercise-build-your-own-embodied-ai-system",level:3}],m={toc:p},c="wrapper";function u({components:e,...n}){return(0,i.yg)(c,(0,o.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"tutorials-and-hands-on-guides"},"Tutorials and Hands-on Guides"),(0,i.yg)("h2",{id:"introduction-to-tutorials"},"Introduction to Tutorials"),(0,i.yg)("p",null,"This section provides practical, hands-on tutorials for building embodied AI systems. Each tutorial includes step-by-step instructions, code examples, and exercises to reinforce learning. The tutorials progress from basic concepts to advanced implementations."),(0,i.yg)("h2",{id:"setting-up-your-development-environment"},"Setting Up Your Development Environment"),(0,i.yg)("h3",{id:"required-software-and-tools"},"Required Software and Tools"),(0,i.yg)("h4",{id:"robot-operating-system-ros"},"Robot Operating System (ROS)"),(0,i.yg)("p",null,"ROS provides the framework for robot software development:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'# Install ROS Noetic (Ubuntu 20.04)\nsudo apt update\nsudo apt install ros-noetic-desktop-full\necho "source /opt/ros/noetic/setup.bash" >> ~/.bashrc\nsource ~/.bashrc\n\n# Install ROS dependencies\nsudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\nsudo rosdep init\nrosdep update\n')),(0,i.yg)("h4",{id:"simulation-environment"},"Simulation Environment"),(0,i.yg)("p",null,"Gazebo provides a physics-based simulation environment:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"sudo apt install ros-noetic-gazebo-ros-pkgs ros-noetic-gazebo-ros-control\n")),(0,i.yg)("h4",{id:"python-development"},"Python Development"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"# Install Python packages for robotics\npip install numpy scipy matplotlib\npip install torch torchvision  # For deep learning\npip install opencv-python     # For computer vision\npip install transforms3d      # For 3D transformations\n")),(0,i.yg)("h3",{id:"hardware-setup-optional"},"Hardware Setup (Optional)"),(0,i.yg)("p",null,"For physical robot tutorials, you may need:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Arduino/Raspberry Pi for basic robotics"),(0,i.yg)("li",{parentName:"ul"},"Robot kit (e.g., TurtleBot3, Poppy Ergo Jr)"),(0,i.yg)("li",{parentName:"ul"},"Sensors (cameras, IMU, distance sensors)")),(0,i.yg)("h2",{id:"tutorial-1-basic-robot-simulation"},"Tutorial 1: Basic Robot Simulation"),(0,i.yg)("h3",{id:"creating-your-first-robot-model"},"Creating Your First Robot Model"),(0,i.yg)("p",null,"Let's create a simple differential drive robot in Gazebo:"),(0,i.yg)("h4",{id:"urdf-robot-model"},"URDF Robot Model"),(0,i.yg)("p",null,"Create a URDF (Unified Robot Description Format) file to define your robot:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'\x3c!-- simple_robot.urdf --\x3e\n<?xml version="1.0"?>\n<robot name="simple_robot">\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.5 0.3 0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.5 0.3 0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Left Wheel --\x3e\n  <link name="left_wheel">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.2"/>\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Right Wheel --\x3e\n  <link name="right_wheel">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.2"/>\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Joints --\x3e\n  <joint name="left_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="left_wheel"/>\n    <origin xyz="-0.2 0.15 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  <joint name="right_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="right_wheel"/>\n    <origin xyz="-0.2 -0.15 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- Differential Drive Plugin --\x3e\n  <gazebo>\n    <plugin name="differential_drive_controller" filename="libgazebo_ros_diff_drive.so">\n      <leftJoint>left_wheel_joint</leftJoint>\n      <rightJoint>right_wheel_joint</rightJoint>\n      <wheelSeparation>0.3</wheelSeparation>\n      <wheelDiameter>0.2</wheelDiameter>\n      <commandTopic>cmd_vel</commandTopic>\n      <odometryTopic>odom</odometryTopic>\n      <odometryFrame>odom</odometryFrame>\n      <robotBaseFrame>base_link</robotBaseFrame>\n    </plugin>\n  </gazebo>\n</robot>\n')),(0,i.yg)("h4",{id:"launch-file"},"Launch File"),(0,i.yg)("p",null,"Create a launch file to spawn your robot in Gazebo:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'\x3c!-- launch/simple_robot.launch --\x3e\n<launch>\n  \x3c!-- Start Gazebo with empty world --\x3e\n  <include file="$(find gazebo_ros)/launch/empty_world.launch">\n    <arg name="paused" value="false"/>\n    <arg name="use_sim_time" value="true"/>\n    <arg name="gui" value="true"/>\n    <arg name="headless" value="false"/>\n    <arg name="debug" value="false"/>\n  </include>\n\n  \x3c!-- Load robot description --\x3e\n  <param name="robot_description" command="$(find xacro)/xacro --inorder \'$(find simple_robot)/urdf/simple_robot.urdf\'" />\n\n  \x3c!-- Spawn robot in Gazebo --\x3e\n  <node name="spawn_urdf" pkg="gazebo_ros" type="spawn_model"\n        args="-param robot_description -model simple_robot -x 0 -y 0 -z 0.1" />\n\n  \x3c!-- Robot state publisher --\x3e\n  <node name="robot_state_publisher" pkg="robot_state_publisher" type="robot_state_publisher" />\n</launch>\n')),(0,i.yg)("h3",{id:"running-the-simulation"},"Running the Simulation"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"# Create a catkin workspace\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws\ncatkin_make\nsource devel/setup.bash\n\n# Create your robot package\ncd src\ncatkin_create_pkg simple_robot std_msgs rospy roscpp\ncd ..\n\n# Copy the URDF file to simple_robot/urdf/\n# Copy the launch file to simple_robot/launch/\n\n# Build and run\ncatkin_make\nsource devel/setup.bash\nroslaunch simple_robot simple_robot.launch\n")),(0,i.yg)("h3",{id:"controlling-the-robot"},"Controlling the Robot"),(0,i.yg)("p",null,"Create a simple Python script to control your robot:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"#!/usr/bin/env python3\nimport rospy\nfrom geometry_msgs.msg import Twist\n\ndef move_robot():\n    rospy.init_node('robot_mover', anonymous=True)\n    velocity_publisher = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\n    rate = rospy.Rate(10)  # 10hz\n\n    vel_msg = Twist()\n\n    # Linear velocity in x\n    vel_msg.linear.x = 0.5\n    vel_msg.linear.y = 0\n    vel_msg.linear.z = 0\n\n    # Angular velocity in z\n    vel_msg.angular.x = 0\n    vel_msg.angular.y = 0\n    vel_msg.angular.z = 0.2\n\n    while not rospy.is_shutdown():\n        velocity_publisher.publish(vel_msg)\n        rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        move_robot()\n    except rospy.ROSInterruptException:\n        pass\n")),(0,i.yg)("h2",{id:"tutorial-2-sensor-integration-and-perception"},"Tutorial 2: Sensor Integration and Perception"),(0,i.yg)("h3",{id:"adding-sensors-to-your-robot"},"Adding Sensors to Your Robot"),(0,i.yg)("p",null,"Let's enhance our robot with sensors:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'\x3c!-- Enhanced robot with sensors --\x3e\n<robot name="sensor_robot">\n  \x3c!-- Base link with camera --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.5 0.3 0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.5 0.3 0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Camera sensor --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n    </inertial>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.2 0 0.05" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins for sensors --\x3e\n  <gazebo reference="camera_link">\n    <sensor type="camera" name="camera1">\n      <update_rate>30.0</update_rate>\n      <camera name="head">\n        <horizontal_fov>1.3962634</horizontal_fov>\n        <image>\n          <width>800</width>\n          <height>600</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.02</near>\n          <far>300</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <alwaysOn>true</alwaysOn>\n        <updateRate>0.0</updateRate>\n        <cameraName>simple_camera</cameraName>\n        <imageTopicName>image_raw</imageTopicName>\n        <cameraInfoTopicName>camera_info</cameraInfoTopicName>\n        <frameName>camera_link</frameName>\n        <hackBaseline>0.07</hackBaseline>\n        <distortion_k1>0.0</distortion_k1>\n        <distortion_k2>0.0</distortion_k2>\n        <distortion_k3>0.0</distortion_k3>\n        <distortion_t1>0.0</distortion_t1>\n        <distortion_t2>0.0</distortion_t2>\n      </plugin>\n    </sensor>\n  </gazebo>\n</robot>\n')),(0,i.yg)("h3",{id:"processing-sensor-data"},"Processing Sensor Data"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'#!/usr/bin/env python3\nimport rospy\nimport cv2\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass ImageProcessor:\n    def __init__(self):\n        rospy.init_node(\'image_processor\', anonymous=True)\n        self.bridge = CvBridge()\n        self.image_sub = rospy.Subscriber("/simple_camera/image_raw", Image, self.callback)\n        self.cv_image = None\n\n    def callback(self, data):\n        try:\n            self.cv_image = self.bridge.imgmsg_to_cv2(data, "bgr8")\n        except Exception as e:\n            rospy.logerr(e)\n\n    def detect_objects(self):\n        if self.cv_image is not None:\n            # Simple color-based object detection\n            hsv = cv2.cvtColor(self.cv_image, cv2.COLOR_BGR2HSV)\n\n            # Define range for red color\n            lower_red = np.array([0, 50, 50])\n            upper_red = np.array([10, 255, 255])\n\n            mask = cv2.inRange(hsv, lower_red, upper_red)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            # Draw contours on image\n            output = self.cv_image.copy()\n            cv2.drawContours(output, contours, -1, (0, 255, 0), 2)\n\n            # Display result\n            cv2.imshow("Processed Image", output)\n            cv2.waitKey(1)\n\ndef main():\n    processor = ImageProcessor()\n    rate = rospy.Rate(10)  # 10 Hz\n\n    while not rospy.is_shutdown():\n        processor.detect_objects()\n        rate.sleep()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,i.yg)("h2",{id:"tutorial-3-ai-integration-with-reinforcement-learning"},"Tutorial 3: AI Integration with Reinforcement Learning"),(0,i.yg)("h3",{id:"setting-up-a-simple-rl-environment"},"Setting up a Simple RL Environment"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"#!/usr/bin/env python3\nimport rospy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport random\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, action_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return torch.tanh(x)  # Actions between -1 and 1\n\nclass RobotEnvironment:\n    def __init__(self):\n        rospy.init_node('rl_robot', anonymous=True)\n        self.cmd_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\n        self.scan_sub = rospy.Subscriber('/scan', LaserScan, self.scan_callback)\n\n        self.scan_data = None\n        self.state_size = 10  # Simplified state representation\n        self.action_size = 2  # Linear and angular velocity\n\n        # Initialize policy network\n        self.policy = PolicyNetwork(self.state_size, self.action_size)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=0.001)\n\n        self.rate = rospy.Rate(10)  # 10 Hz\n\n    def scan_callback(self, data):\n        # Process laser scan data\n        self.scan_data = np.array(data.ranges)\n\n    def get_state(self):\n        if self.scan_data is None:\n            return np.zeros(self.state_size)\n\n        # Simplify the laser scan to fixed size state\n        # Take readings at regular intervals\n        step = len(self.scan_data) // self.state_size\n        simplified = [self.scan_data[i] if not np.isnan(self.scan_data[i]) else 10.0\n                     for i in range(0, len(self.scan_data), step)]\n\n        # Pad if necessary\n        if len(simplified) < self.state_size:\n            simplified.extend([10.0] * (self.state_size - len(simplified)))\n\n        return np.array(simplified[:self.state_size])\n\n    def select_action(self, state):\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            action = self.policy(state_tensor)\n        return action.numpy()[0]\n\n    def move_robot(self, action):\n        cmd = Twist()\n        cmd.linear.x = float(action[0]) * 0.5  # Scale linear velocity\n        cmd.angular.z = float(action[1]) * 0.5  # Scale angular velocity\n        self.cmd_pub.publish(cmd)\n\n    def run_episode(self):\n        for _ in range(100):  # 10 seconds at 10 Hz\n            state = self.get_state()\n            action = self.select_action(state)\n            self.move_robot(action)\n            self.rate.sleep()\n\ndef main():\n    env = RobotEnvironment()\n\n    # Run for multiple episodes\n    for episode in range(1000):\n        rospy.loginfo(f\"Episode {episode}\")\n        env.run_episode()\n\nif __name__ == '__main__':\n    main()\n")),(0,i.yg)("h2",{id:"tutorial-4-humanoid-robot-simulation-with-pybullet"},"Tutorial 4: Humanoid Robot Simulation with PyBullet"),(0,i.yg)("h3",{id:"setting-up-pybullet-environment"},"Setting up PyBullet Environment"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'#!/usr/bin/env python3\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport time\n\nclass SimpleHumanoid:\n    def __init__(self, urdf_path):\n        # Connect to physics server\n        p.connect(p.GUI)\n        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n\n        # Set gravity\n        p.setGravity(0, 0, -9.81)\n\n        # Load plane\n        p.loadURDF("plane.urdf")\n\n        # Load humanoid robot\n        self.robot_id = p.loadURDF(urdf_path, [0, 0, 1])\n\n        # Get joint information\n        self.joint_indices = []\n        for i in range(p.getNumJoints(self.robot_id)):\n            joint_info = p.getJointInfo(self.robot_id, i)\n            if joint_info[2] != p.JOINT_FIXED:  # Not a fixed joint\n                self.joint_indices.append(i)\n\n        # Set control parameters\n        self.max_force = 100\n\n    def move_to_position(self, joint_positions):\n        """Move all joints to specified positions"""\n        for i, joint_idx in enumerate(self.joint_indices):\n            p.setJointMotorControl2(\n                bodyIndex=self.robot_id,\n                jointIndex=joint_idx,\n                controlMode=p.POSITION_CONTROL,\n                targetPosition=joint_positions[i],\n                force=self.max_force\n            )\n\n    def get_robot_state(self):\n        """Get current joint positions and velocities"""\n        joint_states = p.getJointStates(self.robot_id, self.joint_indices)\n        positions = [state[0] for state in joint_states]\n        velocities = [state[1] for state in joint_states]\n        return positions, velocities\n\n    def run_simulation(self, steps=1000):\n        """Run the simulation for specified steps"""\n        for step in range(steps):\n            # Simple walking pattern\n            t = step / 10.0  # Time parameter\n\n            # Generate walking motion (simplified)\n            left_leg_pos = np.sin(t) * 0.2\n            right_leg_pos = np.sin(t + np.pi) * 0.2\n\n            # Set joint positions for walking\n            if len(self.joint_indices) >= 4:  # At least 4 joints for legs\n                # Left hip\n                p.setJointMotorControl2(\n                    bodyIndex=self.robot_id,\n                    jointIndex=self.joint_indices[0],\n                    controlMode=p.POSITION_CONTROL,\n                    targetPosition=left_leg_pos,\n                    force=self.max_force\n                )\n\n                # Right hip\n                p.setJointMotorControl2(\n                    bodyIndex=self.robot_id,\n                    jointIndex=self.joint_indices[1],\n                    controlMode=p.POSITION_CONTROL,\n                    targetPosition=right_leg_pos,\n                    force=self.max_force\n                )\n\n            p.stepSimulation()\n            time.sleep(1./240.)  # Real-time simulation\n\ndef main():\n    # Create a simple humanoid URDF or use an existing one\n    humanoid = SimpleHumanoid("path/to/humanoid.urdf")\n    humanoid.run_simulation()\n    p.disconnect()\n\nif __name__ == "__main__":\n    main()\n')),(0,i.yg)("h2",{id:"tutorial-5-integrating-llms-with-physical-robots"},"Tutorial 5: Integrating LLMs with Physical Robots"),(0,i.yg)("h3",{id:"using-openai-api-for-robot-commands"},"Using OpenAI API for Robot Commands"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'#!/usr/bin/env python3\nimport rospy\nfrom geometry_msgs.msg import Twist\nimport openai\nimport speech_recognition as sr\nimport pyttsx3\n\nclass LLMRobotController:\n    def __init__(self):\n        rospy.init_node(\'llm_robot_controller\', anonymous=True)\n        self.cmd_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n\n        # Initialize speech components\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.tts_engine = pyttsx3.init()\n\n        # Set OpenAI API key (use environment variable in production)\n        openai.api_key = "YOUR_API_KEY_HERE"\n\n        # Define robot actions\n        self.actions = {\n            \'move_forward\': self.move_forward,\n            \'move_backward\': self.move_backward,\n            \'turn_left\': self.turn_left,\n            \'turn_right\': self.turn_right,\n            \'stop\': self.stop_robot\n        }\n\n    def recognize_speech(self):\n        """Capture and recognize speech"""\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n            print("Listening...")\n            audio = self.recognizer.listen(source)\n\n        try:\n            text = self.recognizer.recognize_google(audio)\n            print(f"Recognized: {text}")\n            return text\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            print(f"Error with speech recognition service: {e}")\n            return None\n\n    def generate_response(self, command):\n        """Use LLM to interpret command and generate response"""\n        prompt = f"""\n        You are a helpful robot assistant. A user has given the following command: "{command}"\n\n        Available actions: move_forward, move_backward, turn_left, turn_right, stop\n\n        Respond with the action to take and a brief explanation.\n        Format: ACTION: [action_name], EXPLANATION: [explanation]\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                max_tokens=100\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f"Error with LLM: {e}")\n            return None\n\n    def parse_llm_response(self, response):\n        """Parse the LLM response to extract action"""\n        if not response:\n            return None\n\n        # Simple parsing - in practice, use more robust NLP\n        response_lower = response.lower()\n\n        if \'move_forward\' in response_lower or \'forward\' in response_lower:\n            return \'move_forward\'\n        elif \'move_backward\' in response_lower or \'backward\' in response_lower:\n            return \'move_backward\'\n        elif \'turn_left\' in response_lower or \'left\' in response_lower:\n            return \'turn_left\'\n        elif \'turn_right\' in response_lower or \'right\' in response_lower:\n            return \'turn_right\'\n        elif \'stop\' in response_lower:\n            return \'stop\'\n\n        return None\n\n    def move_forward(self):\n        cmd = Twist()\n        cmd.linear.x = 0.5\n        self.cmd_pub.publish(cmd)\n        self.speak("Moving forward")\n\n    def move_backward(self):\n        cmd = Twist()\n        cmd.linear.x = -0.5\n        self.cmd_pub.publish(cmd)\n        self.speak("Moving backward")\n\n    def turn_left(self):\n        cmd = Twist()\n        cmd.angular.z = 0.5\n        self.cmd_pub.publish(cmd)\n        self.speak("Turning left")\n\n    def turn_right(self):\n        cmd = Twist()\n        cmd.angular.z = -0.5\n        self.cmd_pub.publish(cmd)\n        self.speak("Turning right")\n\n    def stop_robot(self):\n        cmd = Twist()\n        cmd.linear.x = 0\n        cmd.angular.z = 0\n        self.cmd_pub.publish(cmd)\n        self.speak("Stopping")\n\n    def speak(self, text):\n        """Text-to-speech output"""\n        print(f"Robot says: {text}")\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n\n    def run(self):\n        """Main control loop"""\n        self.speak("Robot is ready for commands")\n\n        while not rospy.is_shutdown():\n            # Listen for command\n            command = self.recognize_speech()\n\n            if command:\n                # Get LLM interpretation\n                llm_response = self.generate_response(command)\n                print(f"LLM response: {llm_response}")\n\n                # Parse and execute action\n                action = self.parse_llm_response(llm_response)\n\n                if action and action in self.actions:\n                    self.actions[action]()\n                else:\n                    self.speak("I didn\'t understand that command")\n\n            rospy.sleep(1)\n\ndef main():\n    controller = LLMRobotController()\n    try:\n        controller.run()\n    except rospy.ROSInterruptException:\n        pass\n\nif __name__ == \'__main__\':\n    main()\n')),(0,i.yg)("h2",{id:"best-practices-and-troubleshooting"},"Best Practices and Troubleshooting"),(0,i.yg)("h3",{id:"common-issues-and-solutions"},"Common Issues and Solutions"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Simulation Instability")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Reduce time step in physics engine"),(0,i.yg)("li",{parentName:"ul"},"Check mass and inertia values"),(0,i.yg)("li",{parentName:"ul"},"Ensure proper joint limits"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Sensor Noise")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Apply filtering to sensor data"),(0,i.yg)("li",{parentName:"ul"},"Use multiple sensor fusion"),(0,i.yg)("li",{parentName:"ul"},"Implement outlier detection"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Control Oscillation")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Tune PID parameters"),(0,i.yg)("li",{parentName:"ul"},"Add damping to control system"),(0,i.yg)("li",{parentName:"ul"},"Implement smooth trajectory generation")))),(0,i.yg)("h3",{id:"development-tips"},"Development Tips"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Start Simple")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Begin with basic movements before complex behaviors"),(0,i.yg)("li",{parentName:"ul"},"Test in simulation before real hardware"),(0,i.yg)("li",{parentName:"ul"},"Implement one feature at a time"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Modular Design")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Separate perception, planning, and control"),(0,i.yg)("li",{parentName:"ul"},"Use ROS nodes for different functions"),(0,i.yg)("li",{parentName:"ul"},"Make components reusable"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Safety First")),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Implement emergency stop mechanisms"),(0,i.yg)("li",{parentName:"ul"},"Use simulation extensively"),(0,i.yg)("li",{parentName:"ul"},"Test thoroughly before deployment")))),(0,i.yg)("h3",{id:"exercise-build-your-own-embodied-ai-system"},"Exercise: Build Your Own Embodied AI System"),(0,i.yg)("p",null,"Now that you've completed the tutorials, try building your own embodied AI system:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Design Challenge"),": Create a robot that can navigate to a target while avoiding obstacles"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Implementation"),": Use the techniques learned to implement perception, planning, and control"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"AI Integration"),": Add machine learning for improved navigation"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Evaluation"),": Test in simulation and measure performance")),(0,i.yg)("p",null,"These tutorials provide a foundation for developing embodied AI systems. As you progress, consider more complex challenges like multi-robot coordination, learning from human demonstrations, or integrating advanced AI models for perception and decision-making."))}u.isMDXComponent=!0}}]);