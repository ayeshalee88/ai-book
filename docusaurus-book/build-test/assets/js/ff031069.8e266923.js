"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[603],{15680:(e,n,t)=>{t.d(n,{xA:()=>c,yg:()=>g});var o=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef(function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(t),u=i,g=d["".concat(s,".").concat(u)]||d[u]||m[u]||a;return t?o.createElement(g,r(r({ref:n},c),{},{components:t})):o.createElement(g,r({ref:n},c))});function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,r=new Array(a);r[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[d]="string"==typeof e?e:i,r[1]=l;for(var p=2;p<a;p++)r[p]=t[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},30241:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var o=t(58168),i=(t(96540),t(15680));const a={id:"exercise-solutions",title:"Exercise Solutions and Learning Resources",sidebar_label:"Exercise Solutions"},r="Exercise Solutions and Learning Resources",l={unversionedId:"resources/exercise-solutions",id:"resources/exercise-solutions",title:"Exercise Solutions and Learning Resources",description:"This guide provides solutions to exercises throughout the Physical AI & Humanoid Robotics book and additional learning resources to deepen your understanding.",source:"@site/docs/resources/exercise-solutions.md",sourceDirName:"resources",slug:"/resources/exercise-solutions",permalink:"/ai-book/docs/resources/exercise-solutions",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/resources/exercise-solutions.md",tags:[],version:"current",frontMatter:{id:"exercise-solutions",title:"Exercise Solutions and Learning Resources",sidebar_label:"Exercise Solutions"},sidebar:"tutorialSidebar",previous:{title:"Quickstart",permalink:"/ai-book/docs/quickstart"},next:{title:"Module 1: ROS 2",permalink:"/ai-book/docs/module-1-ros2"}},s={},p=[{value:"Chapter 1: Introduction to Embodied AI",id:"chapter-1-introduction-to-embodied-ai",level:2},{value:"Exercise: Understanding Sensorimotor Loops",id:"exercise-understanding-sensorimotor-loops",level:3},{value:"Chapter 2: Kinematics in Humanoid Robotics",id:"chapter-2-kinematics-in-humanoid-robotics",level:2},{value:"Exercise: 2-DOF Inverse Kinematics Solution",id:"exercise-2-dof-inverse-kinematics-solution",level:3},{value:"Chapter 3: Machine Learning for Locomotion",id:"chapter-3-machine-learning-for-locomotion",level:2},{value:"Exercise: Simple DDPG Implementation Solution",id:"exercise-simple-ddpg-implementation-solution",level:3},{value:"Chapter 4: Simulation Environments",id:"chapter-4-simulation-environments",level:2},{value:"Exercise: Simple PyBullet Robot Solution",id:"exercise-simple-pybullet-robot-solution",level:3},{value:"Additional Learning Resources",id:"additional-learning-resources",level:2},{value:"Books and Textbooks",id:"books-and-textbooks",level:3},{value:"Online Courses and Tutorials",id:"online-courses-and-tutorials",level:3},{value:"Software Libraries and Frameworks",id:"software-libraries-and-frameworks",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Communities and Forums",id:"communities-and-forums",level:3},{value:"Hardware Platforms for Practice",id:"hardware-platforms-for-practice",level:3},{value:"Problem Sets and Challenges",id:"problem-sets-and-challenges",level:2},{value:"Beginner Level",id:"beginner-level",level:3},{value:"Intermediate Level",id:"intermediate-level",level:3},{value:"Advanced Level",id:"advanced-level",level:3},{value:"Solutions to Common Implementation Challenges",id:"solutions-to-common-implementation-challenges",level:2},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:3},{value:"Real-time Control",id:"real-time-control",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"Assessment Rubrics",id:"assessment-rubrics",level:2},{value:"Implementation Projects",id:"implementation-projects",level:3},{value:"Theoretical Understanding",id:"theoretical-understanding",level:3},{value:"Recommended Study Path",id:"recommended-study-path",level:2},{value:"Month 1: Foundations",id:"month-1-foundations",level:3},{value:"Month 2: Control and Dynamics",id:"month-2-control-and-dynamics",level:3},{value:"Month 3: Perception and Learning",id:"month-3-perception-and-learning",level:3},{value:"Month 4: Integration and Application",id:"month-4-integration-and-application",level:3}],c={toc:p},d="wrapper";function m({components:e,...n}){return(0,i.yg)(d,(0,o.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"exercise-solutions-and-learning-resources"},"Exercise Solutions and Learning Resources"),(0,i.yg)("p",null,"This guide provides solutions to exercises throughout the Physical AI & Humanoid Robotics book and additional learning resources to deepen your understanding."),(0,i.yg)("h2",{id:"chapter-1-introduction-to-embodied-ai"},"Chapter 1: Introduction to Embodied AI"),(0,i.yg)("h3",{id:"exercise-understanding-sensorimotor-loops"},"Exercise: Understanding Sensorimotor Loops"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Problem"),": Create a simple simulation of a sensorimotor loop where a robot adjusts its movement based on sensor feedback."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Solution"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleSensorimotorLoop:\n    def __init__(self):\n        self.position = 0\n        self.velocity = 0\n        self.target = 10  # Target position\n        self.sensor_noise = 0.1\n        self.dt = 0.1\n\n    def sense(self):\n        """Sense the environment with some noise"""\n        true_distance = self.target - self.position\n        sensed_distance = true_distance + np.random.normal(0, self.sensor_noise)\n        return sensed_distance\n\n    def act(self, sensed_distance):\n        """Actuator response based on sensed input"""\n        # Simple proportional controller\n        k_p = 0.5\n        force = k_p * sensed_distance\n        return np.clip(force, -2, 2)  # Limit force\n\n    def update_dynamics(self, force):\n        """Update robot dynamics"""\n        # Simple mass-spring-damper model\n        acceleration = force - 0.1 * self.velocity  # Damping\n        self.velocity += acceleration * self.dt\n        self.position += self.velocity * self.dt\n\n    def run_simulation(self, steps=100):\n        """Run the sensorimotor loop simulation"""\n        positions = []\n        velocities = []\n        forces = []\n\n        for step in range(steps):\n            sensed_distance = self.sense()\n            force = self.act(sensed_distance)\n            self.update_dynamics(force)\n\n            positions.append(self.position)\n            velocities.append(self.velocity)\n            forces.append(force)\n\n        return positions, velocities, forces\n\n# Run simulation\nrobot = SimpleSensorimotorLoop()\npositions, velocities, forces = robot.run_simulation()\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\nax1.plot(positions)\nax1.set_title(\'Robot Position Over Time\')\nax1.set_ylabel(\'Position\')\n\nax2.plot(forces)\nax2.set_title(\'Applied Force Over Time\')\nax2.set_ylabel(\'Force\')\nax2.set_xlabel(\'Time Step\')\n\nplt.tight_layout()\nplt.show()\n')),(0,i.yg)("h2",{id:"chapter-2-kinematics-in-humanoid-robotics"},"Chapter 2: Kinematics in Humanoid Robotics"),(0,i.yg)("h3",{id:"exercise-2-dof-inverse-kinematics-solution"},"Exercise: 2-DOF Inverse Kinematics Solution"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Problem"),": Implement inverse kinematics for a 2-DOF planar arm."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Solution"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef inverse_kinematics_2dof(end_pos, link_lengths):\n    """\n    Calculate joint angles for 2-DOF planar arm\n    end_pos: [x, y] desired end-effector position\n    link_lengths: [L1, L2]\n    """\n    x, y = end_pos\n    L1, L2 = link_lengths\n\n    # Calculate distance from base to end-effector\n    r = np.sqrt(x**2 + y**2)\n\n    # Check if position is reachable\n    if r > L1 + L2:\n        raise ValueError("Position is outside workspace")\n    if r < abs(L1 - L2):\n        raise ValueError("Position is inside workspace but unreachable")\n\n    # Calculate joint angles using law of cosines\n    cos_theta2 = (r**2 - L1**2 - L2**2) / (2 * L1 * L2)\n    theta2 = np.arccos(np.clip(cos_theta2, -1, 1))  # Clip to avoid numerical errors\n\n    k1 = L1 + L2 * np.cos(theta2)\n    k2 = L2 * np.sin(theta2)\n\n    theta1 = np.arctan2(y, x) - np.arctan2(k2, k1)\n\n    return np.array([theta1, theta2])\n\ndef forward_kinematics_2dof(joint_angles, link_lengths):\n    """\n    Calculate end-effector position for 2-DOF planar arm\n    joint_angles: [theta1, theta2] in radians\n    link_lengths: [L1, L2]\n    """\n    theta1, theta2 = joint_angles\n    L1, L2 = link_lengths\n\n    # Calculate end-effector position\n    x = L1 * np.cos(theta1) + L2 * np.cos(theta1 + theta2)\n    y = L1 * np.sin(theta1) + L2 * np.sin(theta1 + theta2)\n\n    return np.array([x, y])\n\n# Test the solution\nlink_lengths = [1.0, 0.8]  # Link lengths\ndesired_pos = [1.2, 0.5]   # Desired end-effector position\n\ntry:\n    joint_angles = inverse_kinematics_2dof(desired_pos, link_lengths)\n    print(f"Joint angles: {np.degrees(joint_angles)} degrees")\n\n    # Verify with forward kinematics\n    calculated_pos = forward_kinematics_2dof(joint_angles, link_lengths)\n    print(f"Desired position: {desired_pos}")\n    print(f"Calculated position: {calculated_pos}")\n    print(f"Error: {np.linalg.norm(np.array(desired_pos) - calculated_pos):.6f}")\n\nexcept ValueError as e:\n    print(f"Error: {e}")\n\n# Visualization\ndef plot_robot(joint_angles, link_lengths):\n    theta1, theta2 = joint_angles\n    L1, L2 = link_lengths\n\n    # Calculate joint positions\n    elbow_x = L1 * np.cos(theta1)\n    elbow_y = L1 * np.sin(theta1)\n\n    end_x = elbow_x + L2 * np.cos(theta1 + theta2)\n    end_y = elbow_y + L2 * np.sin(theta1 + theta2)\n\n    # Plot\n    plt.figure(figsize=(8, 8))\n    plt.plot([0, elbow_x, end_x], [0, elbow_y, end_y], \'o-\', linewidth=3, markersize=10)\n    plt.plot(0, 0, \'ro\', markersize=15, label=\'Base\')\n    plt.plot(elbow_x, elbow_y, \'go\', markersize=12, label=\'Elbow\')\n    plt.plot(end_x, end_y, \'bo\', markersize=12, label=\'End Effector\')\n    plt.grid(True)\n    plt.axis(\'equal\')\n    plt.legend()\n    plt.title(\'2-DOF Robot Arm Configuration\')\n    plt.show()\n\nplot_robot(joint_angles, link_lengths)\n')),(0,i.yg)("h2",{id:"chapter-3-machine-learning-for-locomotion"},"Chapter 3: Machine Learning for Locomotion"),(0,i.yg)("h3",{id:"exercise-simple-ddpg-implementation-solution"},"Exercise: Simple DDPG Implementation Solution"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Problem"),": Implement a basic DDPG agent for a simple control task."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Solution"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom collections import deque\n\nclass Actor(nn.Module):\n    """Actor network: maps state to action"""\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = torch.relu(self.l1(state))\n        a = torch.relu(self.l2(a))\n        return self.max_action * torch.tanh(self.l3(a))\n\nclass Critic(nn.Module):\n    """Critic network: maps (state, action) to Q-value"""\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        # Q1 architecture\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, 1)\n\n        # Q2 architecture\n        self.l4 = nn.Linear(state_dim + action_dim, 256)\n        self.l5 = nn.Linear(256, 256)\n        self.l6 = nn.Linear(256, 1)\n\n    def forward(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q1 = torch.relu(self.l1(sa))\n        q1 = torch.relu(self.l2(q1))\n        q1 = self.l3(q1)\n\n        q2 = torch.relu(self.l4(sa))\n        q2 = torch.relu(self.l5(q2))\n        q2 = self.l6(q2)\n        return q1, q2\n\n    def Q1(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q1 = torch.relu(self.l1(sa))\n        q1 = torch.relu(self.l2(q1))\n        q1 = self.l3(q1)\n        return q1\n\nclass DDPGAgent:\n    def __init__(self, state_dim, action_dim, max_action):\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n\n        self.critic = Critic(state_dim, action_dim)\n        self.critic_target = Critic(state_dim, action_dim)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n\n        self.replay_buffer = deque(maxlen=1000000)\n        self.batch_size = 100\n        self.gamma = 0.99  # discount factor\n        self.tau = 0.005   # target network update rate\n        self.policy_noise = 0.2\n        self.noise_clip = 0.5\n        self.policy_freq = 2\n\n        self.total_it = 0  # Initialize iteration counter\n\n    def select_action(self, state, add_noise=True):\n        state = torch.FloatTensor(state.reshape(1, -1))\n        action = self.actor(state).cpu().data.numpy().flatten()\n\n        if add_noise:\n            noise = np.random.normal(0, 0.1, size=len(action))\n            action = action + noise\n            action = np.clip(action, -1, 1)\n\n        return action\n\n    def train(self, batch_size=100):\n        if len(self.replay_buffer) < batch_size:\n            return\n\n        batch = random.sample(self.replay_buffer, batch_size)\n        state, action, next_state, reward, done = map(np.stack, zip(*batch))\n\n        state = torch.FloatTensor(state)\n        action = torch.FloatTensor(action)\n        next_state = torch.FloatTensor(next_state)\n        reward = torch.FloatTensor(reward).unsqueeze(1)\n        done = torch.BoolTensor(done).unsqueeze(1)\n\n        # Compute target Q-value\n        with torch.no_grad():\n            noise = torch.FloatTensor(action).data.normal_(0, self.policy_noise)\n            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n\n            next_action = (self.actor_target(next_state) + noise).clamp(-1, 1)\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n            target_Q = reward + done.float() * self.gamma * torch.min(target_Q1, target_Q2)\n\n        # Get current Q estimates\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Compute critic loss\n        critic_loss = nn.MSELoss()(current_Q1, target_Q) + nn.MSELoss()(current_Q2, target_Q)\n\n        # Optimize critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Increment iteration counter\n        self.total_it += 1\n\n        # Delayed policy updates\n        if self.total_it % self.policy_freq == 0:\n            # Compute actor loss\n            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n\n            # Optimize actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Update target networks\n            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n    def store_transition(self, state, action, next_state, reward, done):\n        self.replay_buffer.append((state, action, next_state, reward, done))\n\n# Simple environment for testing\nclass SimpleEnv:\n    def __init__(self):\n        self.state = np.random.uniform(-1, 1, 2)  # 2D state\n        self.action_dim = 2\n        self.state_dim = 2\n        self.max_action = 1.0\n\n    def reset(self):\n        self.state = np.random.uniform(-1, 1, 2)\n        return self.state\n\n    def step(self, action):\n        # Simple dynamics: move in the direction of action\n        self.state += action * 0.1\n        # Add some noise\n        self.state += np.random.normal(0, 0.01, 2)\n\n        # Reward based on distance to origin (encourage staying near origin)\n        distance = np.linalg.norm(self.state)\n        reward = -distance  # Negative reward for being far from origin\n\n        # Simple termination condition\n        done = distance > 5.0\n\n        return self.state, reward, done, {}\n\n# Test the DDPG implementation\ndef test_ddpg():\n    env = SimpleEnv()\n    agent = DDPGAgent(env.state_dim, env.action_dim, env.max_action)\n\n    # Training loop\n    for episode in range(1000):\n        state = env.reset()\n        episode_reward = 0\n\n        for step in range(50):  # 50 steps per episode\n            action = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n\n            agent.store_transition(state, action, next_state, reward, done)\n            agent.train()\n\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                break\n\n        if episode % 100 == 0:\n            print(f"Episode {episode}, Reward: {episode_reward:.2f}")\n\n    print("DDPG training completed!")\n\n# Uncomment to run test\n# test_ddpg()\n')),(0,i.yg)("h2",{id:"chapter-4-simulation-environments"},"Chapter 4: Simulation Environments"),(0,i.yg)("h3",{id:"exercise-simple-pybullet-robot-solution"},"Exercise: Simple PyBullet Robot Solution"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Problem"),": Create a simple robot in PyBullet and implement basic control."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Solution"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import pybullet as p\nimport pybullet_data\nimport numpy as np\nimport time\n\nclass SimplePyBulletRobot:\n    def __init__(self, gui=True):\n        # Connect to physics server\n        if gui:\n            self.physics_client = p.connect(p.GUI)\n        else:\n            self.physics_client = p.connect(p.DIRECT)\n\n        # Set gravity\n        p.setGravity(0, 0, -9.81)\n\n        # Load plane\n        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n        self.plane_id = p.loadURDF("plane.urdf")\n\n        # Create a simple robot (a base with 2 wheels)\n        self.robot_id = self._create_simple_robot()\n\n        # Define wheel joints\n        self.left_wheel_joint = 0\n        self.right_wheel_joint = 1\n\n    def _create_simple_robot(self):\n        """Create a simple robot with 2 wheels"""\n        # Create collision shape for base\n        base_col = p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.3, 0.15, 0.1])\n\n        # Create visual shape for base\n        base_vis = p.createVisualShape(p.GEOM_BOX, halfExtents=[0.3, 0.15, 0.1],\n                                      rgbaColor=[0.8, 0.4, 0.2, 1])\n\n        # Create base link\n        base_id = p.createMultiBody(\n            baseMass=1.0,\n            baseCollisionShapeIndex=base_col,\n            baseVisualShapeIndex=base_vis,\n            basePosition=[0, 0, 0.2]\n        )\n\n        # Create wheels\n        wheel_radius = 0.1\n        wheel_width = 0.05\n        wheel_mass = 0.2\n\n        # Left wheel collision and visual shapes\n        wheel_col = p.createCollisionShape(p.GEOM_CYLINDER, radius=wheel_radius, length=wheel_width)\n        wheel_vis = p.createVisualShape(p.GEOM_CYLINDER, radius=wheel_radius, length=wheel_width,\n                                       rgbaColor=[0.3, 0.3, 0.3, 1])\n\n        # Add left wheel (fixed joint)\n        p.createConstraint(\n            parentBodyUniqueId=base_id,\n            parentLinkIndex=-1,\n            childBodyUniqueId=-1,\n            childLinkIndex=-1,\n            jointType=p.JOINT_FIXED,\n            jointAxis=[0, 0, 0],\n            parentFramePosition=[0.2, -0.2, 0],\n            childFramePosition=[0.2, -0.2, 0]\n        )\n\n        # Add right wheel (fixed joint)\n        p.createConstraint(\n            parentBodyUniqueId=base_id,\n            parentLinkIndex=-1,\n            childBodyUniqueId=-1,\n            childLinkIndex=-1,\n            jointType=p.JOINT_FIXED,\n            jointAxis=[0, 0, 0],\n            parentFramePosition=[0.2, 0.2, 0],\n            childFramePosition=[0.2, 0.2, 0]\n        )\n\n        # For simplicity in this example, we\'ll use a pre-made URDF or create a more complex one\n        # Actually, let\'s create a simpler version using a single body with wheels\n\n        # Remove the first attempt and create a proper robot with joints\n        p.removeBody(base_id)\n\n        # Create a proper robot with joints\n        # For this example, we\'ll create a simple differential drive robot\n        robotStartPos = [0, 0, 0.2]\n        robotStartOrientation = p.getQuaternionFromEuler([0, 0, 0])\n\n        # Load a simple robot URDF (using a simple one from pybullet data or creating our own)\n        # For this example, let\'s just create a basic model using multiple bodies connected with joints\n        return self._create_differential_drive_robot()\n\n    def _create_differential_drive_robot(self):\n        """Create a differential drive robot with proper joints"""\n        # Base body\n        base_col = p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.3, 0.2, 0.1])\n        base_vis = p.createVisualShape(p.GEOM_BOX, halfExtents=[0.3, 0.2, 0.1],\n                                      rgbaColor=[0.8, 0.4, 0.2, 1])\n\n        base_id = p.createMultiBody(\n            baseMass=1.0,\n            baseCollisionShapeIndex=base_col,\n            baseVisualShapeIndex=base_vis,\n            basePosition=[0, 0, 0.2]\n        )\n\n        # Wheel parameters\n        wheel_radius = 0.1\n        wheel_width = 0.05\n        wheel_mass = 0.1\n\n        # Left wheel collision and visual shapes\n        wheel_col = p.createCollisionShape(p.GEOM_CYLINDER, radius=wheel_radius, length=wheel_width)\n        wheel_vis = p.createVisualShape(p.GEOM_CYLINDER, radius=wheel_radius, length=wheel_width,\n                                       rgbaColor=[0.3, 0.3, 0.3, 1])\n\n        # Left wheel\n        left_wheel_id = p.createMultiBody(\n            baseMass=wheel_mass,\n            baseCollisionShapeIndex=wheel_col,\n            baseVisualShapeIndex=wheel_vis,\n            basePosition=[0.2, -0.2, 0.1]\n        )\n\n        # Right wheel\n        right_wheel_id = p.createMultiBody(\n            baseMass=wheel_mass,\n            baseCollisionShapeIndex=wheel_col,\n            baseVisualShapeIndex=wheel_vis,\n            basePosition=[0.2, 0.2, 0.1]\n        )\n\n        # Create revolute joints to connect wheels to base\n        p.createConstraint(\n            base_id, -1, left_wheel_id, -1, p.JOINT_REVOLUTE,\n            [0, 0, 1], [0, -0.2, 0.1], [0, -0.2, 0.1]\n        )\n\n        p.createConstraint(\n            base_id, -1, right_wheel_id, -1, p.JOINT_REVOLUTE,\n            [0, 0, 1], [0, 0.2, 0.1], [0, 0.2, 0.1]\n        )\n\n        # Return base ID as the main robot ID\n        return base_id\n\n    def set_wheel_velocities(self, left_vel, right_vel):\n        """Set target velocities for the wheels"""\n        # In a real implementation, we would control the wheel joints\n        # For this simplified example, we\'ll just print the intended velocities\n        print(f"Setting wheel velocities - Left: {left_vel}, Right: {right_vel}")\n\n    def step_simulation(self):\n        """Step the simulation"""\n        p.stepSimulation()\n        time.sleep(1./240.)  # Real-time simulation at 240 Hz\n\n    def get_robot_position(self):\n        """Get the robot\'s position and orientation"""\n        pos, orn = p.getBasePositionAndOrientation(self.robot_id)\n        return pos, orn\n\n    def disconnect(self):\n        """Disconnect from physics server"""\n        p.disconnect(self.physics_client)\n\n# Example usage\ndef run_simple_robot_example():\n    robot = SimplePyBulletRobot(gui=True)\n\n    # Simple control loop\n    for i in range(1000):\n        # Simple movement pattern\n        t = i / 240.0  # Time in seconds\n        left_vel = 2.0 + 0.5 * np.sin(t)  # Varying velocity\n        right_vel = 2.0 + 0.5 * np.cos(t)\n\n        robot.set_wheel_velocities(left_vel, right_vel)\n        robot.step_simulation()\n\n        if i % 100 == 0:\n            pos, orn = robot.get_robot_position()\n            print(f"Step {i}, Position: {pos[:2]}")  # Just x,y coordinates\n\n    robot.disconnect()\n\n# Uncomment to run the example\n# run_simple_robot_example()\n')),(0,i.yg)("h2",{id:"additional-learning-resources"},"Additional Learning Resources"),(0,i.yg)("h3",{id:"books-and-textbooks"},"Books and Textbooks"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},'"Robotics: Modelling, Planning and Control"')," by Siciliano et al. - Comprehensive coverage of robot kinematics, dynamics, and control"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},'"Probabilistic Robotics"')," by Thrun, Burgard, and Fox - Focuses on uncertainty in robotics"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},'"Introduction to Autonomous Mobile Robots"')," by Siegwart et al. - Good introduction to mobile robotics"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},'"Reinforcement Learning: An Introduction"')," by Sutton and Barto - Foundational text for RL")),(0,i.yg)("h3",{id:"online-courses-and-tutorials"},"Online Courses and Tutorials"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"MIT 6.832 (Underactuated Robotics)")," - Advanced robotics course available online"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Columbia University Robotics Course")," - Available on edX"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"ETH Zurich Robot Dynamics and Control")," - Comprehensive online materials"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"OpenAI Spinning Up")," - Excellent resource for learning RL")),(0,i.yg)("h3",{id:"software-libraries-and-frameworks"},"Software Libraries and Frameworks"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"PyBullet")," - Physics simulation with Python API"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Gazebo")," - ROS-integrated simulation environment"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Mujoco")," - High-fidelity physics engine"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"ROS/ROS2")," - Robot operating system"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"OpenAI Gym")," - Reinforcement learning environments"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Stable Baselines3")," - RL algorithms implementation"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"PyTorch/TensorFlow")," - Deep learning frameworks")),(0,i.yg)("h3",{id:"research-papers"},"Research Papers"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},'"Continuous control with deep reinforcement learning" (DDPG) - Lillicrap et al.'),(0,i.yg)("li",{parentName:"ol"},'"Addressing Function Approximation Error in Actor-Critic Methods" (TD3) - Fujimoto et al.'),(0,i.yg)("li",{parentName:"ol"},'"Human-level control through deep reinforcement learning" (DQN) - Mnih et al.'),(0,i.yg)("li",{parentName:"ol"},'"Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor" (SAC) - Haarnoja et al.')),(0,i.yg)("h3",{id:"communities-and-forums"},"Communities and Forums"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Reddit r/robotics")," - Active community for robotics discussions"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"ROS Answers")," - Q&A for ROS users"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"PyBullet Forum")," - Community support for PyBullet"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"AI Stack Exchange")," - General AI and robotics questions")),(0,i.yg)("h3",{id:"hardware-platforms-for-practice"},"Hardware Platforms for Practice"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"TurtleBot3")," - Affordable mobile robot platform"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"JetBot")," - NVIDIA's educational robot"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Robotis OP3")," - Humanoid robot for research"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Unitree Go1/A1")," - Quadruped robots for research"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Arduino/Raspberry Pi")," - For building custom robots")),(0,i.yg)("h2",{id:"problem-sets-and-challenges"},"Problem Sets and Challenges"),(0,i.yg)("h3",{id:"beginner-level"},"Beginner Level"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Implement forward and inverse kinematics for a 2-DOF arm"),(0,i.yg)("li",{parentName:"ol"},"Create a simple PID controller for motor position control"),(0,i.yg)("li",{parentName:"ol"},"Implement a basic path following algorithm"),(0,i.yg)("li",{parentName:"ol"},"Simulate a sensorimotor loop with noise")),(0,i.yg)("h3",{id:"intermediate-level"},"Intermediate Level"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Implement a walking gait for a simple biped"),(0,i.yg)("li",{parentName:"ol"},"Train a DQN agent to balance an inverted pendulum"),(0,i.yg)("li",{parentName:"ol"},"Create a SLAM system using simulated sensors"),(0,i.yg)("li",{parentName:"ol"},"Implement impedance control for safe interaction")),(0,i.yg)("h3",{id:"advanced-level"},"Advanced Level"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Develop a whole-body controller for a humanoid robot"),(0,i.yg)("li",{parentName:"ol"},"Train a locomotion policy that generalizes across terrains"),(0,i.yg)("li",{parentName:"ol"},"Implement sim-to-real transfer for a manipulation task"),(0,i.yg)("li",{parentName:"ol"},"Create a multi-robot coordination system")),(0,i.yg)("h2",{id:"solutions-to-common-implementation-challenges"},"Solutions to Common Implementation Challenges"),(0,i.yg)("h3",{id:"sim-to-real-transfer"},"Sim-to-Real Transfer"),(0,i.yg)("p",null,"Problem: Policies trained in simulation don't work on real robots."),(0,i.yg)("p",null,"Solution approaches:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Domain randomization - Randomize simulation parameters"),(0,i.yg)("li",{parentName:"ol"},"System identification - Learn real-world dynamics"),(0,i.yg)("li",{parentName:"ol"},"Robust control design - Account for model uncertainty"),(0,i.yg)("li",{parentName:"ol"},"Online adaptation - Update policy based on real experience")),(0,i.yg)("h3",{id:"real-time-control"},"Real-time Control"),(0,i.yg)("p",null,"Problem: Control loops missing timing deadlines."),(0,i.yg)("p",null,"Solutions:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Use real-time operating systems (RT Linux, Xenomai)"),(0,i.yg)("li",{parentName:"ol"},"Optimize computational complexity"),(0,i.yg)("li",{parentName:"ol"},"Use fixed-step integration"),(0,i.yg)("li",{parentName:"ol"},"Implement priority-based scheduling")),(0,i.yg)("h3",{id:"sensor-fusion"},"Sensor Fusion"),(0,i.yg)("p",null,"Problem: Combining data from multiple sensors reliably."),(0,i.yg)("p",null,"Solutions:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Extended Kalman Filter (EKF) for nonlinear systems"),(0,i.yg)("li",{parentName:"ol"},"Unscented Kalman Filter (UKF) for better nonlinear approximation"),(0,i.yg)("li",{parentName:"ol"},"Particle filters for multimodal distributions"),(0,i.yg)("li",{parentName:"ol"},"Complementary filters for simple fusion tasks")),(0,i.yg)("h2",{id:"assessment-rubrics"},"Assessment Rubrics"),(0,i.yg)("h3",{id:"implementation-projects"},"Implementation Projects"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Functionality (40%)"),": Does the system perform as intended?"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Code Quality (25%)"),": Is the code well-structured and documented?"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Performance (20%)"),": How efficiently does the solution work?"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Robustness (15%)"),": How well does it handle edge cases and errors?")),(0,i.yg)("h3",{id:"theoretical-understanding"},"Theoretical Understanding"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Conceptual Understanding (50%)"),": Grasp of fundamental principles"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Application (30%)"),": Ability to apply concepts to new situations"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Analysis (20%)"),": Critical thinking and problem-solving skills")),(0,i.yg)("h2",{id:"recommended-study-path"},"Recommended Study Path"),(0,i.yg)("h3",{id:"month-1-foundations"},"Month 1: Foundations"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Week 1-2: Mathematics review (linear algebra, calculus, probability)"),(0,i.yg)("li",{parentName:"ul"},"Week 3-4: Introduction to robotics and kinematics")),(0,i.yg)("h3",{id:"month-2-control-and-dynamics"},"Month 2: Control and Dynamics"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Week 5-6: Control theory and implementation"),(0,i.yg)("li",{parentName:"ul"},"Week 7-8: Dynamics and system modeling")),(0,i.yg)("h3",{id:"month-3-perception-and-learning"},"Month 3: Perception and Learning"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Week 9-10: Sensors and perception"),(0,i.yg)("li",{parentName:"ul"},"Week 11-12: Machine learning for robotics")),(0,i.yg)("h3",{id:"month-4-integration-and-application"},"Month 4: Integration and Application"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Week 13-14: Simulation environments"),(0,i.yg)("li",{parentName:"ul"},"Week 15-16: Real-world implementation and testing")),(0,i.yg)("p",null,"This comprehensive set of solutions and resources should provide students with the tools they need to understand and implement the concepts covered in the Physical AI & Humanoid Robotics book."))}m.isMDXComponent=!0}}]);