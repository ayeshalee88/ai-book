"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[471],{15680:(e,n,t)=>{t.d(n,{xA:()=>d,yg:()=>f});var a=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function o(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},p="mdxType",_={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),p=c(t),m=i,f=p["".concat(l,".").concat(m)]||p[m]||_[m]||r;return t?a.createElement(f,s(s({ref:n},d),{},{components:t})):a.createElement(f,s({ref:n},d))});function f(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,s=new Array(r);s[0]=m;var o={};for(var l in n)hasOwnProperty.call(n,l)&&(o[l]=n[l]);o.originalType=e,o[p]="string"==typeof e?e:i,s[1]=o;for(var c=2;c<r;c++)s[c]=t[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},23604:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(58168),i=(t(96540),t(15680));const r={id:"rl-applications",title:"Reinforcement Learning Applications in Physical AI",sidebar_label:"RL Applications"},s="Reinforcement Learning Applications in Physical AI",o={unversionedId:"ai-integration/rl-applications",id:"ai-integration/rl-applications",title:"Reinforcement Learning Applications in Physical AI",description:"Introduction to RL in Physical AI",source:"@site/docs/ai-integration/rl-applications.md",sourceDirName:"ai-integration",slug:"/ai-integration/rl-applications",permalink:"/ai-book/docs/ai-integration/rl-applications",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/ai-integration/rl-applications.md",tags:[],version:"current",frontMatter:{id:"rl-applications",title:"Reinforcement Learning Applications in Physical AI",sidebar_label:"RL Applications"},sidebar:"tutorialSidebar",previous:{title:"ML for Locomotion",permalink:"/ai-book/docs/ai-integration/ml-locomotion"},next:{title:"CV for Interaction",permalink:"/ai-book/docs/ai-integration/cv-interaction"}},l={},c=[{value:"Introduction to RL in Physical AI",id:"introduction-to-rl-in-physical-ai",level:2},{value:"Why RL for Physical AI?",id:"why-rl-for-physical-ai",level:3},{value:"Core RL Concepts in Physical AI",id:"core-rl-concepts-in-physical-ai",level:3},{value:"Markov Decision Process (MDP) Formulation",id:"markov-decision-process-mdp-formulation",level:4},{value:"Key Challenges in Physical AI RL",id:"key-challenges-in-physical-ai-rl",level:4},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:3},{value:"Actor-Critic Methods",id:"actor-critic-methods",level:4},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:4},{value:"RL Applications in Physical AI",id:"rl-applications-in-physical-ai",level:3},{value:"1. Locomotion Control",id:"1-locomotion-control",level:4},{value:"2. Manipulation and Grasping",id:"2-manipulation-and-grasping",level:4},{value:"3. Multi-Agent Coordination",id:"3-multi-agent-coordination",level:4},{value:"Advanced RL Techniques for Physical AI",id:"advanced-rl-techniques-for-physical-ai",level:3},{value:"Model-Based RL",id:"model-based-rl",level:4},{value:"Hierarchical RL",id:"hierarchical-rl",level:4},{value:"Safety in RL for Physical AI",id:"safety-in-rl-for-physical-ai",level:3},{value:"Safe RL with Constrained Optimization",id:"safe-rl-with-constrained-optimization",level:4},{value:"Practical Considerations",id:"practical-considerations",level:3},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:4},{value:"Transfer Learning",id:"transfer-learning",level:4},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:3}],d={toc:c},p="wrapper";function _({components:e,...n}){return(0,i.yg)(p,(0,a.A)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"reinforcement-learning-applications-in-physical-ai"},"Reinforcement Learning Applications in Physical AI"),(0,i.yg)("h2",{id:"introduction-to-rl-in-physical-ai"},"Introduction to RL in Physical AI"),(0,i.yg)("p",null,"Reinforcement Learning (RL) has emerged as a powerful paradigm for developing adaptive and robust control strategies in physical AI systems. Unlike traditional control methods that rely on precise mathematical models, RL algorithms learn optimal behaviors through interaction with the environment, making them particularly suitable for complex, uncertain, and dynamic physical systems."),(0,i.yg)("h3",{id:"why-rl-for-physical-ai"},"Why RL for Physical AI?"),(0,i.yg)("p",null,"Physical AI systems face several challenges that make RL particularly attractive:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Model Uncertainty"),": Real-world dynamics are often difficult to model accurately"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Environmental Variability"),": Systems must adapt to changing conditions and terrains"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Optimization Complexity"),": Multiple competing objectives (speed, efficiency, safety)"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Robustness Requirements"),": Systems must handle disturbances and unexpected situations")),(0,i.yg)("h3",{id:"core-rl-concepts-in-physical-ai"},"Core RL Concepts in Physical AI"),(0,i.yg)("h4",{id:"markov-decision-process-mdp-formulation"},"Markov Decision Process (MDP) Formulation"),(0,i.yg)("p",null,"Physical AI control can be formulated as an MDP with:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"State Space (S)"),": Robot configuration, velocities, sensor readings, environment state"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Action Space (A)"),": Control commands, joint torques, desired positions"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Transition Function (P)"),": Physics simulation or real-world dynamics"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Reward Function (R)"),": Task-specific feedback (e.g., forward velocity, energy efficiency)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Discount Factor (\u03b3)"),": Trade-off between immediate and future rewards")),(0,i.yg)("h4",{id:"key-challenges-in-physical-ai-rl"},"Key Challenges in Physical AI RL"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Continuous State-Action Spaces"),": Most physical systems have continuous states and actions"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Safety Requirements"),": Learning must not damage the robot or environment"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Sample Efficiency"),": Real-world interactions are expensive and time-consuming"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Real-time Constraints"),": Control decisions must be made within strict time limits")),(0,i.yg)("h3",{id:"deep-reinforcement-learning-algorithms"},"Deep Reinforcement Learning Algorithms"),(0,i.yg)("h4",{id:"actor-critic-methods"},"Actor-Critic Methods"),(0,i.yg)("p",null,"Actor-critic methods combine value-based and policy-based approaches:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n        self.tanh = nn.Tanh()\n\n    def forward(self, state):\n        a = torch.relu(self.l1(state))\n        a = torch.relu(self.l2(a))\n        a = self.tanh(self.l3(a))\n        return self.max_action * a\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        # Q-network\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, 1)\n\n    def forward(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q = torch.relu(self.l1(sa))\n        q = torch.relu(self.l2(q))\n        q = self.l3(q)\n        return q\n\nclass TD3Agent:\n    def __init__(self, state_dim, action_dim, max_action):\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n\n        self.critic_1 = Critic(state_dim, action_dim)\n        self.critic_1_target = Critic(state_dim, action_dim)\n        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=1e-3)\n\n        self.critic_2 = Critic(state_dim, action_dim)\n        self.critic_2_target = Critic(state_dim, action_dim)\n        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=1e-3)\n\n        self.replay_buffer = deque(maxlen=1000000)\n        self.max_action = max_action\n        self.discount = 0.99\n        self.tau = 0.005\n        self.policy_noise = 0.2 * max_action\n        self.noise_clip = 0.5 * max_action\n        self.policy_freq = 2\n        self.total_it = 0\n\n    def select_action(self, state, add_noise=True):\n        state = torch.FloatTensor(state.reshape(1, -1))\n        action = self.actor(state).cpu().data.numpy().flatten()\n\n        if add_noise:\n            noise = np.random.normal(0, 0.1 * self.max_action, size=action.shape)\n            action = action + noise\n            action = np.clip(action, -self.max_action, self.max_action)\n\n        return action\n\n    def train(self, batch_size=256):\n        if len(self.replay_buffer) < batch_size:\n            return\n\n        self.total_it += 1\n\n        # Sample replay buffer\n        batch = random.sample(self.replay_buffer, batch_size)\n        state, action, next_state, reward, done = map(np.stack, zip(*batch))\n\n        state = torch.FloatTensor(state)\n        action = torch.FloatTensor(action)\n        next_state = torch.FloatTensor(next_state)\n        reward = torch.FloatTensor(reward).unsqueeze(1)\n        done = torch.BoolTensor(done).unsqueeze(1)\n\n        # Select next action\n        noise = torch.FloatTensor(action).data.normal_(0, self.policy_noise)\n        noise = noise.clamp(-self.noise_clip, self.noise_clip)\n\n        next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n\n        # Compute target Q-value\n        target_Q1 = self.critic_1_target(next_state, next_action)\n        target_Q2 = self.critic_2_target(next_state, next_action)\n        target_Q = reward + done.float() * self.discount * torch.min(target_Q1, target_Q2)\n\n        # Get current Q estimates\n        current_Q1 = self.critic_1(state, action)\n        current_Q2 = self.critic_2(state, action)\n\n        # Compute critic loss\n        critic_loss = nn.MSELoss()(current_Q1, target_Q) + nn.MSELoss()(current_Q2, target_Q)\n\n        # Optimize critics\n        self.critic_1_optimizer.zero_grad()\n        self.critic_2_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_1_optimizer.step()\n        self.critic_2_optimizer.step()\n\n        # Delayed policy updates\n        if self.total_it % self.policy_freq == 0:\n            # Compute actor loss\n            actor_loss = -self.critic_1(state, self.actor(state)).mean()\n\n            # Optimize actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Update target networks\n            for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n            for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n    def store_transition(self, state, action, next_state, reward, done):\n        self.replay_buffer.append((state, action, next_state, reward, done))\n")),(0,i.yg)("h4",{id:"proximal-policy-optimization-ppo"},"Proximal Policy Optimization (PPO)"),(0,i.yg)("p",null,"PPO is particularly effective for continuous control tasks:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"class PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2,\n                 K_epochs=80, entropy_coef=0.01):\n        self.lr = lr\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.K_epochs = K_epochs\n        self.entropy_coef = entropy_coef\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.Tanh(),\n            nn.Linear(256, 256),\n            nn.Tanh(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        )\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.Tanh(),\n            nn.Linear(256, 256),\n            nn.Tanh(),\n            nn.Linear(256, 1)\n        )\n\n        self.optimizer = optim.Adam(list(self.actor.parameters()) +\n                                   list(self.critic.parameters()), lr=lr)\n\n        self.MseLoss = nn.MSELoss()\n\n    def act(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0)\n\n        action_probs = torch.softmax(self.actor(state), dim=-1)\n        dist = torch.distributions.Categorical(action_probs)\n        action = dist.sample()\n\n        log_prob = dist.log_prob(action)\n\n        return action.item(), log_prob.item()\n\n    def evaluate(self, state, action):\n        action_probs = torch.softmax(self.actor(state), dim=-1)\n        dist = torch.distributions.Categorical(action_probs)\n\n        logprobs = dist.log_prob(action)\n        entropy = dist.entropy()\n        state_values = self.critic(state)\n\n        return logprobs, torch.squeeze(state_values), entropy\n\n    def update(self, memory):\n        # Monte Carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(memory.rewards),\n                                     reversed(memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # Normalizing the rewards\n        rewards = torch.tensor(rewards, dtype=torch.float32)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # Convert list to tensor\n        old_states = torch.squeeze(torch.stack(memory.states)).detach()\n        old_actions = torch.squeeze(torch.stack(memory.actions)).detach()\n        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).detach()\n\n        # Optimize policy for K epochs\n        for _ in range(self.K_epochs):\n            # Evaluate old actions and values\n            logprobs, state_values, dist_entropy = self.evaluate(old_states, old_actions)\n\n            # Finding the ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # Finding Surrogate Loss\n            advantages = rewards - state_values.detach()\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Final loss of clipped objective PPO\n            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - self.entropy_coef * dist_entropy\n\n            # Take gradient step\n            self.optimizer.zero_grad()\n            loss.mean().backward()\n            self.optimizer.step()\n")),(0,i.yg)("h3",{id:"rl-applications-in-physical-ai"},"RL Applications in Physical AI"),(0,i.yg)("h4",{id:"1-locomotion-control"},"1. Locomotion Control"),(0,i.yg)("p",null,"RL has been successfully applied to learn complex locomotion patterns:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"class LocomotionEnvironment:\n    def __init__(self):\n        # Initialize physics simulation\n        self.sim = self.initialize_simulation()\n        self.robot = self.create_robot()\n        self.max_episode_steps = 1000\n        self.current_step = 0\n\n    def reset(self):\n        # Reset robot to initial state\n        self.sim.reset()\n        self.current_step = 0\n        return self.get_state()\n\n    def get_state(self):\n        # Return robot state: positions, velocities, IMU readings, etc.\n        robot_state = self.robot.get_state()\n        return np.concatenate([\n            robot_state['joint_positions'],\n            robot_state['joint_velocities'],\n            robot_state['imu_data'],\n            robot_state['contact_sensors']\n        ])\n\n    def step(self, action):\n        # Apply action to robot\n        self.robot.apply_action(action)\n\n        # Step simulation\n        self.sim.step()\n\n        # Get new state\n        next_state = self.get_state()\n\n        # Calculate reward\n        reward = self.calculate_locomotion_reward()\n\n        # Check termination\n        done = self.is_terminated()\n        self.current_step += 1\n\n        info = {}  # Additional information\n        return next_state, reward, done, info\n\n    def calculate_locomotion_reward(self):\n        # Forward velocity reward\n        forward_vel = self.robot.get_forward_velocity()\n        reward = max(0, forward_vel) * 0.5\n\n        # Energy efficiency penalty\n        torques = self.robot.get_applied_torques()\n        energy_cost = np.sum(np.abs(torques))\n        reward -= energy_cost * 0.001\n\n        # Stability reward\n        upright = self.robot.get_upright_orientation()\n        if upright > 0.8:  # Robot is mostly upright\n            reward += 0.2\n        else:\n            reward -= 1.0  # Penalty for falling\n\n        # Survival bonus\n        reward += 0.05  # Small bonus for staying alive\n\n        return reward\n\n    def is_terminated(self):\n        # Check if episode should terminate\n        fallen = self.robot.get_upright_orientation() < 0.3\n        timeout = self.current_step >= self.max_episode_steps\n        return fallen or timeout\n\n# Example training loop\ndef train_locomotion_agent():\n    env = LocomotionEnvironment()\n    state_dim = env.reset().shape[0]\n    action_dim = env.action_space.shape[0]\n    max_action = float(env.action_space.high[0])\n\n    agent = TD3Agent(state_dim, action_dim, max_action)\n\n    episodes = 1000\n    for episode in range(episodes):\n        state = env.reset()\n        episode_reward = 0\n        done = False\n\n        while not done:\n            action = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n\n            agent.store_transition(state, action, next_state, reward, done)\n            agent.train()\n\n            state = next_state\n            episode_reward += reward\n\n        if episode % 10 == 0:\n            print(f\"Episode {episode}, Reward: {episode_reward:.2f}\")\n")),(0,i.yg)("h4",{id:"2-manipulation-and-grasping"},"2. Manipulation and Grasping"),(0,i.yg)("p",null,"RL for dexterous manipulation tasks:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"class ManipulationEnvironment:\n    def __init__(self):\n        self.sim = self.initialize_simulation()\n        self.robot_arm = self.create_robot_arm()\n        self.object = self.create_object()\n\n    def calculate_manipulation_reward(self):\n        # Distance to target\n        current_pos = self.robot_arm.get_ee_position()\n        target_pos = self.object.get_position()\n        dist_to_target = np.linalg.norm(current_pos - target_pos)\n\n        # Reward based on distance (closer = higher reward)\n        distance_reward = np.exp(-dist_to_target)\n\n        # Grasp success reward\n        grasp_success = self.check_grasp_success()\n        grasp_reward = 10.0 if grasp_success else 0.0\n\n        # Energy penalty\n        torques = self.robot_arm.get_joint_torques()\n        energy_penalty = -0.001 * np.sum(np.abs(torques))\n\n        total_reward = distance_reward + grasp_reward + energy_penalty\n        return total_reward\n\n    def check_grasp_success(self):\n        # Check if object is successfully grasped\n        contact_points = self.sim.get_contact_points()\n        # Implementation depends on specific grasp detection logic\n        pass\n")),(0,i.yg)("h4",{id:"3-multi-agent-coordination"},"3. Multi-Agent Coordination"),(0,i.yg)("p",null,"RL for coordinating multiple physical agents:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"class MultiAgentEnvironment:\n    def __init__(self, num_agents=2):\n        self.agents = [self.create_agent(i) for i in range(num_agents)]\n        self.num_agents = num_agents\n\n    def step(self, actions):\n        # Apply actions to all agents simultaneously\n        next_states = []\n        rewards = []\n        dones = []\n\n        for i, action in enumerate(actions):\n            state, reward, done, _ = self.agents[i].step(action)\n            next_states.append(state)\n            rewards.append(reward)\n            dones.append(done)\n\n        # Check if all agents are done\n        all_done = all(dones)\n\n        # Calculate coordination rewards\n        coordination_reward = self.calculate_coordination_reward()\n\n        # Add coordination reward to all agents\n        rewards = [r + coordination_reward for r in rewards]\n\n        return next_states, rewards, [all_done] * self.num_agents, {}\n\n    def calculate_coordination_reward(self):\n        # Example: reward for maintaining formation\n        agent_positions = [agent.get_position() for agent in self.agents]\n\n        # Calculate formation error\n        formation_error = 0\n        for i in range(self.num_agents):\n            for j in range(i+1, self.num_agents):\n                desired_dist = self.get_desired_distance(i, j)\n                actual_dist = np.linalg.norm(agent_positions[i] - agent_positions[j])\n                formation_error += abs(actual_dist - desired_dist)\n\n        # Reward is negative of error (lower error = higher reward)\n        return -formation_error * 0.1\n")),(0,i.yg)("h3",{id:"advanced-rl-techniques-for-physical-ai"},"Advanced RL Techniques for Physical AI"),(0,i.yg)("h4",{id:"model-based-rl"},"Model-Based RL"),(0,i.yg)("p",null,"Learning environment models to improve sample efficiency:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class WorldModel(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(WorldModel, self).__init__()\n\n        # Encoder: state -> latent representation\n        self.encoder = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64)\n        )\n\n        # Dynamics model: latent_state, action -> next_latent_state\n        self.dynamics = nn.Sequential(\n            nn.Linear(64 + action_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64)\n        )\n\n        # Decoder: latent_state -> state\n        self.decoder = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, state_dim)\n        )\n\n        # Reward model: latent_state, action -> reward\n        self.reward_model = nn.Sequential(\n            nn.Linear(64 + action_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, state, action):\n        latent = self.encoder(state)\n        next_latent = self.dynamics(torch.cat([latent, action], dim=-1))\n        next_state = self.decoder(next_latent)\n        reward = self.reward_model(torch.cat([latent, action], dim=-1))\n\n        return next_state, reward\n\nclass ModelBasedAgent:\n    def __init__(self, state_dim, action_dim):\n        self.world_model = WorldModel(state_dim, action_dim)\n        self.policy = Actor(state_dim, action_dim, max_action=1.0)\n        self.model_optimizer = optim.Adam(self.world_model.parameters(), lr=1e-3)\n        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=1e-4)\n\n    def train_world_model(self, states, actions, next_states, rewards):\n        # Predict next states and rewards\n        pred_next_states, pred_rewards = self.world_model(states, actions)\n\n        # Compute model loss\n        state_loss = nn.MSELoss()(pred_next_states, next_states)\n        reward_loss = nn.MSELoss()(pred_rewards, rewards.unsqueeze(1))\n        model_loss = state_loss + reward_loss\n\n        # Update model\n        self.model_optimizer.zero_grad()\n        model_loss.backward()\n        self.model_optimizer.step()\n\n        return model_loss.item()\n\n    def plan_with_model(self, current_state, horizon=10):\n        """Plan action sequence using learned model"""\n        best_action_sequence = None\n        best_return = float(\'-inf\')\n\n        # Try multiple action sequences and select best\n        for _ in range(100):  # Number of random rollouts\n            current_state_copy = current_state.clone()\n            total_return = 0\n\n            action_sequence = []\n            for t in range(horizon):\n                # Sample random action\n                action = torch.randn(self.action_dim) * 0.5\n                action_sequence.append(action)\n\n                # Use model to predict next state and reward\n                next_state, reward = self.world_model(current_state_copy, action)\n                total_return += reward.item() * (0.99 ** t)  # Discounted reward\n\n                current_state_copy = next_state\n\n            if total_return > best_return:\n                best_return = total_return\n                best_action_sequence = action_sequence\n\n        # Return first action of best sequence\n        return best_action_sequence[0] if best_action_sequence else None\n')),(0,i.yg)("h4",{id:"hierarchical-rl"},"Hierarchical RL"),(0,i.yg)("p",null,"Breaking complex tasks into subtasks:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class HierarchicalAgent:\n    def __init__(self, state_dim, action_dim, num_skills=5):\n        # High-level policy: state -> skill selection\n        self.high_level_policy = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_skills)\n        )\n\n        # Low-level policies: state -> action for each skill\n        self.low_level_policies = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(state_dim, 256),\n                nn.ReLU(),\n                nn.Linear(256, 256),\n                nn.ReLU(),\n                nn.Linear(256, action_dim)\n            ) for _ in range(num_skills)\n        ])\n\n        # Skill termination conditions\n        self.termination_networks = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(state_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, 1),\n                nn.Sigmoid()\n            ) for _ in range(num_skills)\n        ])\n\n        self.optimizer = optim.Adam(\n            list(self.high_level_policy.parameters()) +\n            list(self.low_level_policies.parameters()) +\n            list(self.termination_networks.parameters()),\n            lr=1e-4\n        )\n\n    def select_skill(self, state):\n        """Select skill using high-level policy"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        skill_logits = self.high_level_policy(state_tensor)\n        skill_probs = torch.softmax(skill_logits, dim=-1)\n\n        # Sample skill\n        skill = torch.multinomial(skill_probs, 1).item()\n        return skill\n\n    def execute_skill(self, state, skill):\n        """Execute low-level policy for selected skill"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action = self.low_level_policies[skill](state_tensor)\n        return action.squeeze(0).detach().numpy()\n\n    def should_terminate_skill(self, state, skill):\n        """Check if current skill should terminate"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        termination_prob = self.termination_networks[skill](state_tensor)\n        return termination_prob.item() > 0.5\n')),(0,i.yg)("h3",{id:"safety-in-rl-for-physical-ai"},"Safety in RL for Physical AI"),(0,i.yg)("h4",{id:"safe-rl-with-constrained-optimization"},"Safe RL with Constrained Optimization"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class SafeRLAgent:\n    def __init__(self, state_dim, action_dim, safety_threshold=0.1):\n        self.critic = Critic(state_dim, action_dim)\n        self.actor = Actor(state_dim, action_dim, max_action=1.0)\n        self.safety_critic = Critic(state_dim, action_dim)  # For safety constraint\n        self.safety_threshold = safety_threshold\n\n    def safe_action_selection(self, state):\n        """Select action that satisfies safety constraints"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n\n        # Get action from policy\n        action = self.actor(state_tensor)\n\n        # Check safety constraint\n        safety_value = self.safety_critic(state_tensor, action)\n\n        if safety_value.item() > self.safety_threshold:\n            # Action is unsafe, use conservative action\n            return self.get_conservative_action(state)\n\n        return action.squeeze(0).detach().numpy()\n\n    def get_conservative_action(self, state):\n        """Return safe, conservative action"""\n        # Implementation depends on specific safety requirements\n        return np.zeros(self.action_dim)\n')),(0,i.yg)("h3",{id:"practical-considerations"},"Practical Considerations"),(0,i.yg)("h4",{id:"hyperparameter-tuning"},"Hyperparameter Tuning"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"def hyperparameter_tuning():\n    \"\"\"Systematic hyperparameter search for RL\"\"\"\n    import itertools\n\n    # Define hyperparameter ranges\n    learning_rates = [1e-3, 3e-4, 1e-4]\n    batch_sizes = [64, 128, 256]\n    discount_factors = [0.95, 0.99, 0.995]\n\n    best_score = float('-inf')\n    best_params = {}\n\n    for lr, bs, gamma in itertools.product(learning_rates, batch_sizes, discount_factors):\n        # Train agent with current parameters\n        agent = TD3Agent(state_dim, action_dim, max_action)\n        score = train_and_evaluate(agent, lr, bs, gamma)\n\n        if score > best_score:\n            best_score = score\n            best_params = {'lr': lr, 'bs': bs, 'gamma': gamma}\n\n    return best_params\n")),(0,i.yg)("h4",{id:"transfer-learning"},"Transfer Learning"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def transfer_learning(source_agent, target_env):\n    """Transfer learned policy to new environment"""\n    # Initialize target agent with source agent\'s weights\n    target_agent = TD3Agent(target_env.state_dim, target_env.action_dim,\n                           target_env.max_action)\n\n    # Copy relevant network weights\n    target_agent.actor.load_state_dict(source_agent.actor.state_dict())\n\n    # Fine-tune on target environment\n    for episode in range(100):  # Fewer episodes for fine-tuning\n        state = target_env.reset()\n        done = False\n\n        while not done:\n            action = target_agent.select_action(state)\n            next_state, reward, done, _ = target_env.step(action)\n\n            target_agent.store_transition(state, action, next_state, reward, done)\n            target_agent.train()\n\n            state = next_state\n\n    return target_agent\n')),(0,i.yg)("h3",{id:"learning-objectives"},"Learning Objectives"),(0,i.yg)("p",null,"After studying this chapter, you should be able to:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Implement various deep RL algorithms for physical AI applications"),(0,i.yg)("li",{parentName:"ol"},"Design appropriate reward functions for different physical AI tasks"),(0,i.yg)("li",{parentName:"ol"},"Address safety considerations in RL for physical systems"),(0,i.yg)("li",{parentName:"ol"},"Apply model-based and hierarchical RL techniques"),(0,i.yg)("li",{parentName:"ol"},"Handle continuous state-action spaces in physical environments"),(0,i.yg)("li",{parentName:"ol"},"Evaluate and compare different RL approaches for physical AI")),(0,i.yg)("h3",{id:"hands-on-exercise"},"Hands-On Exercise"),(0,i.yg)("p",null,"Implement a simple RL agent to learn a basic manipulation task:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Create a simulation environment for a robotic arm"),(0,i.yg)("li",{parentName:"ol"},"Define a pick-and-place task"),(0,i.yg)("li",{parentName:"ol"},"Implement a PPO agent for the task"),(0,i.yg)("li",{parentName:"ol"},"Design an appropriate reward function"),(0,i.yg)("li",{parentName:"ol"},"Train the agent and analyze its performance"),(0,i.yg)("li",{parentName:"ol"},"Discuss challenges encountered and potential improvements")),(0,i.yg)("p",null,"This exercise will help you understand the practical aspects of applying RL to physical AI systems."))}_.isMDXComponent=!0}}]);