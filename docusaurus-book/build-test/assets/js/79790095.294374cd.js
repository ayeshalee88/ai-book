"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[593],{15680:(e,n,t)=>{t.d(n,{xA:()=>c,yg:()=>u});var o=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=o.createContext({}),p=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},c=function(e){var n=p(e.components);return o.createElement(l.Provider,{value:n},e.children)},m="mdxType",_={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},d=o.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),d=i,u=m["".concat(l,".").concat(d)]||m[d]||_[d]||r;return t?o.createElement(u,a(a({ref:n},c),{},{components:t})):o.createElement(u,a({ref:n},c))});function u(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,a=new Array(r);a[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:i,a[1]=s;for(var p=2;p<r;p++)a[p]=t[p];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},60452:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>_,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var o=t(58168),i=(t(96540),t(15680));const r={id:"open-source-projects",title:"Open Source Humanoid Robotics Projects",sidebar_label:"Open Source Projects"},a="Open Source Humanoid Robotics Projects",s={unversionedId:"case-studies/open-source-projects",id:"case-studies/open-source-projects",title:"Open Source Humanoid Robotics Projects",description:"Introduction",source:"@site/docs/case-studies/open-source-projects.md",sourceDirName:"case-studies",slug:"/case-studies/open-source-projects",permalink:"/ai-book/docs/case-studies/open-source-projects",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/case-studies/open-source-projects.md",tags:[],version:"current",frontMatter:{id:"open-source-projects",title:"Open Source Humanoid Robotics Projects",sidebar_label:"Open Source Projects"},sidebar:"tutorialSidebar",previous:{title:"Tesla Optimus",permalink:"/ai-book/docs/case-studies/tesla-optimus"},next:{title:"Simulation Environments",permalink:"/ai-book/docs/tutorials/simulation-environments"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Benefits of Open-Source Robotics",id:"benefits-of-open-source-robotics",level:3},{value:"Prominent Open-Source Humanoid Projects",id:"prominent-open-source-humanoid-projects",level:2},{value:"1. ROS (Robot Operating System) and Associated Projects",id:"1-ros-robot-operating-system-and-associated-projects",level:3},{value:"ROS-Industrial and Humanoid Extensions",id:"ros-industrial-and-humanoid-extensions",level:4},{value:"Gazebo Integration",id:"gazebo-integration",level:4},{value:"2. Poppy Project",id:"2-poppy-project",level:3},{value:"Technical Overview",id:"technical-overview",level:4},{value:"3. DARwIn-OP",id:"3-darwin-op",level:3},{value:"Key Features",id:"key-features",level:4},{value:"4. NAO Robot (Educational Focus)",id:"4-nao-robot-educational-focus",level:3},{value:"Community Contributions",id:"community-contributions",level:4},{value:"5. InMoov",id:"5-inmoov",level:3},{value:"Technical Implementation",id:"technical-implementation",level:4},{value:"Community Ecosystem and Resources",id:"community-ecosystem-and-resources",level:2},{value:"GitHub Organizations and Repositories",id:"github-organizations-and-repositories",level:3},{value:"Popular Repositories",id:"popular-repositories",level:4},{value:"Simulation Environments",id:"simulation-environments",level:3},{value:"Development Tools and Libraries",id:"development-tools-and-libraries",level:2},{value:"Commonly Used Libraries",id:"commonly-used-libraries",level:3},{value:"Educational Impact and Learning Resources",id:"educational-impact-and-learning-resources",level:2},{value:"Tutorials and Documentation",id:"tutorials-and-documentation",level:3},{value:"Beginner-Friendly Tutorials",id:"beginner-friendly-tutorials",level:4},{value:"Challenges and Future Directions",id:"challenges-and-future-directions",level:2},{value:"Technical Challenges",id:"technical-challenges",level:3},{value:"Community Growth Areas",id:"community-growth-areas",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Discussion Questions",id:"discussion-questions",level:2}],c={toc:p},m="wrapper";function _({components:e,...n}){return(0,i.yg)(m,(0,o.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"open-source-humanoid-robotics-projects"},"Open Source Humanoid Robotics Projects"),(0,i.yg)("h2",{id:"introduction"},"Introduction"),(0,i.yg)("p",null,"The open-source community has made significant contributions to humanoid robotics development, creating platforms that democratize access to advanced robotics research and development. These projects provide valuable learning opportunities, research platforms, and collaboration opportunities for the global robotics community."),(0,i.yg)("h3",{id:"benefits-of-open-source-robotics"},"Benefits of Open-Source Robotics"),(0,i.yg)("p",null,"Open-source humanoid robotics projects offer several advantages:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Accessibility"),": Lower barriers to entry for researchers and developers"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Collaboration"),": Global community contributions and improvements"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Transparency"),": Open development processes and code review"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Cost-Effectiveness"),": Reduced development costs through shared resources"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Rapid Innovation"),": Fast iteration through community feedback"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Educational Value"),": Learning opportunities for students and researchers")),(0,i.yg)("h2",{id:"prominent-open-source-humanoid-projects"},"Prominent Open-Source Humanoid Projects"),(0,i.yg)("h3",{id:"1-ros-robot-operating-system-and-associated-projects"},"1. ROS (Robot Operating System) and Associated Projects"),(0,i.yg)("p",null,"ROS has become the de facto standard for robotics software development, with numerous humanoid-specific packages and tools."),(0,i.yg)("h4",{id:"ros-industrial-and-humanoid-extensions"},"ROS-Industrial and Humanoid Extensions"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"# Example of using ROS for humanoid robot control\nimport rospy\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import PoseStamped\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport actionlib\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\n\nclass OpenSourceHumanoidController:\n    def __init__(self, robot_name=\"my_humanoid\"):\n        self.robot_name = robot_name\n        self.joint_names = [\n            'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',\n            'right_hip_joint', 'right_knee_joint', 'right_ankle_joint',\n            'left_shoulder_joint', 'left_elbow_joint', 'left_wrist_joint',\n            'right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint'\n        ]\n\n        # Publishers and subscribers\n        self.joint_state_sub = rospy.Subscriber(\n            f'/{robot_name}/joint_states', JointState, self.joint_state_callback\n        )\n        self.command_pub = rospy.Publisher(\n            f'/{robot_name}/joint_trajectory_controller/command',\n            JointTrajectory, queue_size=10\n        )\n\n        # Action client for trajectory execution\n        self.trajectory_client = actionlib.SimpleActionClient(\n            f'/{robot_name}/joint_trajectory_controller/follow_joint_trajectory',\n            FollowJointTrajectoryAction\n        )\n        self.trajectory_client.wait_for_server()\n\n        self.current_joint_states = JointState()\n\n    def joint_state_callback(self, msg):\n        \"\"\"Callback for joint state updates\"\"\"\n        self.current_joint_states = msg\n\n    def move_to_pose(self, joint_positions, duration=5.0):\n        \"\"\"Move robot to specified joint positions\"\"\"\n        trajectory = JointTrajectory()\n        trajectory.joint_names = self.joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = joint_positions\n        point.velocities = [0.0] * len(joint_positions)\n        point.accelerations = [0.0] * len(joint_positions)\n        point.time_from_start = rospy.Duration(duration)\n\n        trajectory.points.append(point)\n\n        # Send trajectory goal\n        goal = FollowJointTrajectoryGoal()\n        goal.trajectory = trajectory\n        goal.goal_time_tolerance = rospy.Duration(0.1)\n\n        self.trajectory_client.send_goal(goal)\n        self.trajectory_client.wait_for_result()\n\n    def walk_trajectory(self):\n        \"\"\"Generate and execute a walking trajectory\"\"\"\n        # Simplified walking pattern - in reality this would be much more complex\n        walk_sequence = [\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Standing\n            [0.1, -0.3, 0.2, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Step forward left\n            [0.0, 0.0, 0.0, 0.1, -0.3, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Step forward right\n        ]\n\n        for i, pose in enumerate(walk_sequence):\n            rospy.loginfo(f\"Executing step {i+1} of walk sequence\")\n            self.move_to_pose(pose, duration=2.0)\n            rospy.sleep(0.5)  # Small pause between steps\n")),(0,i.yg)("h4",{id:"gazebo-integration"},"Gazebo Integration"),(0,i.yg)("p",null,"Gazebo provides physics simulation for testing humanoid robots:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'\x3c!-- Example URDF snippet for a simple humanoid robot --\x3e\n<?xml version="1.0"?>\n<robot name="simple_humanoid">\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="5.0"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Head --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1 1 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.004" ixy="0" ixz="0" iyy="0.004" iyz="0" izz="0.004"/>\n    </inertial>\n  </link>\n\n  <joint name="neck_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.15" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="3"/>\n  </joint>\n\n  \x3c!-- Left leg --\x3e\n  <link name="left_hip">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1"/>\n      </material>\n    </visual>\n    <inertial>\n      <mass value="2.0"/>\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.005"/>\n    </inertial>\n  </link>\n\n  <joint name="left_hip_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_hip"/>\n    <origin xyz="-0.1 0 -0.15" rpy="0 0 0"/>\n    <axis xyz="1 0 0"/>\n    <limit lower="-1.57" upper="1.57" effort="200" velocity="2"/>\n  </joint>\n\n  \x3c!-- Additional links and joints would continue similarly --\x3e\n</robot>\n')),(0,i.yg)("h3",{id:"2-poppy-project"},"2. Poppy Project"),(0,i.yg)("p",null,"The Poppy Project is a prominent open-source humanoid robotics platform designed for research and education."),(0,i.yg)("h4",{id:"technical-overview"},"Technical Overview"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# Poppy-inspired humanoid controller\nimport pypot.robot\nfrom pypot.creatures import PoppyHumanoid\nimport numpy as np\n\nclass PoppyHumanoidController:\n    def __init__(self):\n        # Connect to the Poppy robot\n        self.robot = PoppyHumanoid(simulator=\'vrep\')  # or \'poppy-simu\' for simulation\n\n        # Set up motion primitives\n        self.setup_motion_primitives()\n\n    def setup_motion_primitives(self):\n        """Setup basic motion primitives for the robot"""\n        # Register basic movements\n        self.robot.attach_primitive(HeadMovement(self.robot), \'head_movement\')\n        self.robot.attach_primitive(ArmMovement(self.robot), \'arm_movement\')\n        self.robot.attach_primitive(WalkMovement(self.robot), \'walk_movement\')\n\n    def head_tracking(self, target_position):\n        """Move head to track a target"""\n        # Calculate head position to look at target\n        head_pan = np.arctan2(target_position[1], target_position[0])\n        head_tilt = np.arctan2(target_position[2],\n                              np.sqrt(target_position[0]**2 + target_position[1]**2))\n\n        # Limit head movement to safe ranges\n        head_pan = np.clip(head_pan, -1.0, 1.0)\n        head_tilt = np.clip(head_tilt, -0.5, 0.5)\n\n        # Move head joints\n        self.robot.head_y.rotate_to(head_tilt, duration=0.5)\n        self.robot.head_z.rotate_to(head_pan, duration=0.5)\n\n    def balance_control(self):\n        """Implement basic balance control"""\n        # Get accelerometer data\n        accelerometer_data = self.get_accelerometer_data()\n\n        # Calculate necessary adjustments\n        if abs(accelerometer_data[\'pitch\']) > 0.1:\n            # Adjust hip joints to maintain balance\n            adjustment = -accelerometer_data[\'pitch\'] * 10\n            self.robot.abs_y.goto_position(adjustment, duration=0.1)\n\n        if abs(accelerometer_data[\'roll\']) > 0.1:\n            # Adjust hip joints to maintain balance\n            adjustment = -accelerometer_data[\'roll\'] * 10\n            self.robot.abs_r.goto_position(adjustment, duration=0.1)\n\n    def get_accelerometer_data(self):\n        """Simulate getting accelerometer data"""\n        # In a real implementation, this would read from IMU sensors\n        return {\n            \'pitch\': np.random.normal(0, 0.01),  # Small random pitch\n            \'roll\': np.random.normal(0, 0.01),   # Small random roll\n            \'acc_x\': 0,\n            \'acc_y\': 0,\n            \'acc_z\': 9.81\n        }\n\nclass HeadMovement:\n    """Motion primitive for head movement"""\n    def __init__(self, robot):\n        self.robot = robot\n\n    def look_at(self, x, y, z):\n        """Move head to look at coordinates (x, y, z)"""\n        # Calculate angles to target\n        pan_angle = np.arctan2(y, x)\n        tilt_angle = np.arctan2(z, np.sqrt(x**2 + y**2))\n\n        # Apply limits\n        pan_angle = np.clip(pan_angle, -0.5, 0.5)\n        tilt_angle = np.clip(tilt_angle, -0.3, 0.3)\n\n        # Execute movement\n        self.robot.head_y.goto_position(tilt_angle, duration=0.5)\n        self.robot.head_z.goto_position(pan_angle, duration=0.5)\n')),(0,i.yg)("h3",{id:"3-darwin-op"},"3. DARwIn-OP"),(0,i.yg)("p",null,"DARwIn-OP (Dynamic Artificial Intelligence Robot with Open Platform) is an open-source humanoid robot platform developed for research and education."),(0,i.yg)("h4",{id:"key-features"},"Key Features"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# DARwIn-OP inspired controller\nimport cv2\nimport numpy as np\nfrom collections import deque\n\nclass DARwInController:\n    def __init__(self):\n        # Initialize robot joints and sensors\n        self.joint_ids = {\n            \'head_pan\': 1, \'head_tilt\': 2,\n            \'l_shoulder_pitch\': 3, \'l_shoulder_roll\': 4,\n            \'l_elbow\': 5, \'l_hip_yaw\': 6,\n            \'l_hip_roll\': 7, \'l_hip_pitch\': 8,\n            \'l_knee\': 9, \'l_ankle_pitch\': 10,\n            \'l_ankle_roll\': 11, \'r_hip_yaw\': 12,\n            \'r_hip_roll\': 13, \'r_hip_pitch\': 14,\n            \'r_knee\': 15, \'r_ankle_pitch\': 16,\n            \'r_ankle_roll\': 17, \'r_shoulder_pitch\': 18,\n            \'r_shoulder_roll\': 19, \'r_elbow\': 20\n        }\n\n        # Initialize camera\n        self.camera = cv2.VideoCapture(0)\n\n        # Walking pattern generator\n        self.walk_generator = WalkPatternGenerator()\n\n        # PID controllers for joints\n        self.pid_controllers = {\n            joint: PIDController(kp=10, ki=0.1, kd=0.05)\n            for joint in self.joint_ids.values()\n        }\n\n    def vision_processing(self):\n        """Process visual input for object detection and tracking"""\n        ret, frame = self.camera.read()\n        if not ret:\n            return None\n\n        # Convert to HSV for color-based detection\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color (example)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n\n        # Create mask\n        mask = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Find largest contour\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                # Calculate center of contour\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n                    return (cx, cy)  # Return center coordinates\n\n        return None\n\n    def track_object(self, object_position):\n        """Track object with head movement"""\n        if object_position is None:\n            return\n\n        frame_width = self.camera.get(cv2.CAP_PROP_FRAME_WIDTH)\n        frame_height = self.camera.get(cv2.CAP_PROP_FRAME_HEIGHT)\n\n        # Calculate error from center\n        center_x, center_y = frame_width / 2, frame_height / 2\n        error_x = object_position[0] - center_x\n        error_y = object_position[1] - center_y\n\n        # Convert pixel error to angle (simplified)\n        angle_x = error_x * 0.001  # Conversion factor\n        angle_y = error_y * 0.001  # Conversion factor\n\n        # Apply PID control to head joints\n        head_pan_cmd = self.pid_controllers[self.joint_ids[\'head_pan\']].update(angle_x, 0)\n        head_tilt_cmd = self.pid_controllers[self.joint_ids[\'head_tilt\']].update(angle_y, 0)\n\n        # Send commands to robot\n        self.set_joint_position(\'head_pan\', head_pan_cmd)\n        self.set_joint_position(\'head_tilt\', head_tilt_cmd)\n\n    def walking_controller(self, step_length=0.05, step_height=0.02):\n        """Generate walking pattern"""\n        # Generate walking trajectory\n        left_foot_traj, right_foot_traj = self.walk_generator.generate_walk_pattern(\n            step_length, step_height\n        )\n\n        # Execute walking pattern\n        for left_pos, right_pos in zip(left_foot_traj, right_foot_traj):\n            self.move_leg_to_position(\'left\', left_pos)\n            self.move_leg_to_position(\'right\', right_pos)\n            # Add balance adjustments between steps\n\n    def set_joint_position(self, joint_name, position):\n        """Set position of a specific joint"""\n        joint_id = self.joint_ids[joint_name]\n        # Send command to robot (implementation depends on communication protocol)\n        pass\n\n    def move_leg_to_position(self, leg, position):\n        """Move leg to specified position using inverse kinematics"""\n        # Calculate inverse kinematics for leg\n        joint_angles = self.inverse_kinematics_leg(leg, position)\n\n        # Set joint positions\n        if leg == \'left\':\n            self.set_joint_position(\'l_hip_pitch\', joint_angles[0])\n            self.set_joint_position(\'l_knee\', joint_angles[1])\n            self.set_joint_position(\'l_ankle_pitch\', joint_angles[2])\n        else:  # right leg\n            self.set_joint_position(\'r_hip_pitch\', joint_angles[0])\n            self.set_joint_position(\'r_knee\', joint_angles[1])\n            self.set_joint_position(\'r_ankle_pitch\', joint_angles[2])\n\n    def inverse_kinematics_leg(self, leg, target_position):\n        """Calculate inverse kinematics for leg"""\n        # Simplified 2D inverse kinematics for leg\n        x, y = target_position[:2]  # Use x,y coordinates\n\n        # Calculate leg length\n        l1 = 0.1  # thigh length (example)\n        l2 = 0.1  # shin length (example)\n\n        # Calculate distance to target\n        r = np.sqrt(x**2 + y**2)\n\n        # Check if target is reachable\n        if r > l1 + l2:\n            # Target is outside workspace, extend fully toward target\n            angle = np.arctan2(y, x)\n            return [angle, 0, 0]  # Simplified return\n        elif r < abs(l1 - l2):\n            # Target is inside workspace but unreachable\n            return [0, 0, 0]  # Return default position\n\n        # Calculate joint angles using law of cosines\n        cos_angle2 = (r**2 - l1**2 - l2**2) / (2 * l1 * l2)\n        angle2 = np.arccos(np.clip(cos_angle2, -1, 1))\n\n        k1 = l1 + l2 * np.cos(angle2)\n        k2 = l2 * np.sin(angle2)\n\n        angle1 = np.arctan2(y, x) - np.arctan2(k2, k1)\n\n        # Calculate ankle angle for proper foot orientation\n        angle3 = -(angle1 + angle2)  # Simplified\n\n        return [angle1, angle2, angle3]\n\nclass WalkPatternGenerator:\n    """Generate walking patterns for humanoid robots"""\n    def __init__(self):\n        self.step_sequence = deque()\n\n    def generate_walk_pattern(self, step_length=0.05, step_height=0.02, steps=10):\n        """Generate a sequence of foot positions for walking"""\n        left_trajectory = []\n        right_trajectory = []\n\n        # Generate walking pattern for specified number of steps\n        for i in range(steps):\n            # Left foot trajectory (when right foot is stance)\n            left_x = step_length * i if i % 2 == 0 else step_length * (i + 1)\n            left_y = 0.05 if i % 2 == 0 else -0.05  # Alternating feet\n            left_z = step_height if i % 2 == 1 else 0  # Swing phase\n\n            # Right foot trajectory (when left foot is stance)\n            right_x = step_length * (i + 1) if i % 2 == 1 else step_length * i\n            right_y = -0.05 if i % 2 == 0 else 0.05  # Alternating feet\n            right_z = 0 if i % 2 == 1 else step_height  # Swing phase\n\n            left_trajectory.append([left_x, left_y, left_z])\n            right_trajectory.append([right_x, right_y, right_z])\n\n        return left_trajectory, right_trajectory\n\nclass PIDController:\n    """Simple PID controller for joint control"""\n    def __init__(self, kp, ki, kd):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.prev_error = 0\n        self.integral = 0\n        self.dt = 0.01  # Time step\n\n    def update(self, setpoint, measurement):\n        """Update PID controller and return control output"""\n        error = setpoint - measurement\n\n        self.integral += error * self.dt\n        derivative = (error - self.prev_error) / self.dt\n\n        output = (self.kp * error +\n                 self.ki * self.integral +\n                 self.kd * derivative)\n\n        self.prev_error = error\n        return output\n')),(0,i.yg)("h3",{id:"4-nao-robot-educational-focus"},"4. NAO Robot (Educational Focus)"),(0,i.yg)("p",null,"While NAO is commercial, it has a strong open-source community and educational resources."),(0,i.yg)("h4",{id:"community-contributions"},"Community Contributions"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# NAO-inspired behavior controller\nimport qi  # Aldebaran\'s QiMessaging framework\nimport numpy as np\nfrom naoqi import ALProxy\n\nclass CommunityNAOController:\n    def __init__(self, robot_ip="127.0.0.1", robot_port=9559):\n        # Connect to robot\n        self.session = qi.Session()\n        try:\n            self.session.connect("tcp://" + robot_ip + ":" + str(robot_port))\n        except RuntimeError:\n            print("Could not connect to robot")\n            return\n\n        # Initialize proxies\n        self.motion_proxy = self.session.service("ALMotion")\n        self.vision_proxy = self.session.service("ALVideoDevice")\n        self.audio_proxy = self.session.service("ALTextToSpeech")\n        self.speech_proxy = self.session.service("ALSpeechRecognition")\n        self.memory_proxy = self.session.service("ALMemory")\n\n        # Initialize behavior engine\n        self.behavior_manager = self.session.service("ALBehaviorManager")\n\n    def look_at_person(self, x, y, z):\n        """Make robot look at a person at coordinates (x, y, z)"""\n        # Calculate head angles to look at target\n        head_yaw = np.arctan2(y, x)\n        head_pitch = np.arctan2(z, np.sqrt(x**2 + y**2))\n\n        # Set head stiffness\n        self.motion_proxy.stiffnessInterpolation("Head", 1.0, 1.0)\n\n        # Move head to look at target\n        names = ["HeadYaw", "HeadPitch"]\n        angles = [head_yaw, head_pitch]\n        fractionMaxSpeed = 0.2\n\n        self.motion_proxy.angleInterpolation(names, angles, fractionMaxSpeed, True)\n\n    def detect_faces(self):\n        """Detect faces using camera and return positions"""\n        # Subscribe to camera\n        camera_id = 0\n        resolution = 2  # VGA\n        color_space = 11  # RGB\n        fps = 5\n\n        video_client = self.vision_proxy.subscribe("python_client",\n                                                  resolution, color_space, fps)\n\n        # Get image from camera\n        image = self.vision_proxy.getImageRemote(video_client)\n\n        if image is None:\n            self.vision_proxy.unsubscribe(video_client)\n            return []\n\n        # Process image to detect faces (simplified)\n        # In reality, this would use OpenCV or similar\n        faces = self.process_face_detection(image)\n\n        # Unsubscribe from camera\n        self.vision_proxy.unsubscribe(video_client)\n\n        return faces\n\n    def process_face_detection(self, image_data):\n        """Process image data to detect faces"""\n        # This is a simplified placeholder\n        # Real implementation would use OpenCV or similar\n        # For example: cascade classifiers, deep learning models, etc.\n        return []  # Return list of face positions\n\n    def interactive_behavior(self):\n        """Run interactive behavior with face detection and tracking"""\n        while True:\n            # Detect faces in view\n            faces = self.detect_faces()\n\n            if faces:\n                # Look at the first detected face\n                x, y, z = faces[0]  # Assume first face is target\n                self.look_at_person(x, y, z)\n\n                # Say hello if person is close enough\n                if np.sqrt(x**2 + y**2 + z**2) < 1.0:  # Within 1 meter\n                    self.audio_proxy.say("Hello! I see you!")\n\n            # Small delay to prevent overwhelming the system\n            time.sleep(0.1)\n\n    def dance_behavior(self):\n        """Community-created dance behavior"""\n        # Define dance sequence\n        dance_sequence = [\n            {"joint": "RShoulderPitch", "angle": -1.0, "duration": 0.5},\n            {"joint": "LShoulderPitch", "angle": -1.0, "duration": 0.5},\n            {"joint": "RShoulderRoll", "angle": 0.5, "duration": 0.3},\n            {"joint": "LShoulderRoll", "angle": -0.5, "duration": 0.3},\n            {"joint": "RHipPitch", "angle": -0.2, "duration": 0.4},\n            {"joint": "LHipPitch", "angle": -0.2, "duration": 0.4},\n        ]\n\n        # Execute dance sequence\n        for move in dance_sequence:\n            self.motion_proxy.angleInterpolation(\n                move["joint"],\n                move["angle"],\n                move["duration"],\n                True\n            )\n\n    def learn_behavior(self, demonstration_data):\n        """Learn new behavior from demonstration"""\n        # This would implement learning from demonstration techniques\n        # Community projects often focus on programming by demonstration\n        pass\n')),(0,i.yg)("h3",{id:"5-inmoov"},"5. InMoov"),(0,i.yg)("p",null,"InMoov is a completely 3D-printable humanoid robot designed by Gael Langevin."),(0,i.yg)("h4",{id:"technical-implementation"},"Technical Implementation"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"# InMoov-inspired controller\nimport time\nimport threading\nfrom collections import defaultdict\n\nclass InMoovController:\n    def __init__(self):\n        # Initialize servo controllers\n        self.servos = {\n            'head_yaw': ServoController(pin=1),\n            'head_pitch': ServoController(pin=2),\n            'head_roll': ServoController(pin=3),\n            'eye_left_right': ServoController(pin=4),\n            'eye_up_down': ServoController(pin=5),\n            'jaw': ServoController(pin=6),\n            'left_arm_shoulder': ServoController(pin=7),\n            'left_arm_bicep': ServoController(pin=8),\n            'left_arm_elbow': ServoController(pin=9),\n            'left_arm_forearm': ServoController(pin=10),\n            'left_hand_wrist': ServoController(pin=11),\n            'left_hand_grip': ServoController(pin=12),\n            'right_arm_shoulder': ServoController(pin=13),\n            'right_arm_bicep': ServoController(pin=14),\n            'right_arm_elbow': ServoController(pin=15),\n            'right_arm_forearm': ServoController(pin=16),\n            'right_hand_wrist': ServoController(pin=17),\n            'right_hand_grip': ServoController(pin=18),\n        }\n\n        # Initialize sensors\n        self.sensors = {\n            'ultrasonic_front': UltrasonicSensor(pin=1),\n            'ultrasonic_left': UltrasonicSensor(pin=2),\n            'ultrasonic_right': UltrasonicSensor(pin=3),\n            'touch_sensors': [TouchSensor(pin=i) for i in range(4, 8)]\n        }\n\n        # Initialize behaviors\n        self.behaviors = {}\n        self.active_behavior = None\n        self.behavior_thread = None\n\n    def initialize_robot(self):\n        \"\"\"Initialize all servos to safe positions\"\"\"\n        for servo_name, servo in self.servos.items():\n            servo.set_position(90)  # Center position\n            time.sleep(0.1)  # Small delay between initializations\n\n    def look_around(self):\n        \"\"\"Simple behavior to look around\"\"\"\n        # Scan left to right\n        for angle in range(60, 120, 5):\n            self.servos['head_yaw'].set_position(angle)\n            time.sleep(0.1)\n\n        # Scan right to left\n        for angle in range(120, 60, -5):\n            self.servos['head_yaw'].set_position(angle)\n            time.sleep(0.1)\n\n        # Look up and down\n        for angle in range(70, 110, 5):\n            self.servos['head_pitch'].set_position(angle)\n            time.sleep(0.1)\n\n        for angle in range(110, 70, -5):\n            self.servos['head_pitch'].set_position(angle)\n            time.sleep(0.1)\n\n    def wave_hello(self):\n        \"\"\"Wave gesture with right arm\"\"\"\n        # Move right arm to wave position\n        self.servos['right_arm_shoulder'].set_position(45)\n        self.servos['right_arm_elbow'].set_position(135)\n        self.servos['right_arm_forearm'].set_position(90)\n\n        time.sleep(1)\n\n        # Wave motion\n        for _ in range(3):\n            self.servos['right_arm_forearm'].set_position(60)\n            time.sleep(0.3)\n            self.servos['right_arm_forearm'].set_position(120)\n            time.sleep(0.3)\n\n        # Return to neutral position\n        self.servos['right_arm_shoulder'].set_position(90)\n        self.servos['right_arm_elbow'].set_position(90)\n        self.servos['right_arm_forearm'].set_position(90)\n\n    def avoid_obstacles(self):\n        \"\"\"Simple obstacle avoidance using ultrasonic sensors\"\"\"\n        while True:\n            front_distance = self.sensors['ultrasonic_front'].read()\n            left_distance = self.sensors['ultrasonic_left'].read()\n            right_distance = self.sensors['ultrasonic_right'].read()\n\n            if front_distance < 20:  # Obstacle detected in front\n                if left_distance > right_distance:\n                    # Turn left\n                    self.servos['head_yaw'].set_position(60)\n                else:\n                    # Turn right\n                    self.servos['head_yaw'].set_position(120)\n            else:\n                # Look straight ahead\n                self.servos['head_yaw'].set_position(90)\n\n            time.sleep(0.1)  # Check every 100ms\n\n    def autonomous_behavior(self):\n        \"\"\"Run autonomous behavior sequence\"\"\"\n        behaviors = [self.look_around, self.wave_hello]\n\n        while True:\n            for behavior in behaviors:\n                if self.active_behavior is None:\n                    behavior()\n                else:\n                    time.sleep(1)  # Wait if another behavior is active\n\n    def start_autonomous_mode(self):\n        \"\"\"Start autonomous behavior in background thread\"\"\"\n        self.behavior_thread = threading.Thread(target=self.autonomous_behavior)\n        self.behavior_thread.daemon = True\n        self.behavior_thread.start()\n\nclass ServoController:\n    \"\"\"Controller for a single servo motor\"\"\"\n    def __init__(self, pin, min_pulse=500, max_pulse=2500):\n        self.pin = pin\n        self.min_pulse = min_pulse\n        self.max_pulse = max_pulse\n        self.current_position = 90  # Default center position\n\n        # Initialize PWM (in real implementation)\n        # self.pwm = Adafruit_PCA9685.PCA9685()\n        # self.pwm.set_pwm_freq(60)\n\n    def set_position(self, angle):\n        \"\"\"Set servo to specific angle (0-180 degrees)\"\"\"\n        # Convert angle to pulse width\n        pulse_width = self.min_pulse + (angle / 180.0) * (self.max_pulse - self.min_pulse)\n\n        # In real implementation:\n        # self.pwm.set_pwm(self.pin, 0, int(pulse_width / 1000 * 60))\n\n        self.current_position = angle\n\n    def get_position(self):\n        \"\"\"Get current servo position\"\"\"\n        return self.current_position\n\nclass UltrasonicSensor:\n    \"\"\"Ultrasonic distance sensor (HC-SR04)\"\"\"\n    def __init__(self, pin):\n        self.trigger_pin = pin\n        self.echo_pin = pin + 1  # Typically adjacent pin\n\n    def read(self):\n        \"\"\"Read distance in centimeters\"\"\"\n        # In real implementation:\n        # Send trigger pulse\n        # Measure echo duration\n        # Calculate distance = duration * speed_of_sound / 2\n        return 100  # Placeholder value\n\nclass TouchSensor:\n    \"\"\"Simple touch sensor\"\"\"\n    def __init__(self, pin):\n        self.pin = pin\n        self.state = False\n\n    def read(self):\n        \"\"\"Read touch sensor state\"\"\"\n        # In real implementation, read digital input\n        return self.state\n")),(0,i.yg)("h2",{id:"community-ecosystem-and-resources"},"Community Ecosystem and Resources"),(0,i.yg)("h3",{id:"github-organizations-and-repositories"},"GitHub Organizations and Repositories"),(0,i.yg)("p",null,"The open-source robotics community has created numerous valuable resources:"),(0,i.yg)("h4",{id:"popular-repositories"},"Popular Repositories"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"# Example of setting up a common robotics development environment\n# This would be part of a community tutorial\n\n# Install ROS (Robot Operating System)\nsudo apt update\nsudo apt install ros-noetic-desktop-full\nsource /opt/ros/noetic/setup.bash\n\n# Install common robotics libraries\nsudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n\n# Initialize rosdep\nsudo rosdep init\nrosdep update\n\n# Create a catkin workspace\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/\ncatkin_make\n\n# Source the workspace\nsource devel/setup.bash\n\n# Clone popular robotics repositories\ncd ~/catkin_ws/src\ngit clone https://github.com/ROBOTIS-GIT/DynamixelSDK.git\ngit clone https://github.com/ahundt/robotics_setup_scrips.git\ngit clone https://github.com/atenpas/handeye_calibration.git\n\n# Build the workspace\ncd ~/catkin_ws\ncatkin_make\n")),(0,i.yg)("h3",{id:"simulation-environments"},"Simulation Environments"),(0,i.yg)("p",null,"Community projects often leverage open-source simulation:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# Example using PyBullet for humanoid simulation\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport time\n\nclass OpenSourceHumanoidSimulator:\n    def __init__(self, urdf_path="humanoid.urdf", gui=True):\n        # Connect to physics server\n        if gui:\n            self.physics_client = p.connect(p.GUI)\n        else:\n            self.physics_client = p.connect(p.DIRECT)\n\n        # Set gravity\n        p.setGravity(0, 0, -9.81)\n\n        # Load plane\n        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n        self.plane_id = p.loadURDF("plane.urdf")\n\n        # Load humanoid robot\n        self.robot_id = p.loadURDF(urdf_path, [0, 0, 1])  # Start 1m above ground\n\n        # Get joint information\n        self.joint_info = {}\n        self.motor_joints = []\n        for i in range(p.getNumJoints(self.robot_id)):\n            joint_info = p.getJointInfo(self.robot_id, i)\n            joint_name = joint_info[1].decode(\'utf-8\')\n            joint_type = joint_info[2]\n\n            if joint_type == p.JOINT_REVOLUTE or joint_type == p.JOINT_PRISMATIC:\n                self.motor_joints.append(i)\n                self.joint_info[i] = {\n                    \'name\': joint_name,\n                    \'type\': joint_type,\n                    \'lower_limit\': joint_info[8],\n                    \'upper_limit\': joint_info[9],\n                    \'max_force\': joint_info[10],\n                    \'max_velocity\': joint_info[11]\n                }\n\n    def set_joint_positions(self, joint_positions):\n        """Set target positions for all joints"""\n        for i, joint_id in enumerate(self.motor_joints):\n            p.setJointMotorControl2(\n                bodyIndex=self.robot_id,\n                jointIndex=joint_id,\n                controlMode=p.POSITION_CONTROL,\n                targetPosition=joint_positions[i],\n                force=self.joint_info[joint_id][\'max_force\']\n            )\n\n    def get_joint_states(self):\n        """Get current joint positions and velocities"""\n        joint_states = []\n        for joint_id in self.motor_joints:\n            state = p.getJointState(self.robot_id, joint_id)\n            joint_states.append({\n                \'position\': state[0],\n                \'velocity\': state[1],\n                \'force\': state[3]\n            })\n        return joint_states\n\n    def get_robot_position(self):\n        """Get robot base position and orientation"""\n        pos, orn = p.getBasePositionAndOrientation(self.robot_id)\n        return np.array(pos), np.array(orn)\n\n    def step_simulation(self):\n        """Step the simulation forward"""\n        p.stepSimulation()\n        time.sleep(1./240.)  # Real-time simulation at 240 Hz\n\n    def run_walking_simulation(self, steps=1000):\n        """Run a simple walking simulation"""\n        for step in range(steps):\n            # Simple walking pattern (simplified)\n            t = step / 100.0  # Time variable\n\n            # Generate simple walking joint angles\n            left_leg_angle = 0.2 * np.sin(t * 2)\n            right_leg_angle = 0.2 * np.sin(t * 2 + np.pi)\n\n            # Set joint positions (simplified - real walking would be more complex)\n            joint_positions = [0] * len(self.motor_joints)\n            # Set some basic walking motion\n            if len(joint_positions) > 2:\n                joint_positions[0] = left_leg_angle  # Left hip\n                joint_positions[1] = -left_leg_angle * 0.5  # Left knee compensation\n                joint_positions[2] = right_leg_angle  # Right hip\n                joint_positions[3] = -right_leg_angle * 0.5  # Right knee compensation\n\n            self.set_joint_positions(joint_positions)\n            self.step_simulation()\n\n    def disconnect(self):\n        """Disconnect from physics server"""\n        p.disconnect(self.physics_client)\n\n# Usage example\nif __name__ == "__main__":\n    simulator = OpenSourceHumanoidSimulator(gui=True)\n\n    # Run walking simulation\n    simulator.run_walking_simulation(steps=2000)\n\n    # Clean up\n    simulator.disconnect()\n')),(0,i.yg)("h2",{id:"development-tools-and-libraries"},"Development Tools and Libraries"),(0,i.yg)("h3",{id:"commonly-used-libraries"},"Commonly Used Libraries"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# Example of using common open-source robotics libraries together\nimport numpy as np\nimport transforms3d as t3d  # For 3D transformations\nfrom scipy.spatial.transform import Rotation as R\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nclass HumanoidDevelopmentTools:\n    """Collection of tools for humanoid robot development"""\n\n    @staticmethod\n    def forward_kinematics(joint_angles, dh_parameters):\n        """\n        Calculate forward kinematics using Denavit-Hartenberg parameters\n\n        Args:\n            joint_angles: List of joint angles\n            dh_parameters: List of [a, alpha, d, theta_offset] for each joint\n        """\n        transformation = np.eye(4)  # Identity matrix\n\n        for i, (angle, dh) in enumerate(zip(joint_angles, dh_parameters)):\n            a, alpha, d, theta_offset = dh\n            theta = angle + theta_offset\n\n            # DH transformation matrix\n            cos_t, sin_t = np.cos(theta), np.sin(theta)\n            cos_a, sin_a = np.cos(alpha), np.sin(alpha)\n\n            joint_transform = np.array([\n                [cos_t, -sin_t * cos_a, sin_t * sin_a, a * cos_t],\n                [sin_t, cos_t * cos_a, -cos_t * sin_a, a * sin_t],\n                [0, sin_a, cos_a, d],\n                [0, 0, 0, 1]\n            ])\n\n            transformation = transformation @ joint_transform\n\n        return transformation\n\n    @staticmethod\n    def inverse_kinematics_2dof(x, y, l1, l2):\n        """\n        Calculate inverse kinematics for 2-DOF planar arm\n\n        Args:\n            x, y: Target position\n            l1, l2: Link lengths\n        """\n        r = np.sqrt(x**2 + y**2)\n\n        # Check if position is reachable\n        if r > l1 + l2:\n            # Position is outside workspace\n            return None\n        elif r < abs(l1 - l2):\n            # Position is inside workspace but unreachable\n            return None\n\n        # Calculate second joint angle\n        cos_theta2 = (r**2 - l1**2 - l2**2) / (2 * l1 * l2)\n        theta2 = np.arccos(np.clip(cos_theta2, -1, 1))\n\n        # Calculate first joint angle\n        k1 = l1 + l2 * np.cos(theta2)\n        k2 = l2 * np.sin(theta2)\n\n        theta1 = np.arctan2(y, x) - np.arctan2(k2, k1)\n\n        return [theta1, theta2]\n\n    @staticmethod\n    def trajectory_generation(start_pos, end_pos, duration, dt=0.01):\n        """\n        Generate smooth trajectory using quintic polynomial interpolation\n        """\n        t = np.arange(0, duration, dt)\n        trajectory = []\n\n        for ti in t:\n            # Quintic polynomial coefficients for smooth start/end\n            tau = ti / duration  # Normalized time [0, 1]\n\n            # 5th order polynomial: a5*t^5 + a4*t^4 + a3*t^3 + a2*t^2 + a1*t + a0\n            # With boundary conditions: pos(0)=start, pos(1)=end, vel(0)=0, vel(1)=0, acc(0)=0, acc(1)=0\n            f = 6*tau**5 - 15*tau**4 + 10*tau**3  # Shape function\n\n            pos = start_pos + f * (end_pos - start_pos)\n            trajectory.append(pos)\n\n        return np.array(trajectory)\n\n    @staticmethod\n    def balance_controller(com_position, com_velocity, target_com_position, dt=0.01):\n        """\n        Simple inverted pendulum balance controller\n        """\n        # Linear inverted pendulum model: z_ddot = g/h * (z - zmp)\n        pendulum_height = 0.8  # Height of COM above ground\n        gravity = 9.81\n\n        # Calculate desired ZMP to move COM to target\n        zmp_desired = target_com_position - (pendulum_height / gravity) * (\n            100 * (target_com_position - com_position) +  # Position feedback\n            10 * (0 - com_velocity)  # Velocity feedback (desired velocity = 0)\n        )\n\n        return zmp_desired\n\n    @staticmethod\n    def visualize_robot_skeleton(joint_positions, connections):\n        """\n        Visualize robot skeleton in 3D\n\n        Args:\n            joint_positions: Dictionary of joint names to [x, y, z] positions\n            connections: List of (joint1, joint2) tuples indicating connections\n        """\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\'3d\')\n\n        # Plot joints\n        for joint_name, pos in joint_positions.items():\n            ax.scatter(pos[0], pos[1], pos[2], c=\'red\', s=50)\n            ax.text(pos[0], pos[1], pos[2], joint_name, fontsize=8)\n\n        # Plot connections\n        for joint1, joint2 in connections:\n            if joint1 in joint_positions and joint2 in joint_positions:\n                pos1 = joint_positions[joint1]\n                pos2 = joint_positions[joint2]\n                ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], [pos1[2], pos2[2]], \'b-\')\n\n        ax.set_xlabel(\'X\')\n        ax.set_ylabel(\'Y\')\n        ax.set_zlabel(\'Z\')\n        ax.set_title(\'Robot Skeleton Visualization\')\n\n        plt.show()\n\n# Example usage of development tools\ndef example_usage():\n    tools = HumanoidDevelopmentTools()\n\n    # Example: Calculate forward kinematics\n    dh_params = [\n        [0.1, 0, 0, 0],      # Joint 1\n        [0.1, 0, 0, 0],      # Joint 2\n        [0.05, 0, 0, 0],     # Joint 3\n    ]\n    joint_angles = [0.1, 0.2, 0.3]\n    end_effector_pose = tools.forward_kinematics(joint_angles, dh_params)\n    print(f"End effector pose:\\n{end_effector_pose}")\n\n    # Example: Generate trajectory\n    start_pos = np.array([0, 0, 0])\n    end_pos = np.array([1, 1, 0])\n    trajectory = tools.trajectory_generation(start_pos, end_pos, duration=2.0)\n    print(f"Generated trajectory with {len(trajectory)} points")\n\n    # Example: Balance control\n    com_pos = np.array([0.1, 0.0, 0.8])\n    com_vel = np.array([0.01, 0.0, 0.0])\n    target_pos = np.array([0.0, 0.0, 0.8])\n    zmp = tools.balance_controller(com_pos[:2], com_vel[:2], target_pos[:2])\n    print(f"Desired ZMP for balance: {zmp}")\n\nif __name__ == "__main__":\n    example_usage()\n')),(0,i.yg)("h2",{id:"educational-impact-and-learning-resources"},"Educational Impact and Learning Resources"),(0,i.yg)("h3",{id:"tutorials-and-documentation"},"Tutorials and Documentation"),(0,i.yg)("p",null,"The open-source community provides extensive learning resources:"),(0,i.yg)("h4",{id:"beginner-friendly-tutorials"},"Beginner-Friendly Tutorials"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'# Educational example: Simple balance controller for learning\nclass EducationalBalanceController:\n    """\n    Educational balance controller demonstrating basic principles\n    of humanoid balance control\n    """\n\n    def __init__(self, robot_mass=30.0, com_height=0.5, gravity=9.81):\n        self.mass = robot_mass\n        self.com_height = com_height\n        self.gravity = gravity\n\n        # PID controller parameters\n        self.kp = 50.0  # Proportional gain\n        self.ki = 1.0   # Integral gain\n        self.kd = 10.0  # Derivative gain\n\n        # Error integration for PID\n        self.integral_error = 0\n        self.previous_error = 0\n        self.dt = 0.01  # Time step (100 Hz)\n\n    def update(self, measured_com_x, desired_com_x, measured_com_vel_x):\n        """\n        Update balance controller and return corrective force\n        """\n        # Calculate error\n        error = desired_com_x - measured_com_x\n\n        # Update integral\n        self.integral_error += error * self.dt\n\n        # Calculate derivative\n        derivative = (error - self.previous_error) / self.dt\n\n        # PID control\n        corrective_force = (self.kp * error +\n                           self.ki * self.integral_error +\n                           self.kd * derivative)\n\n        # Account for velocity feedback (damping)\n        velocity_feedback = -10.0 * measured_com_vel_x\n        total_force = corrective_force + velocity_feedback\n\n        # Update previous error\n        self.previous_error = error\n\n        return total_force\n\n    def calculate_zmp(self, com_pos, com_acc):\n        """\n        Calculate Zero Moment Point from Center of Mass information\n        """\n        # ZMP_x = CoM_x - h/g * CoM_acc_x\n        zmp_x = com_pos[0] - (self.com_height / self.gravity) * com_acc[0]\n        zmp_y = com_pos[1] - (self.com_height / self.gravity) * com_acc[1]\n\n        return np.array([zmp_x, zmp_y])\n\n# Educational example: Simple walking pattern generator\nclass EducationalWalkGenerator:\n    """\n    Educational walking pattern generator\n    Demonstrates basic concepts of bipedal walking\n    """\n\n    def __init__(self):\n        self.step_length = 0.3  # meters\n        self.step_height = 0.05  # meters\n        self.step_duration = 1.0  # seconds per step\n        self.com_height = 0.8  # meters\n\n    def generate_simple_walk(self, num_steps=4):\n        """\n        Generate a simple walking pattern\n        """\n        walk_pattern = []\n\n        for step in range(num_steps):\n            # Each step consists of double support, single support, double support\n            step_phase = step % 2  # 0 for left foot support, 1 for right foot support\n\n            # Generate trajectory for this step\n            step_trajectory = self.generate_step_trajectory(step_phase, step)\n            walk_pattern.extend(step_trajectory)\n\n        return walk_pattern\n\n    def generate_step_trajectory(self, support_foot, step_num):\n        """\n        Generate trajectory for a single step\n        """\n        trajectory = []\n\n        # Define keyframes for the step\n        # This is a simplified representation\n        if support_foot == 0:  # Left foot is supporting\n            # Move right foot forward\n            for t in np.linspace(0, self.step_duration, 50):\n                # Swing right foot from back to front\n                foot_x = -0.1 + (self.step_length + 0.2) * (t / self.step_duration)\n                foot_y = 0.1 if t < self.step_duration/2 else -0.1  # Step width\n                foot_z = self.step_height * np.sin(np.pi * t / self.step_duration)  # Arc motion\n\n                trajectory.append({\n                    \'time\': step_num * self.step_duration + t,\n                    \'left_foot\': [step_num * self.step_length, -0.1, 0],\n                    \'right_foot\': [foot_x, foot_y, foot_z],\n                    \'com_trajectory\': self.calculate_com_trajectory(t, step_num)\n                })\n        else:  # Right foot is supporting\n            # Move left foot forward\n            for t in np.linspace(0, self.step_duration, 50):\n                # Swing left foot from back to front\n                foot_x = -0.1 + (self.step_length + 0.2) * (t / self.step_duration)\n                foot_y = -0.1 if t < self.step_duration/2 else 0.1  # Step width\n                foot_z = self.step_height * np.sin(np.pi * t / self.step_duration)  # Arc motion\n\n                trajectory.append({\n                    \'time\': step_num * self.step_duration + t,\n                    \'left_foot\': [foot_x, foot_y, foot_z],\n                    \'right_foot\': [step_num * self.step_length, 0.1, 0],\n                    \'com_trajectory\': self.calculate_com_trajectory(t, step_num)\n                })\n\n        return trajectory\n\n    def calculate_com_trajectory(self, time_in_step, step_num):\n        """\n        Calculate Center of Mass trajectory during walking\n        """\n        # Simplified CoM trajectory that moves forward smoothly\n        com_x = step_num * self.step_length + 0.5 * self.step_length * (time_in_step / self.step_duration)\n        com_y = 0.0  # Keep CoM centered laterally\n        com_z = self.com_height  # Keep CoM at constant height\n\n        return [com_x, com_y, com_z]\n')),(0,i.yg)("h2",{id:"challenges-and-future-directions"},"Challenges and Future Directions"),(0,i.yg)("h3",{id:"technical-challenges"},"Technical Challenges"),(0,i.yg)("p",null,"Open-source humanoid robotics faces several challenges:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Hardware Integration"),": Connecting diverse hardware components"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Real-time Performance"),": Meeting strict timing requirements"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Safety"),": Ensuring safe operation around humans"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Cost"),": Balancing capability with affordability"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Standardization"),": Different interfaces and protocols")),(0,i.yg)("h3",{id:"community-growth-areas"},"Community Growth Areas"),(0,i.yg)("p",null,"Future development focuses on:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"AI Integration"),": Machine learning and neural networks"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Human-Robot Interaction"),": Better social capabilities"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Autonomous Learning"),": Robots that improve through experience"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Modular Design"),": Easier customization and upgrades"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Simulation-to-Reality"),": Better transfer from simulation to real robots")),(0,i.yg)("h2",{id:"learning-objectives"},"Learning Objectives"),(0,i.yg)("p",null,"After studying this case, you should be able to:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Identify major open-source humanoid robotics projects"),(0,i.yg)("li",{parentName:"ol"},"Understand the benefits and challenges of open-source development"),(0,i.yg)("li",{parentName:"ol"},"Implement basic control algorithms for humanoid robots"),(0,i.yg)("li",{parentName:"ol"},"Use common simulation and development tools"),(0,i.yg)("li",{parentName:"ol"},"Appreciate the educational value of open-source projects"),(0,i.yg)("li",{parentName:"ol"},"Evaluate the impact of community-driven development")),(0,i.yg)("h2",{id:"discussion-questions"},"Discussion Questions"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"How do open-source robotics projects democratize access to advanced robotics research?"),(0,i.yg)("li",{parentName:"ol"},"What are the main challenges in developing humanoid robots with open-source tools?"),(0,i.yg)("li",{parentName:"ol"},"How might the open-source approach accelerate innovation in humanoid robotics?"),(0,i.yg)("li",{parentName:"ol"},"What role do simulation environments play in open-source robotics development?"),(0,i.yg)("li",{parentName:"ol"},"How can the open-source community address safety concerns in humanoid robotics?")),(0,i.yg)("p",null,"The open-source humanoid robotics community continues to make significant contributions to the field, providing accessible platforms for research, education, and innovation. These projects serve as valuable learning tools and development platforms for the next generation of humanoid robots."))}_.isMDXComponent=!0}}]);