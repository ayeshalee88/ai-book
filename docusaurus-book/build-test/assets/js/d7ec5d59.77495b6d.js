"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[733],{15680:(e,n,o)=>{o.d(n,{xA:()=>p,yg:()=>u});var t=o(96540);function a(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function s(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),o.push.apply(o,t)}return o}function r(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?s(Object(o),!0).forEach(function(n){a(e,n,o[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):s(Object(o)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))})}return e}function i(e,n){if(null==e)return{};var o,t,a=function(e,n){if(null==e)return{};var o,t,a={},s=Object.keys(e);for(t=0;t<s.length;t++)o=s[t],n.indexOf(o)>=0||(a[o]=e[o]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)o=s[t],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var l=t.createContext({}),c=function(e){var n=t.useContext(l),o=n;return e&&(o="function"==typeof e?e(n):r(r({},n),e)),o},p=function(e){var n=c(e.components);return t.createElement(l.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},g=t.forwardRef(function(e,n){var o=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=c(o),g=a,u=m["".concat(l,".").concat(g)]||m[g]||d[g]||s;return o?t.createElement(u,r(r({ref:n},p),{},{components:o})):t.createElement(u,r({ref:n},p))});function u(e,n){var o=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var s=o.length,r=new Array(s);r[0]=g;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i[m]="string"==typeof e?e:a,r[1]=i;for(var c=2;c<s;c++)r[c]=o[c];return t.createElement.apply(null,r)}return t.createElement.apply(null,o)}g.displayName="MDXCreateElement"},38085:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var t=o(58168),a=(o(96540),o(15680));const s={id:"sensorimotor-loops",title:"Sensorimotor Loops in Embodied Systems",sidebar_label:"Sensorimotor Loops"},r="Sensorimotor Loops in Embodied Systems",i={unversionedId:"embodied-ai/sensorimotor-loops",id:"embodied-ai/sensorimotor-loops",title:"Sensorimotor Loops in Embodied Systems",description:"Understanding Sensorimotor Loops",source:"@site/docs/embodied-ai/sensorimotor-loops.md",sourceDirName:"embodied-ai",slug:"/embodied-ai/sensorimotor-loops",permalink:"/ai-book/docs/embodied-ai/sensorimotor-loops",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/embodied-ai/sensorimotor-loops.md",tags:[],version:"current",frontMatter:{id:"sensorimotor-loops",title:"Sensorimotor Loops in Embodied Systems",sidebar_label:"Sensorimotor Loops"},sidebar:"tutorialSidebar",previous:{title:"Fundamentals",permalink:"/ai-book/docs/embodied-ai/fundamentals"},next:{title:"Design Principles",permalink:"/ai-book/docs/humanoid-robotics/design-principles"}},l={},c=[{value:"Understanding Sensorimotor Loops",id:"understanding-sensorimotor-loops",level:2},{value:"The Basic Structure",id:"the-basic-structure",level:3},{value:"Types of Sensorimotor Loops",id:"types-of-sensorimotor-loops",level:3},{value:"1. Reflexive Loops (Fastest)",id:"1-reflexive-loops-fastest",level:4},{value:"2. Motor Control Loops (Fast)",id:"2-motor-control-loops-fast",level:4},{value:"3. Behavioral Loops (Medium)",id:"3-behavioral-loops-medium",level:4},{value:"4. Learning Loops (Slow)",id:"4-learning-loops-slow",level:4},{value:"Practical Implementation Examples",id:"practical-implementation-examples",level:3},{value:"Example 1: Simple Obstacle Avoidance",id:"example-1-simple-obstacle-avoidance",level:4},{value:"Example 2: Adaptive Grasping",id:"example-2-adaptive-grasping",level:4},{value:"Hierarchical Sensorimotor Loops",id:"hierarchical-sensorimotor-loops",level:3},{value:"Example: Walking Robot",id:"example-walking-robot",level:4},{value:"Sensorimotor Contingencies",id:"sensorimotor-contingencies",level:3},{value:"Key Contingencies",id:"key-contingencies",level:4},{value:"Implementing Sensorimotor Learning",id:"implementing-sensorimotor-learning",level:3},{value:"Example: Q-Learning for Sensorimotor Control",id:"example-q-learning-for-sensorimotor-control",level:4},{value:"Challenges and Considerations",id:"challenges-and-considerations",level:3},{value:"1. Temporal Coordination",id:"1-temporal-coordination",level:4},{value:"2. Noise and Uncertainty",id:"2-noise-and-uncertainty",level:4},{value:"3. Stability",id:"3-stability",level:4},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:3},{value:"Related Topics",id:"related-topics",level:3}],p={toc:c},m="wrapper";function d({components:e,...n}){return(0,a.yg)(m,(0,t.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"sensorimotor-loops-in-embodied-systems"},"Sensorimotor Loops in Embodied Systems"),(0,a.yg)("h2",{id:"understanding-sensorimotor-loops"},"Understanding Sensorimotor Loops"),(0,a.yg)("p",null,"A ",(0,a.yg)("strong",{parentName:"p"},"sensorimotor loop")," is the fundamental feedback mechanism that enables embodied systems to interact with their environment. It represents the continuous cycle of sensing, processing, acting, and experiencing the consequences of those actions in the physical world."),(0,a.yg)("h3",{id:"the-basic-structure"},"The Basic Structure"),(0,a.yg)("p",null,"The sensorimotor loop consists of four key components:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"Environment \u2192 Sensors \u2192 Controller \u2192 Actuators \u2192 Environment\n")),(0,a.yg)("p",null,"Each component plays a crucial role:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Environment"),": Provides sensory input and receives motor output"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Sensors"),": Convert environmental stimuli into information the system can process"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Controller"),": Processes sensory information and generates motor commands"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Actuators"),": Convert motor commands into physical actions that affect the environment")),(0,a.yg)("h3",{id:"types-of-sensorimotor-loops"},"Types of Sensorimotor Loops"),(0,a.yg)("h4",{id:"1-reflexive-loops-fastest"},"1. Reflexive Loops (Fastest)"),(0,a.yg)("p",null,"These operate at the fastest timescale, typically milliseconds, and often bypass higher-level processing:"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example: Leg Withdrawal Reflex")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"Touch hot surface \u2192 Thermal sensors detect heat \u2192 Spinal cord processes \u2192 Leg muscles contract \u2192 Leg moves away\n")),(0,a.yg)("h4",{id:"2-motor-control-loops-fast"},"2. Motor Control Loops (Fast)"),(0,a.yg)("p",null,"These operate at the level of coordinated movements:"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example: Balance Control")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"Body tilts \u2192 Vestibular system detects tilt \u2192 Cerebellum processes \u2192 Postural muscles adjust \u2192 Body returns to upright\n")),(0,a.yg)("h4",{id:"3-behavioral-loops-medium"},"3. Behavioral Loops (Medium)"),(0,a.yg)("p",null,"These involve goal-directed behaviors:"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example: Reaching for an Object")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"See object \u2192 Visual processing \u2192 Motor planning \u2192 Arm movement \u2192 Hand grasps object\n")),(0,a.yg)("h4",{id:"4-learning-loops-slow"},"4. Learning Loops (Slow)"),(0,a.yg)("p",null,"These operate over longer timescales and involve adaptation:"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example: Learning to Ride a Bicycle")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"Attempt to ride \u2192 Multiple sensory inputs \u2192 Error detection \u2192 Neural adaptation \u2192 Improved performance\n")),(0,a.yg)("h3",{id:"practical-implementation-examples"},"Practical Implementation Examples"),(0,a.yg)("h4",{id:"example-1-simple-obstacle-avoidance"},"Example 1: Simple Obstacle Avoidance"),(0,a.yg)("p",null,"Let's implement a basic sensorimotor loop for obstacle avoidance:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import time\nimport numpy as np\n\nclass ObstacleAvoidanceRobot:\n    def __init__(self):\n        self.position = [0, 0]\n        self.direction = 0  # angle in radians\n        self.speed = 1.0\n\n    def sense_environment(self):\n        """Simulate sensor readings - returns distance to obstacles in front, left, right"""\n        # In a real robot, this would read from ultrasonic or IR sensors\n        # For simulation, we\'ll create a simple environment\n        distances = {\n            \'front\': self._check_distance(0),      # straight ahead\n            \'left\': self._check_distance(np.pi/4), # 45 degrees left\n            \'right\': self._check_distance(-np.pi/4) # 45 degrees right\n        }\n        return distances\n\n    def _check_distance(self, angle_offset):\n        """Check distance in a specific direction (simplified)"""\n        # Simulated distance - in reality, this would come from sensors\n        # Return a random distance between 0.5 and 3.0 meters\n        return np.random.uniform(0.5, 3.0)\n\n    def controller(self, sensor_data):\n        """Simple reactive controller"""\n        if sensor_data[\'front\'] < 0.8:  # obstacle too close in front\n            if sensor_data[\'left\'] > sensor_data[\'right\']:\n                # Turn left if more space on left\n                return {\'turn\': -0.5, \'speed\': self.speed}\n            else:\n                # Turn right if more space on right\n                return {\'turn\': 0.5, \'speed\': self.speed}\n        else:\n            # Move forward if clear ahead\n            return {\'turn\': 0.0, \'speed\': self.speed}\n\n    def actuate(self, motor_commands):\n        """Execute motor commands"""\n        self.direction += motor_commands[\'turn\']\n        dx = motor_commands[\'speed\'] * np.cos(self.direction) * 0.1  # 0.1 second time step\n        dy = motor_commands[\'speed\'] * np.sin(self.direction) * 0.1\n        self.position[0] += dx\n        self.position[1] += dy\n\n    def run_sensorimotor_loop(self, steps=100):\n        """Execute the sensorimotor loop for specified steps"""\n        for step in range(steps):\n            # Sense\n            sensor_data = self.sense_environment()\n\n            # Process and decide\n            motor_commands = self.controller(sensor_data)\n\n            # Act\n            self.actuate(motor_commands)\n\n            print(f"Step {step}: Position {self.position}, Direction {self.direction:.2f}")\n            time.sleep(0.01)  # Small delay to simulate real-time operation\n\n# Example usage\nrobot = ObstacleAvoidanceRobot()\nrobot.run_sensorimotor_loop(10)\n')),(0,a.yg)("h4",{id:"example-2-adaptive-grasping"},"Example 2: Adaptive Grasping"),(0,a.yg)("p",null,"A more complex example involving haptic feedback:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'class AdaptiveGrasping:\n    def __init__(self):\n        self.finger_positions = [0.0, 0.0]  # left and right fingers\n        self.grasp_force = 0.0\n        self.object_slip_detected = False\n\n    def sense(self):\n        """Sense tactile and force information"""\n        return {\n            \'contact_force\': self._get_contact_force(),\n            \'slip_sensors\': self._get_slip_data(),\n            \'finger_positions\': self.finger_positions.copy()\n        }\n\n    def _get_contact_force(self):\n        """Simulate force sensors in fingertips"""\n        # In reality, this would come from force/torque sensors\n        return np.random.uniform(0.5, 5.0)  # Newtons\n\n    def _get_slip_data(self):\n        """Simulate slip detection sensors"""\n        # In reality, this might come from tactile sensors or accelerometers\n        return np.random.random() > 0.95  # 5% chance of slip detection\n\n    def controller(self, sensor_data):\n        """Adaptive grasp controller"""\n        if sensor_data[\'slip_sensors\']:\n            # Increase grasp force if slip detected\n            return {\'force_increment\': 0.5, \'adjustment\': \'tighten\'}\n        elif sensor_data[\'contact_force\'] > 8.0:\n            # Decrease force if too high (to avoid damaging object)\n            return {\'force_increment\': -0.2, \'adjustment\': \'loosen\'}\n        else:\n            # Maintain current force\n            return {\'force_increment\': 0.0, \'adjustment\': \'maintain\'}\n\n    def actuate(self, commands):\n        """Adjust grip based on commands"""\n        self.grasp_force += commands[\'force_increment\']\n        self.grasp_force = max(0.1, min(self.grasp_force, 10.0))  # Clamp to safe range\n\n    def run_grasp_loop(self, max_iterations=50):\n        """Execute grasping loop"""\n        for i in range(max_iterations):\n            sensor_data = self.sense()\n            commands = self.controller(sensor_data)\n            self.actuate(commands)\n\n            print(f"Iteration {i}: Force={self.grasp_force:.2f}N, Adjustment={commands[\'adjustment\']}")\n\n            if commands[\'adjustment\'] == \'maintain\':\n                # If maintaining, we\'ve achieved stable grasp\n                print("Stable grasp achieved!")\n                break\n\n# Example usage\ngrasper = AdaptiveGrasp()\ngrasper.run_grasp_loop()\n')),(0,a.yg)("h3",{id:"hierarchical-sensorimotor-loops"},"Hierarchical Sensorimotor Loops"),(0,a.yg)("p",null,"Real embodied systems often have multiple interconnected loops operating at different timescales:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"High-Level Goals (seconds-minutes)\n    \u2193\nBehavior Selection (hundreds of ms-seconds)\n    \u2193\nMotor Control (tens of ms-hundreds of ms)\n    \u2193\nReflexes (ms)\n")),(0,a.yg)("h4",{id:"example-walking-robot"},"Example: Walking Robot"),(0,a.yg)("p",null,"A walking robot might have these hierarchical loops:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Reflex Loop")," (10ms): Ankle adjustments for ground irregularities"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Balance Loop")," (50ms): Postural adjustments to maintain stability"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Gait Loop")," (200ms): Step timing and placement"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Navigation Loop")," (1s): Path planning and obstacle avoidance")),(0,a.yg)("h3",{id:"sensorimotor-contingencies"},"Sensorimotor Contingencies"),(0,a.yg)("p",null,'The relationship between actions and sensory changes is called sensorimotor contingencies. These are the "rules" that govern how actions affect sensory input.'),(0,a.yg)("h4",{id:"key-contingencies"},"Key Contingencies"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Reafference"),": Sensory changes caused by one's own actions"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Exafference"),": Sensory changes caused by external events"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Predictive Processing"),": The system's ability to predict sensory consequences of actions")),(0,a.yg)("h3",{id:"implementing-sensorimotor-learning"},"Implementing Sensorimotor Learning"),(0,a.yg)("h4",{id:"example-q-learning-for-sensorimotor-control"},"Example: Q-Learning for Sensorimotor Control"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import numpy as np\n\nclass SensorimotorQAgent:\n    def __init__(self, state_size, action_size, learning_rate=0.1, discount=0.95, epsilon=0.1):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.learning_rate = learning_rate\n        self.discount = discount\n        self.epsilon = epsilon\n        self.q_table = np.zeros((state_size, action_size))\n\n    def sense_and_discretize(self, sensor_data):\n        """Convert continuous sensor data to discrete state"""\n        # This is a simplified example - real systems would have more complex discretization\n        if sensor_data[\'front_distance\'] < 1.0:\n            return 0  # Close obstacle\n        elif sensor_data[\'front_distance\'] < 2.0:\n            return 1  # Medium distance\n        else:\n            return 2  # Far obstacle\n\n    def choose_action(self, state):\n        """Epsilon-greedy action selection"""\n        if np.random.random() < self.epsilon:\n            return np.random.choice(self.action_size)  # Explore\n        else:\n            return np.argmax(self.q_table[state, :])   # Exploit\n\n    def update_q_table(self, state, action, reward, next_state):\n        """Update Q-table using Bellman equation"""\n        best_next_action = np.max(self.q_table[next_state, :])\n        td_target = reward + self.discount * best_next_action\n        td_error = td_target - self.q_table[state, action]\n        self.q_table[state, action] += self.learning_rate * td_error\n\n# Example usage in a simple navigation task\ndef run_sensorimotor_learning():\n    agent = SensorimotorQAgent(state_size=3, action_size=3)  # 3 states, 3 actions\n\n    for episode in range(1000):\n        # Simulate environment\n        sensor_data = {\'front_distance\': np.random.uniform(0.5, 3.0)}\n        state = agent.sense_and_discretize(sensor_data)\n        action = agent.choose_action(state)\n\n        # Simulate action consequences and reward\n        reward = calculate_reward(action, sensor_data)\n        next_sensor_data = simulate_environment_step(action, sensor_data)\n        next_state = agent.sense_and_discretize(next_sensor_data)\n\n        # Update Q-table\n        agent.update_q_table(state, action, reward, next_state)\n\n        if episode % 100 == 0:\n            print(f"Episode {episode}, Average Q-value: {np.mean(agent.q_table):.3f}")\n\ndef calculate_reward(action, sensor_data):\n    """Reward function for navigation"""\n    if sensor_data[\'front_distance\'] < 0.8 and action != 0:  # Moving forward with close obstacle\n        return -10  # Collision penalty\n    elif sensor_data[\'front_distance\'] > 2.0:\n        return 1    # Safe distance reward\n    else:\n        return 0    # Neutral\n\ndef simulate_environment_step(action, sensor_data):\n    """Simulate how action affects sensor readings"""\n    # Simplified simulation\n    new_distance = sensor_data[\'front_distance\'] + (np.random.uniform(-0.2, 0.2))\n    return {\'front_distance\': max(0.1, new_distance)}\n')),(0,a.yg)("h3",{id:"challenges-and-considerations"},"Challenges and Considerations"),(0,a.yg)("h4",{id:"1-temporal-coordination"},"1. Temporal Coordination"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Different sensors and actuators operate at different rates"),(0,a.yg)("li",{parentName:"ul"},"Delays in sensing and actuation can destabilize loops"),(0,a.yg)("li",{parentName:"ul"},"Synchronization is crucial for stable behavior")),(0,a.yg)("h4",{id:"2-noise-and-uncertainty"},"2. Noise and Uncertainty"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Sensors provide noisy, incomplete information"),(0,a.yg)("li",{parentName:"ul"},"Actuators may not execute commands perfectly"),(0,a.yg)("li",{parentName:"ul"},"Environmental changes can affect the loop dynamics")),(0,a.yg)("h4",{id:"3-stability"},"3. Stability"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Feedback loops can become unstable if not properly designed"),(0,a.yg)("li",{parentName:"ul"},"Gain scheduling may be needed for different operating conditions"),(0,a.yg)("li",{parentName:"ul"},"Robust control techniques help handle uncertainties")),(0,a.yg)("h3",{id:"learning-objectives"},"Learning Objectives"),(0,a.yg)("p",null,"After completing this chapter, you should be able to:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Describe the components and operation of sensorimotor loops"),(0,a.yg)("li",{parentName:"ol"},"Implement simple sensorimotor control algorithms"),(0,a.yg)("li",{parentName:"ol"},"Distinguish between different types of sensorimotor loops"),(0,a.yg)("li",{parentName:"ol"},"Design hierarchical control architectures"),(0,a.yg)("li",{parentName:"ol"},"Apply learning algorithms to sensorimotor control"),(0,a.yg)("li",{parentName:"ol"},"Identify and address challenges in sensorimotor implementation")),(0,a.yg)("h3",{id:"hands-on-exercise"},"Hands-On Exercise"),(0,a.yg)("p",null,"Create a simple sensorimotor loop that controls a simulated robot arm to track a moving target:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Implement sensors that detect target position"),(0,a.yg)("li",{parentName:"ol"},"Design a controller that calculates required joint movements"),(0,a.yg)("li",{parentName:"ol"},"Implement actuators that move the arm"),(0,a.yg)("li",{parentName:"ol"},"Test the system with different target movement patterns"),(0,a.yg)("li",{parentName:"ol"},"Analyze the stability and performance of your loop")),(0,a.yg)("p",null,"This exercise will help you understand the practical challenges of implementing sensorimotor loops in real systems."),(0,a.yg)("h3",{id:"related-topics"},"Related Topics"),(0,a.yg)("p",null,"For deeper exploration of concepts covered in this chapter, see:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("a",{parentName:"li",href:"../embodied-ai/introduction"},"Fundamentals of Physical AI")," - Core principles of embodied AI"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("a",{parentName:"li",href:"../humanoid-robotics/kinematics"},"Kinematics in Humanoid Robotics")," - Mathematical foundations for robotic systems"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("a",{parentName:"li",href:"../humanoid-robotics/control-systems"},"Control Systems for Humanoid Robots")," - Control theory applied to robotic systems"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("a",{parentName:"li",href:"../ai-integration/ml-locomotion"},"Machine Learning for Locomotion")," - ML approaches to sensorimotor control"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("a",{parentName:"li",href:"../challenges-ethics/safety-considerations"},"Safety Considerations in Physical AI Systems")," - Safety aspects in physical AI implementations")))}d.isMDXComponent=!0}}]);