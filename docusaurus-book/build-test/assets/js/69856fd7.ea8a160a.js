"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[246],{15680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>d});var a=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),g=i,d=u["".concat(l,".").concat(g)]||u[g]||m[g]||o;return t?a.createElement(d,r(r({ref:n},p),{},{components:t})):a.createElement(d,r({ref:n},p))});function d(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,r=new Array(o);r[0]=g;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=t[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},66236:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(58168),i=(t(96540),t(15680));const o={id:"module-4-vision-language-action",title:"Vision-Language-Action (VLA)",sidebar_label:"Module 4: VLA"},r="Vision-Language-Action (VLA)",s={unversionedId:"module-4-vision-language-action",id:"module-4-vision-language-action",title:"Vision-Language-Action (VLA)",description:"Vision-Language-Action Paradigm",source:"@site/docs/module-4-vision-language-action.mdx",sourceDirName:".",slug:"/module-4-vision-language-action",permalink:"/ai-book/docs/module-4-vision-language-action",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/module-4-vision-language-action.mdx",tags:[],version:"current",frontMatter:{id:"module-4-vision-language-action",title:"Vision-Language-Action (VLA)",sidebar_label:"Module 4: VLA"},sidebar:"tutorialSidebar",previous:{title:"Module 3: AI Robot Brain",permalink:"/ai-book/docs/module-3-ai-robot-brain"}},l={},c=[{value:"Vision-Language-Action Paradigm",id:"vision-language-action-paradigm",level:2},{value:"Voice-to-Action Using OpenAI Whisper",id:"voice-to-action-using-openai-whisper",level:2},{value:"Whisper Integration Pipeline:",id:"whisper-integration-pipeline",level:3},{value:"Voice Command Interpretation:",id:"voice-command-interpretation",level:3},{value:"LLM-Based Cognitive Task Planning",id:"llm-based-cognitive-task-planning",level:2},{value:"Task Decomposition Example:",id:"task-decomposition-example",level:3},{value:"LLM Integration with ROS 2:",id:"llm-integration-with-ros-2",level:3},{value:"Translating Plans into ROS 2 Actions",id:"translating-plans-into-ros-2-actions",level:2},{value:"Action Translation Framework:",id:"action-translation-framework",level:3},{value:"Capstone: Autonomous Humanoid End-to-End Pipeline",id:"capstone-autonomous-humanoid-end-to-end-pipeline",level:2},{value:"System Architecture:",id:"system-architecture",level:3},{value:"Example Interaction Flow:",id:"example-interaction-flow",level:3},{value:"Complete Pipeline Implementation:",id:"complete-pipeline-implementation",level:3}],p={toc:c},u="wrapper";function m({components:e,...n}){return(0,i.yg)(u,(0,a.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"vision-language-action-vla"},"Vision-Language-Action (VLA)"),(0,i.yg)("h2",{id:"vision-language-action-paradigm"},"Vision-Language-Action Paradigm"),(0,i.yg)("p",null,"The Vision-Language-Action (VLA) paradigm represents a breakthrough in robotic cognition, enabling robots to understand natural language commands, perceive their environment visually, and execute complex physical actions. This unified approach to perception, cognition, and action allows humanoid robots to interact naturally with humans and adapt to novel situations without explicit programming for every scenario."),(0,i.yg)("p",null,"VLA systems combine:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Vision"),": Real-time scene understanding and object recognition"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Language"),": Natural language comprehension and interpretation"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Action"),": Physical execution of tasks in the real world")),(0,i.yg)("p",null,"This integration creates a cognitive loop where robots can interpret human instructions, perceive the current state of their environment, plan appropriate actions, and execute them while continuously monitoring and adapting to changes."),(0,i.yg)("h2",{id:"voice-to-action-using-openai-whisper"},"Voice-to-Action Using OpenAI Whisper"),(0,i.yg)("p",null,"OpenAI Whisper enables robust speech recognition capabilities that transform spoken human commands into structured robotic actions. The integration of voice recognition with robotic action planning creates intuitive human-robot interaction experiences."),(0,i.yg)("h3",{id:"whisper-integration-pipeline"},"Whisper Integration Pipeline:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport threading\nimport queue\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__("voice_to_action")\n\n        # Initialize Whisper model\n        self.whisper_model = whisper.load_model("base.en")\n\n        # Audio input subscription\n        self.audio_subscriber = self.create_subscription(\n            AudioData,\n            \'/audio/input\',\n            self.audio_callback,\n            10)\n\n        # Command publisher\n        self.command_publisher = self.create_publisher(\n            String,\n            \'/natural_language_command\',\n            10)\n\n        # Audio processing queue\n        self.audio_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_audio_stream)\n        self.processing_thread.start()\n\n    def audio_callback(self, msg):\n        """Receive audio data and add to processing queue"""\n        self.audio_queue.put(msg.data)\n\n    def process_audio_stream(self):\n        """Process audio stream using Whisper"""\n        while rclpy.ok():\n            try:\n                audio_data = self.audio_queue.get(timeout=1.0)\n\n                # Convert audio data to format suitable for Whisper\n                audio_array = self.convert_audio_format(audio_data)\n\n                # Transcribe using Whisper\n                result = self.whisper_model.transcribe(audio_array)\n                transcription = result["text"]\n\n                # Publish interpreted command\n                cmd_msg = String()\n                cmd_msg.data = transcription\n                self.command_publisher.publish(cmd_msg)\n\n                self.get_logger().info(f"Interpreted command: {transcription}")\n\n            except queue.Empty:\n                continue\n\n    def convert_audio_format(self, audio_data):\n        """Convert ROS audio format to format suitable for Whisper"""\n        # Implementation details for audio conversion\n        pass\n')),(0,i.yg)("h3",{id:"voice-command-interpretation"},"Voice Command Interpretation:"),(0,i.yg)("p",null,"The system processes natural language commands and maps them to executable robotic actions:"),(0,i.yg)("table",null,(0,i.yg)("thead",{parentName:"table"},(0,i.yg)("tr",{parentName:"thead"},(0,i.yg)("th",{parentName:"tr",align:null},"Voice Command"),(0,i.yg)("th",{parentName:"tr",align:null},"Interpreted Action"),(0,i.yg)("th",{parentName:"tr",align:null},"ROS 2 Message"))),(0,i.yg)("tbody",{parentName:"table"},(0,i.yg)("tr",{parentName:"tbody"},(0,i.yg)("td",{parentName:"tr",align:null},'"Pick up the red ball"'),(0,i.yg)("td",{parentName:"tr",align:null},"Object manipulation"),(0,i.yg)("td",{parentName:"tr",align:null},(0,i.yg)("inlineCode",{parentName:"td"},"/manipulation/grasp_object"))),(0,i.yg)("tr",{parentName:"tbody"},(0,i.yg)("td",{parentName:"tr",align:null},'"Walk to the kitchen"'),(0,i.yg)("td",{parentName:"tr",align:null},"Navigation"),(0,i.yg)("td",{parentName:"tr",align:null},(0,i.yg)("inlineCode",{parentName:"td"},"/move_base/goal"))),(0,i.yg)("tr",{parentName:"tbody"},(0,i.yg)("td",{parentName:"tr",align:null},'"Show me the blue cup"'),(0,i.yg)("td",{parentName:"tr",align:null},"Object search"),(0,i.yg)("td",{parentName:"tr",align:null},(0,i.yg)("inlineCode",{parentName:"td"},"/perception/search_object"))))),(0,i.yg)("h2",{id:"llm-based-cognitive-task-planning"},"LLM-Based Cognitive Task Planning"),(0,i.yg)("p",null,"Large Language Models (LLMs) serve as high-level cognitive planners that decompose complex human instructions into sequences of primitive robotic actions. This planning capability enables robots to handle novel situations and multi-step tasks."),(0,i.yg)("h3",{id:"task-decomposition-example"},"Task Decomposition Example:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},'Human Command: "Please bring me a cold drink from the refrigerator"\nDecomposed Actions:\n1. Navigate to kitchen area\n2. Locate refrigerator\n3. Open refrigerator door\n4. Identify cold drink\n5. Grasp cold drink\n6. Close refrigerator door\n7. Navigate to human\n8. Deliver cold drink\n')),(0,i.yg)("h3",{id:"llm-integration-with-ros-2"},"LLM Integration with ROS 2:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import openai\nfrom rclpy.qos import QoSProfile\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\n\nclass LLMTaskPlannerNode(Node):\n    def __init__(self):\n        super().__init__("llm_task_planner")\n\n        # Initialize OpenAI client\n        self.openai_client = openai.OpenAI(api_key="your-api-key")\n\n        # Subscriptions for environmental context\n        self.vision_context_sub = self.create_subscription(\n            String,\n            \'/vision/context\',\n            self.vision_context_callback,\n            10)\n\n        self.location_sub = self.create_subscription(\n            PoseStamped,\n            \'/current_location\',\n            self.location_callback,\n            10)\n\n        # Publisher for task sequences\n        self.task_publisher = self.create_publisher(\n            String,\n            \'/planned_tasks\',\n            10)\n\n        self.current_context = {}\n        self.current_location = None\n\n    def vision_context_callback(self, msg):\n        """Update current visual context"""\n        self.current_context["objects"] = eval(msg.data)\n\n    def location_callback(self, msg):\n        """Update current location"""\n        self.current_location = msg.pose\n\n    def plan_task_sequence(self, natural_language_command):\n        """Generate task sequence from natural language command"""\n\n        system_prompt = """\n        You are a robotic task planner. Given a human command and environmental context,\n        break down the command into a sequence of executable robotic actions.\n        Respond in JSON format with a list of actions.\n        """\n\n        user_prompt = f"""\n        Human Command: {natural_language_command}\n        Current Objects in Environment: {self.current_context.get("objects", {{}})}\n        Current Location: {self.current_location}\n\n        Provide a detailed sequence of robotic actions to fulfill the command.\n        """\n\n        response = self.openai_client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.1\n        )\n\n        task_sequence = response.choices[0].message.content\n        return self.parse_task_json(task_sequence)\n\n    def parse_task_json(self, task_json_str):\n        """Parse and validate the task sequence from LLM"""\n        try:\n            import json\n            tasks = json.loads(task_json_str)\n            return tasks\n        except:\n            # Handle cases where LLM doesn\'t return perfect JSON\n            return self.fallback_parse(task_json_str)\n')),(0,i.yg)("h2",{id:"translating-plans-into-ros-2-actions"},"Translating Plans into ROS 2 Actions"),(0,i.yg)("p",null,"The cognitive system translates high-level plans into specific ROS 2 action calls, bridging the gap between abstract goals and concrete robotic behaviors."),(0,i.yg)("h3",{id:"action-translation-framework"},"Action Translation Framework:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class PlanExecutor:\n    def __init__(self):\n        self.action_clients = {\n            "navigation": ActionClient(NavigateToPose),\n            "manipulation": ActionClient(PickPlace),\n            "perception": ActionClient(ObjectDetection),\n            "speech": ActionClient(TextToSpeech)\n        }\n\n    def execute_plan(self, task_sequence):\n        """Execute the sequence of tasks"""\n        for task in task_sequence:\n            action_type = task["action_type"]\n\n            if action_type == "navigate":\n                self.execute_navigation(task)\n            elif action_type == "grasp":\n                self.execute_grasping(task)\n            elif action_type == "detect":\n                self.execute_detection(task)\n            elif action_type == "speak":\n                self.execute_speech(task)\n\n    def execute_navigation(self, task):\n        """Execute navigation task"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = "map"\n        goal_msg.pose.pose.position.x = task["target_x"]\n        goal_msg.pose.pose.position.y = task["target_y"]\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        future = self.action_clients["navigation"].send_goal_async(goal_msg)\n        return future\n\n    def execute_grasping(self, task):\n        """Execute object grasping task"""\n        goal_msg = PickPlace.Goal()\n        goal_msg.object_name = task["object_name"]\n        goal_msg.target_pose = task["target_pose"]\n\n        future = self.action_clients["manipulation"].send_goal_async(goal_msg)\n        return future\n')),(0,i.yg)("h2",{id:"capstone-autonomous-humanoid-end-to-end-pipeline"},"Capstone: Autonomous Humanoid End-to-End Pipeline"),(0,i.yg)("p",null,"The culmination of VLA integration creates a complete autonomous pipeline that connects human intention to robotic action:"),(0,i.yg)("h3",{id:"system-architecture"},"System Architecture:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"[Human Speech] \u2192 [Whisper ASR] \u2192 [LLM Planner] \u2192 [Action Executor] \u2192 [Robot Action]\n      \u2193              \u2193               \u2193              \u2193               \u2193\n[Audio Input] \u2192 [Text Command] \u2192 [Task Sequence] \u2192 [ROS Actions] \u2192 [Physical Result]\n")),(0,i.yg)("h3",{id:"example-interaction-flow"},"Example Interaction Flow:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Perception Phase"),": Robot observes environment using cameras and sensors"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Communication Phase"),': Human speaks command ("Please clean the table")'),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Interpretation Phase"),": Whisper transcribes speech, LLM understands intent"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Planning Phase"),": Cognitive system generates task sequence"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Execution Phase"),": Robot executes navigation, manipulation, and cleanup tasks"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Monitoring Phase"),": Robot monitors task completion and reports status")),(0,i.yg)("h3",{id:"complete-pipeline-implementation"},"Complete Pipeline Implementation:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class VLAPipelineNode(Node):\n    def __init__(self):\n        super().__init__("vla_pipeline")\n\n        # Initialize all components\n        self.voice_processor = VoiceToActionNode()\n        self.llm_planner = LLMTaskPlannerNode()\n        self.action_executor = PlanExecutor()\n\n        # State management\n        self.pipeline_state = "waiting_for_command"\n        self.current_task = None\n\n        # Timer for pipeline coordination\n        self.pipeline_timer = self.create_timer(0.1, self.pipeline_step)\n\n    def pipeline_step(self):\n        """Main pipeline execution loop"""\n        if self.pipeline_state == "waiting_for_command":\n            # Wait for voice command\n            if self.voice_processor.has_new_command():\n                self.pipeline_state = "interpreting_command"\n\n        elif self.pipeline_state == "interpreting_command":\n            # Use LLM to plan tasks\n            command = self.voice_processor.get_latest_command()\n            self.current_task = self.llm_planner.plan_task_sequence(command)\n            self.pipeline_state = "executing_tasks"\n\n        elif self.pipeline_state == "executing_tasks":\n            # Execute planned tasks\n            if self.action_executor.execute_plan(self.current_task):\n                self.pipeline_state = "reporting_completion"\n\n        elif self.pipeline_state == "reporting_completion":\n            # Report completion and return to waiting\n            self.speak_completion()\n            self.pipeline_state = "waiting_for_command"\n\n    def speak_completion(self):\n        """Report task completion to human"""\n        completion_msg = String()\n        completion_msg.data = "Task completed successfully"\n        self.speech_publisher.publish(completion_msg)\n')),(0,i.yg)("figure",null,(0,i.yg)("img",{src:"/img/vla-end-to-end-pipeline.png",alt:"VLA End-to-End Pipeline Diagram"}),(0,i.yg)("figcaption",null,"Complete Vision-Language-Action pipeline connecting human commands to robotic actions")),(0,i.yg)("p",null,"This integrated system represents the future of human-robot interaction, where complex humanoid robots can understand natural language, perceive their environment, and execute sophisticated tasks autonomously while adapting to changing conditions and requirements."))}m.isMDXComponent=!0}}]);