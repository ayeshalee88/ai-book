"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[974],{15680:(e,n,t)=>{t.d(n,{xA:()=>m,yg:()=>f});var i=t(96540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach(function(n){o(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,i,o=function(e,n){if(null==e)return{};var t,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=i.createContext({}),c=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},m=function(e){var n=c(e.components);return i.createElement(l.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef(function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),p=c(t),u=o,f=p["".concat(l,".").concat(u)]||p[u]||d[u]||a;return t?i.createElement(f,r(r({ref:n},m),{},{components:t})):i.createElement(f,r({ref:n},m))});function f(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,r=new Array(a);r[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:o,r[1]=s;for(var c=2;c<a;c++)r[c]=t[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}u.displayName="MDXCreateElement"},71140:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>m});var i=t(58168),o=(t(96540),t(15680)),a=t(99098);const r={id:"cv-interaction",title:"Computer Vision for Physical AI Interaction",sidebar_label:"CV for Interaction"},s="Computer Vision for Physical AI Interaction",l={unversionedId:"ai-integration/cv-interaction",id:"ai-integration/cv-interaction",title:"Computer Vision for Physical AI Interaction",description:"Introduction to Vision-Based Physical AI",source:"@site/docs/ai-integration/cv-interaction.md",sourceDirName:"ai-integration",slug:"/ai-integration/cv-interaction",permalink:"/ai-book/docs/ai-integration/cv-interaction",draft:!1,editUrl:"https://github.com/ayeshalee88/ai-book/edit/main/docusaurus-book/docs/ai-integration/cv-interaction.md",tags:[],version:"current",frontMatter:{id:"cv-interaction",title:"Computer Vision for Physical AI Interaction",sidebar_label:"CV for Interaction"},sidebar:"tutorialSidebar",previous:{title:"RL Applications",permalink:"/ai-book/docs/ai-integration/rl-applications"},next:{title:"Boston Dynamics",permalink:"/ai-book/docs/case-studies/boston-dynamics"}},c={},m=[{value:"Introduction to Vision-Based Physical AI",id:"introduction-to-vision-based-physical-ai",level:2},{value:"The Vision-Action Loop",id:"the-vision-action-loop",level:3},{value:"Key Requirements for Physical AI Vision",id:"key-requirements-for-physical-ai-vision",level:3},{value:"Real-Time Performance",id:"real-time-performance",level:4},{value:"Robustness",id:"robustness",level:4},{value:"Actionability",id:"actionability",level:4},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Deep Learning Approaches",id:"deep-learning-approaches",level:3},{value:"YOLO (You Only Look Once) for Real-Time Detection",id:"yolo-you-only-look-once-for-real-time-detection",level:4},{value:"Feature-Based Detection",id:"feature-based-detection",level:4},{value:"3D Object Detection",id:"3d-object-detection",level:3},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Position-Based Visual Servoing",id:"position-based-visual-servoing",level:3},{value:"Image-Based Visual Servoing",id:"image-based-visual-servoing",level:3},{value:"Scene Understanding for Manipulation",id:"scene-understanding-for-manipulation",level:2},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:3},{value:"Depth Perception and 3D Understanding",id:"depth-perception-and-3d-understanding",level:2},{value:"Depth Estimation",id:"depth-estimation",level:3},{value:"Stereo Vision",id:"stereo-vision",level:3},{value:"Visual Attention and Focus",id:"visual-attention-and-focus",level:2},{value:"Saliency Detection",id:"saliency-detection",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:2},{value:"Real-Time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Model Optimization for Edge Deployment",id:"model-optimization-for-edge-deployment",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Safe Visual Processing",id:"safe-visual-processing",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Interactive Vision Demonstration",id:"interactive-vision-demonstration",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2}],p={toc:m},d="wrapper";function u({components:e,...n}){return(0,o.yg)(d,(0,i.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"computer-vision-for-physical-ai-interaction"},"Computer Vision for Physical AI Interaction"),(0,o.yg)("h2",{id:"introduction-to-vision-based-physical-ai"},"Introduction to Vision-Based Physical AI"),(0,o.yg)("p",null,"Computer vision is a critical component of physical AI systems, enabling robots to perceive and understand their environment for effective interaction. Unlike traditional computer vision applications focused on image classification or object detection, vision for physical AI must provide real-time, actionable information that guides robotic behavior and enables safe, effective interaction with the physical world."),(0,o.yg)("h3",{id:"the-vision-action-loop"},"The Vision-Action Loop"),(0,o.yg)("p",null,"Physical AI systems close the loop between vision and action:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"Image Capture \u2192 Visual Processing \u2192 Action Decision \u2192 Physical Action \u2192 Environment Change \u2192 New Image Capture\n")),(0,o.yg)("p",null,"This continuous cycle requires:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Real-time processing"),": Fast inference for responsive behavior"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Robust perception"),": Reliable operation under varying conditions"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Actionable output"),": Visual information that directly informs control"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Sensor fusion"),": Integration with other sensory modalities")),(0,o.yg)("h3",{id:"key-requirements-for-physical-ai-vision"},"Key Requirements for Physical AI Vision"),(0,o.yg)("h4",{id:"real-time-performance"},"Real-Time Performance"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"High frame rates"),": 30-60 FPS for responsive interaction"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Low latency"),": <50ms from capture to action for safety"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Efficient algorithms"),": Optimized for embedded platforms")),(0,o.yg)("h4",{id:"robustness"},"Robustness"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Illumination invariance"),": Operation under varying lighting"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Viewpoint invariance"),": Recognition from different angles"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Occlusion handling"),": Robustness to partial object visibility"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Motion blur tolerance"),": Recognition during movement")),(0,o.yg)("h4",{id:"actionability"},"Actionability"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Metric measurements"),": Real-world dimensions and positions"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Uncertainty quantification"),": Confidence in visual estimates"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Temporal consistency"),": Stable tracking across frames")),(0,o.yg)("h2",{id:"object-detection-and-recognition"},"Object Detection and Recognition"),(0,o.yg)("h3",{id:"deep-learning-approaches"},"Deep Learning Approaches"),(0,o.yg)("h4",{id:"yolo-you-only-look-once-for-real-time-detection"},"YOLO (You Only Look Once) for Real-Time Detection"),(0,o.yg)("p",null,"YOLO is particularly well-suited for robotic applications due to its speed:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport numpy as np\n\nclass YOLODetector:\n    def __init__(self, model_path, confidence_threshold=0.5):\n        # Load pre-trained YOLO model\n        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n        self.model.eval()\n        self.confidence_threshold = confidence_threshold\n\n    def detect_objects(self, image):\n        \"\"\"\n        Detect objects in image and return bounding boxes, classes, and confidence scores\n        \"\"\"\n        # Convert PIL image to tensor\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n\n        # Perform inference\n        results = self.model(image)\n\n        # Extract detections\n        detections = results.pandas().xyxy[0]  # Convert to pandas DataFrame\n\n        # Filter by confidence\n        confident_detections = detections[detections['confidence'] > self.confidence_threshold]\n\n        objects = []\n        for _, detection in confident_detections.iterrows():\n            obj = {\n                'class': detection['name'],\n                'confidence': detection['confidence'],\n                'bbox': [detection['xmin'], detection['ymin'],\n                         detection['xmax'], detection['ymax']],\n                'center': [(detection['xmin'] + detection['xmax']) / 2,\n                           (detection['ymin'] + detection['ymax']) / 2]\n            }\n            objects.append(obj)\n\n        return objects\n\n    def get_object_pose(self, object_name, image):\n        \"\"\"\n        Get the 2D position of an object in the image\n        \"\"\"\n        detections = self.detect_objects(image)\n\n        for obj in detections:\n            if obj['class'] == object_name:\n                # Convert 2D image coordinates to robot frame coordinates\n                pixel_x, pixel_y = obj['center']\n                # Apply camera calibration to get metric coordinates\n                metric_coords = self.pixel_to_metric(pixel_x, pixel_y)\n                return metric_coords, obj['confidence']\n\n        return None, 0.0\n\n    def pixel_to_metric(self, pixel_x, pixel_y):\n        \"\"\"\n        Convert pixel coordinates to metric coordinates using camera calibration\n        \"\"\"\n        # This would use actual camera calibration parameters\n        # For example: using intrinsic matrix and known object size\n        return [pixel_x * 0.001, pixel_y * 0.001]  # Simplified conversion\n")),(0,o.yg)("h4",{id:"feature-based-detection"},"Feature-Based Detection"),(0,o.yg)("p",null,"For specific objects or markers, feature-based methods can be more robust:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import cv2\nimport numpy as np\n\nclass FeatureDetector:\n    def __init__(self, reference_image_path):\n        self.reference_image = cv2.imread(reference_image_path, 0)\n        self.sift = cv2.SIFT_create()\n        self.bf = cv2.BFMatcher()\n\n        # Compute reference keypoints and descriptors\n        self.ref_kp, self.ref_desc = self.sift.detectAndCompute(self.reference_image, None)\n\n    def detect_object(self, query_image):\n        """\n        Detect object using SIFT features\n        """\n        query_gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n        query_kp, query_desc = self.sift.detectAndCompute(query_gray, None)\n\n        if query_desc is None or self.ref_desc is None:\n            return None\n\n        # Match features\n        matches = self.bf.knnMatch(self.ref_desc, query_desc, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for m, n in matches:\n            if m and n and m.distance < 0.75 * n.distance:\n                good_matches.append(m)\n\n        # Require minimum number of matches\n        if len(good_matches) >= 10:\n            # Estimate homography for pose\n            src_pts = np.float32([self.ref_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([query_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n            homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n            if homography is not None:\n                # Calculate object center\n                h, w = self.reference_image.shape\n                object_center = np.float32([[w/2, h/2]]).reshape(-1, 1, 2)\n                transformed_center = cv2.perspectiveTransform(object_center, homography)\n                return transformed_center[0][0], len(good_matches)\n\n        return None, 0\n')),(0,o.yg)("h3",{id:"3d-object-detection"},"3D Object Detection"),(0,o.yg)("p",null,"For physical interaction, 3D information is crucial:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import open3d as o3d\nimport numpy as np\n\nclass Object3DTracker:\n    def __init__(self, voxel_size=0.01):\n        self.voxel_size = voxel_size\n        self.previous_cloud = None\n        self.tracked_objects = {}\n\n    def process_point_cloud(self, point_cloud):\n        """\n        Process 3D point cloud to detect and track objects\n        """\n        # Downsample for efficiency\n        cloud_down = point_cloud.voxel_down_sample(voxel_size=self.voxel_size)\n\n        # Plane segmentation (for table detection)\n        plane_model, inliers = cloud_down.segment_plane(\n            distance_threshold=0.01,\n            ransac_n=3,\n            num_iterations=1000\n        )\n\n        # Remove plane points to isolate objects\n        object_cloud = cloud_down.select_by_index(inliers, invert=True)\n\n        # Cluster objects\n        with o3d.utility.VectorsInt() as cluster_idx:\n            labels = np.array(object_cloud.cluster_dbscan(\n                eps=self.voxel_size * 5,\n                min_points=10,\n                print_progress=False\n            ))\n\n        # Process each cluster\n        objects = []\n        for i in range(labels.max() + 1):\n            cluster_indices = np.where(labels == i)[0]\n            if len(cluster_indices) > 50:  # Minimum cluster size\n                cluster_cloud = object_cloud.select_by_index(cluster_indices)\n\n                # Calculate bounding box and center\n                bbox = cluster_cloud.get_axis_aligned_bounding_box()\n                center = bbox.get_center()\n\n                obj = {\n                    \'id\': i,\n                    \'center\': center,\n                    \'bbox\': [bbox.min_bound, bbox.max_bound],\n                    \'point_count\': len(cluster_indices)\n                }\n                objects.append(obj)\n\n        return objects\n\n    def estimate_pose(self, object_cloud, model_cloud):\n        """\n        Estimate 6D pose of object using ICP\n        """\n        # Estimate initial transformation\n        initial_transformation = self.estimate_initial_transform(object_cloud, model_cloud)\n\n        # Refine using ICP\n        result = o3d.pipelines.registration.registration_icp(\n            object_cloud, model_cloud, 0.02, initial_transformation,\n            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=200)\n        )\n\n        return result.transformation\n\n    def estimate_initial_transform(self, source, target):\n        """\n        Estimate initial transformation using FPFH features\n        """\n        source_fpfh = self.compute_fpfh(source)\n        target_fpfh = self.compute_fpfh(target)\n\n        # Correspondence estimation\n        corres = o3d.pipelines.registration.compute_correspondences(\n            source_fpfh, target_fpfh, 0.05\n        )\n\n        # RANSAC for robust transformation estimation\n        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n            source, target, source_fpfh, target_fpfh, True,\n            o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n            4, [\n                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(0.05)\n            ],\n            o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n        )\n\n        return result.transformation\n')),(0,o.yg)("h2",{id:"visual-servoing"},"Visual Servoing"),(0,o.yg)("p",null,"Visual servoing uses visual feedback to control robot motion:"),(0,o.yg)("h3",{id:"position-based-visual-servoing"},"Position-Based Visual Servoing"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class PositionBasedServo:\n    def __init__(self, camera_matrix, target_position):\n        self.K = camera_matrix  # Camera intrinsic matrix\n        self.target_pos = target_position\n        self.kp = 0.1  # Proportional gain\n\n    def compute_control(self, current_object_pos, current_ee_pos):\n        """\n        Compute control command to move end-effector to target\n        current_object_pos: [x, y, z] object position in camera frame\n        current_ee_pos: [x, y, z] end-effector position in robot base frame\n        """\n        # Transform object position to robot base frame\n        object_robot_frame = self.camera_to_robot_frame(current_object_pos)\n\n        # Calculate position error\n        pos_error = self.target_pos - object_robot_frame\n\n        # Compute desired end-effector velocity\n        ee_vel = self.kp * pos_error\n\n        return ee_vel\n\n    def camera_to_robot_frame(self, camera_pos):\n        """\n        Transform position from camera frame to robot base frame\n        This would use the calibrated transformation between camera and robot\n        """\n        # Simplified transformation - in practice this uses calibrated extrinsics\n        return camera_pos  # Placeholder\n')),(0,o.yg)("h3",{id:"image-based-visual-servoing"},"Image-Based Visual Servoing"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class ImageBasedServo:\n    def __init__(self, target_features, camera_params):\n        self.target_features = target_features  # Target image features\n        self.camera_params = camera_params\n        self.kp = 0.5\n\n    def compute_control(self, current_image_features):\n        """\n        Compute control based on image feature error\n        """\n        # Calculate feature error\n        feature_error = self.target_features - current_image_features\n\n        # Compute interaction matrix (simplified)\n        interaction_matrix = self.compute_interaction_matrix()\n\n        # Calculate image velocity\n        image_vel = self.kp * feature_error\n\n        # Convert to camera velocity\n        camera_vel = np.linalg.inv(interaction_matrix) @ image_vel\n\n        return camera_vel\n\n    def compute_interaction_matrix(self):\n        """\n        Compute image Jacobian (interaction matrix)\n        This relates image feature velocities to camera velocities\n        """\n        # Simplified 2D case for point features\n        # In practice, this depends on the specific features being used\n        L = np.zeros((2, 6))  # 2D features, 6D camera motion\n        # Fill in the interaction matrix based on feature type\n        return L\n')),(0,o.yg)("h2",{id:"scene-understanding-for-manipulation"},"Scene Understanding for Manipulation"),(0,o.yg)("h3",{id:"semantic-segmentation"},"Semantic Segmentation"),(0,o.yg)("p",null,"Semantic segmentation provides pixel-level understanding:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torchvision.transforms as transforms\n\nclass SemanticSegmenter:\n    def __init__(self, model_path):\n        # Load pre-trained segmentation model\n        self.model = torch.hub.load('pytorch/vision:v0.10.0',\n                                   'deeplabv3_resnet50',\n                                   pretrained=True)\n        self.model.eval()\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def segment_image(self, image):\n        \"\"\"\n        Segment image into semantic classes\n        \"\"\"\n        input_tensor = self.transform(image).unsqueeze(0)\n\n        with torch.no_grad():\n            output = self.model(input_tensor)['out'][0]\n            predicted = torch.argmax(output, dim=0).cpu().numpy()\n\n        return predicted\n\n    def get_manipulable_objects(self, segmented_image, image):\n        \"\"\"\n        Extract manipulable objects from segmentation\n        \"\"\"\n        # Common classes for manipulation\n        manipulable_classes = ['bottle', 'cup', 'book', 'bowl', 'remote', 'cell phone']\n\n        objects = []\n        for class_id, class_name in enumerate(self.get_class_names()):\n            if class_name in manipulable_classes:\n                mask = (segmented_image == class_id)\n                if np.any(mask):\n                    # Calculate object properties\n                    contours, _ = cv2.findContours(\n                        mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                    )\n\n                    for contour in contours:\n                        if cv2.contourArea(contour) > 100:  # Minimum size\n                            # Calculate bounding box\n                            x, y, w, h = cv2.boundingRect(contour)\n                            center_x, center_y = x + w//2, y + h//2\n\n                            obj = {\n                                'class': class_name,\n                                'bbox': [x, y, w, h],\n                                'center': [center_x, center_y],\n                                'contour': contour\n                            }\n                            objects.append(obj)\n\n        return objects\n\n    def get_class_names(self):\n        \"\"\"\n        Return mapping from class ID to name\n        \"\"\"\n        return [\n            'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n            'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n        ]\n")),(0,o.yg)("h3",{id:"instance-segmentation"},"Instance Segmentation"),(0,o.yg)("p",null,"For individual object tracking:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"class InstanceSegmenter:\n    def __init__(self, confidence_threshold=0.5):\n        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s-seg', pretrained=True)\n        self.confidence_threshold = confidence_threshold\n\n    def segment_instances(self, image):\n        \"\"\"\n        Segment individual object instances\n        \"\"\"\n        results = self.model(image)\n\n        # Get masks and bounding boxes\n        masks = results.masks.data.cpu().numpy() if results.masks is not None else []\n        boxes = results.pandas().xyxy[0] if results.pandas().xyxy[0] is not None else []\n\n        instances = []\n        for i, (mask, box) in enumerate(zip(masks, boxes)):\n            if box['confidence'] > self.confidence_threshold:\n                instance = {\n                    'class': box['name'],\n                    'confidence': box['confidence'],\n                    'bbox': [box['xmin'], box['ymin'], box['xmax'], box['ymax']],\n                    'mask': mask,\n                    'area': np.sum(mask > 0.5)\n                }\n                instances.append(instance)\n\n        return instances\n")),(0,o.yg)("h2",{id:"depth-perception-and-3d-understanding"},"Depth Perception and 3D Understanding"),(0,o.yg)("h3",{id:"depth-estimation"},"Depth Estimation"),(0,o.yg)("p",null,"For robots without depth sensors, monocular depth estimation:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class MonocularDepthEstimator:\n    def __init__(self):\n        # Load pre-trained depth estimation model\n        # Using MiDaS for monocular depth estimation\n        self.model = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")\n        self.model.eval()\n\n        # Transform for MiDaS\n        self.transform = transforms.Compose([\n            transforms.Resize(384),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def estimate_depth(self, image):\n        """\n        Estimate depth from single RGB image\n        """\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        input_tensor = self.transform(image).unsqueeze(0)\n\n        with torch.no_grad():\n            prediction = self.model(input_tensor)\n\n        # Resize back to original size\n        depth_map = torch.nn.functional.interpolate(\n            prediction.unsqueeze(1),\n            size=image.size[::-1],  # [height, width]\n            mode="bicubic",\n            align_corners=False,\n        ).squeeze()\n\n        return depth_map.cpu().numpy()\n')),(0,o.yg)("h3",{id:"stereo-vision"},"Stereo Vision"),(0,o.yg)("p",null,"For more accurate depth with stereo cameras:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class StereoDepthEstimator:\n    def __init__(self, left_camera_params, right_camera_params):\n        # Stereo rectification parameters\n        self.left_camera_matrix = left_camera_params[\'K\']\n        self.right_camera_matrix = right_camera_params[\'K\']\n        self.distortion_coeffs = left_camera_params[\'distortion\']\n\n        # Stereo block matcher\n        self.stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n    def compute_disparity(self, left_image, right_image):\n        """\n        Compute disparity map from stereo images\n        """\n        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n\n        # Compute disparity\n        disparity = self.stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\n\n        return disparity\n\n    def disparity_to_depth(self, disparity, baseline, focal_length):\n        """\n        Convert disparity to depth\n        baseline: distance between cameras (meters)\n        focal_length: focal length in pixels\n        """\n        # Avoid division by zero\n        disparity[disparity == 0] = 0.01\n        depth = (baseline * focal_length) / disparity\n        return depth\n')),(0,o.yg)("h2",{id:"visual-attention-and-focus"},"Visual Attention and Focus"),(0,o.yg)("h3",{id:"saliency-detection"},"Saliency Detection"),(0,o.yg)("p",null,"Identifying important regions in images:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"class SaliencyDetector:\n    def __init__(self):\n        # Use OpenCV's spectral residual saliency detector\n        self.saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n\n    def detect_salient_regions(self, image):\n        \"\"\"\n        Detect salient regions in image\n        \"\"\"\n        success, saliency_map = self.saliency.computeSaliency(image)\n\n        if success:\n            # Threshold to get binary mask of salient regions\n            _, binary_map = cv2.threshold(saliency_map, 0.5, 255, cv2.THRESH_BINARY)\n            binary_map = binary_map.astype(np.uint8)\n\n            # Find contours of salient regions\n            contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            salient_regions = []\n            for contour in contours:\n                if cv2.contourArea(contour) > 100:  # Minimum region size\n                    # Calculate bounding rectangle\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x, center_y = x + w//2, y + h//2\n\n                    region = {\n                        'bbox': [x, y, w, h],\n                        'center': [center_x, center_y],\n                        'area': cv2.contourArea(contour)\n                    }\n                    salient_regions.append(region)\n\n            return salient_regions, saliency_map\n\n        return [], None\n")),(0,o.yg)("h2",{id:"multi-modal-fusion"},"Multi-Modal Fusion"),(0,o.yg)("p",null,"Combining vision with other sensors:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"class MultiModalFusion:\n    def __init__(self):\n        self.vision_processor = YOLODetector(\"yolo_model.pt\")\n        self.depth_estimator = MonocularDepthEstimator()\n        self.tactile_threshold = 0.5\n\n    def fuse_modalities(self, rgb_image, tactile_data=None, force_data=None):\n        \"\"\"\n        Fuse visual and tactile/force information\n        \"\"\"\n        # Process visual information\n        objects = self.vision_processor.detect_objects(rgb_image)\n        depth_map = self.depth_estimator.estimate_depth(rgb_image)\n\n        fused_percepts = []\n\n        for obj in objects:\n            # Get depth at object center\n            center_x, center_y = int(obj['center'][0]), int(obj['center'][1])\n            depth = depth_map[center_y, center_x] if (0 <= center_y < depth_map.shape[0] and\n                                                    0 <= center_x < depth_map.shape[1]) else None\n\n            # Incorporate tactile information if available\n            tactile_feedback = None\n            if tactile_data is not None:\n                # Check if object location corresponds to tactile contact\n                tactile_feedback = self.process_tactile_data(tactile_data, obj['bbox'])\n\n            fused_percept = {\n                'class': obj['class'],\n                'confidence': obj['confidence'],\n                'bbox': obj['bbox'],\n                'center': obj['center'],\n                'depth': depth,\n                'tactile': tactile_feedback,\n                'fusion_confidence': self.compute_fusion_confidence(obj, depth, tactile_feedback)\n            }\n\n            fused_percepts.append(fused_percept)\n\n        return fused_percepts\n\n    def process_tactile_data(self, tactile_data, bbox):\n        \"\"\"\n        Process tactile sensor data in context of visual detection\n        \"\"\"\n        # Implementation depends on tactile sensor type and layout\n        # This is a simplified example\n        if tactile_data.get('contact', False):\n            return {\n                'contact': True,\n                'force': tactile_data.get('force', 0),\n                'location': tactile_data.get('location', 'unknown')\n            }\n        return None\n\n    def compute_fusion_confidence(self, vision_percept, depth, tactile):\n        \"\"\"\n        Compute confidence in fused percept\n        \"\"\"\n        confidence = vision_percept['confidence']\n\n        # Boost confidence if depth is reasonable\n        if depth and 0.1 < depth < 3.0:  # Reasonable depth range\n            confidence *= 1.2\n\n        # Boost confidence if tactile confirms contact\n        if tactile and tactile['contact']:\n            confidence *= 1.3\n\n        return min(confidence, 1.0)  # Cap at 1.0\n")),(0,o.yg)("h2",{id:"real-time-performance-optimization"},"Real-Time Performance Optimization"),(0,o.yg)("h3",{id:"model-optimization-for-edge-deployment"},"Model Optimization for Edge Deployment"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class OptimizedVisionPipeline:\n    def __init__(self, model_path):\n        # Load optimized model (e.g., TensorRT, ONNX Runtime, OpenVINO)\n        import onnxruntime as ort\n\n        self.session = ort.InferenceSession(model_path)\n        self.input_name = self.session.get_inputs()[0].name\n\n    def infer(self, image):\n        """\n        Run optimized inference\n        """\n        # Preprocess image\n        input_tensor = self.preprocess(image)\n\n        # Run inference\n        results = self.session.run(None, {self.input_name: input_tensor})\n\n        return self.postprocess(results)\n\n    def preprocess(self, image):\n        """\n        Preprocess image for optimized model\n        """\n        # Resize and normalize\n        image = cv2.resize(image, (640, 640))\n        image = image.astype(np.float32) / 255.0\n        image = np.transpose(image, (2, 0, 1))  # HWC to CHW\n        image = np.expand_dims(image, axis=0)   # Add batch dimension\n\n        return image\n\n    def postprocess(self, results):\n        """\n        Postprocess model outputs\n        """\n        # Implementation depends on model architecture\n        return results[0]\n')),(0,o.yg)("h2",{id:"safety-considerations"},"Safety Considerations"),(0,o.yg)("h3",{id:"safe-visual-processing"},"Safe Visual Processing"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class SafeVisionProcessor:\n    def __init__(self):\n        self.max_processing_time = 0.05  # 50ms maximum processing time\n        self.confidence_threshold = 0.7\n        self.min_object_size = 50  # Minimum pixels for valid detection\n\n    def process_with_safety(self, image):\n        """\n        Process image with safety checks\n        """\n        import time\n\n        start_time = time.time()\n\n        # Run object detection\n        detections = self.detect_objects(image)\n\n        # Filter by safety criteria\n        safe_detections = []\n        for detection in detections:\n            # Check confidence\n            if detection[\'confidence\'] < self.confidence_threshold:\n                continue\n\n            # Check object size\n            bbox = detection[\'bbox\']\n            obj_size = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n            if obj_size < self.min_object_size:\n                continue\n\n            # Check processing time\n            if time.time() - start_time > self.max_processing_time:\n                print("Warning: Processing time exceeded safety limit")\n                break\n\n            safe_detections.append(detection)\n\n        return safe_detections\n')),(0,o.yg)("h2",{id:"learning-objectives"},"Learning Objectives"),(0,o.yg)("p",null,"After studying this chapter, you should be able to:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Implement various computer vision techniques for physical AI interaction"),(0,o.yg)("li",{parentName:"ol"},"Design vision pipelines that provide actionable information for control"),(0,o.yg)("li",{parentName:"ol"},"Apply visual servoing for precise robot control"),(0,o.yg)("li",{parentName:"ol"},"Fuse visual information with other sensory modalities"),(0,o.yg)("li",{parentName:"ol"},"Optimize vision systems for real-time performance"),(0,o.yg)("li",{parentName:"ol"},"Address safety considerations in vision-based control")),(0,o.yg)("h2",{id:"interactive-vision-demonstration"},"Interactive Vision Demonstration"),(0,o.yg)("p",null,"Try out computer vision concepts with our interactive demonstration:"),(0,o.yg)(a.A,{simulationType:"computer_vision",parameters:{environment:"object_detection",duration:5,speed:1},mdxType:"InteractiveDemo"}),(0,o.yg)("h2",{id:"hands-on-exercise"},"Hands-On Exercise"),(0,o.yg)("p",null,"Implement a vision-based grasping system:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Set up a camera for object detection"),(0,o.yg)("li",{parentName:"ol"},"Implement object detection using YOLO or similar"),(0,o.yg)("li",{parentName:"ol"},"Calculate 3D position of detected objects"),(0,o.yg)("li",{parentName:"ol"},"Plan robot motion to approach and grasp objects"),(0,o.yg)("li",{parentName:"ol"},"Implement visual feedback for grasp verification"),(0,o.yg)("li",{parentName:"ol"},"Test the system with various objects and evaluate performance")),(0,o.yg)("p",null,"This exercise will help you understand the practical challenges of integrating computer vision with robotic manipulation."))}u.isMDXComponent=!0},99098:(e,n,t)=>{t.d(n,{A:()=>p});var i=t(96540);const o="interactiveDemoContainer_Fk3m",a="controls_FW2Z",r="primaryButton_kJWA",s="secondaryButton_HzQ9",l="results_aQNw",c="info_g0cS",m="fallbackContent_igYf",p=({simulationType:e="locomotion",parameters:n={},onSimulationStart:t,onSimulationEnd:p})=>{const[d,u]=i.useState(!1),[f,g]=i.useState(null);return i.createElement("div",{className:o,role:"region","aria-labelledby":"interactive-demo-title"},i.createElement("h3",{id:"interactive-demo-title"},"Interactive Simulation Demo"),i.createElement("p",null,"Type: ",e),i.createElement("noscript",null,i.createElement("div",{className:m},i.createElement("h4",null,"Interactive Simulation Disabled"),i.createElement("p",null,"To see the interactive simulation demo, please enable JavaScript."),i.createElement("p",null,"Simulation type: ",e),i.createElement("p",null,"Parameters: ",JSON.stringify(n,null,2)))),i.createElement("div",{className:a,role:"group","aria-label":"Simulation controls"},i.createElement("button",{onClick:()=>{u(!0),t&&t(),setTimeout(()=>{g({status:"completed",results:`Simulation of type "${e}" completed successfully`,timestamp:(new Date).toISOString()}),u(!1),p&&p()},2e3)},disabled:d,"aria-busy":d,className:r},d?"Running...":"Start Simulation"),i.createElement("button",{onClick:()=>{u(!1),g(null)},className:s},"Reset")),f&&i.createElement("div",{className:l,role:"status","aria-live":"polite"},i.createElement("h4",null,"Results:"),i.createElement("p",null,f.results),i.createElement("p",null,"Time: ",f.timestamp)),!f&&i.createElement("div",{className:c,role:"status","aria-live":"polite"},i.createElement("p",null,'Click "Start Simulation" to run the interactive demo.')))}}}]);