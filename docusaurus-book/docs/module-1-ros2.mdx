---
id: "module-1-ros2"
title: "The Robotic Nervous System (ROS 2)"
sidebar_label: "Module 1: ROS 2"
---

# The Robotic Nervous System (ROS 2)

## Introduction to ROS 2

Robot Operating System 2 (ROS 2) serves as the foundational middleware for modern humanoid robots and AI agents. Unlike its predecessor, ROS 2 provides enhanced reliability, scalability, and real-time capabilities essential for safety-critical applications like humanoid robotics. Think of ROS 2 as the nervous system of your robot - connecting sensors, actuators, and computational nodes in a coordinated fashion.

ROS 2 introduces improved communication protocols, better security features, and support for real-time systems. It enables distributed computing across multiple machines, allowing humanoid robots to process sensor data, execute control algorithms, and coordinate complex behaviors seamlessly.

## ROS 2 Nodes, Topics, and Services

### Nodes
Nodes are the fundamental building blocks of ROS 2 applications. Each node typically handles a specific task - such as sensor processing, motion planning, or actuator control. In humanoid robots, you might have nodes for:
- Joint controller nodes
- Sensor processing nodes (IMU, cameras, LiDAR)
- Perception nodes (object detection, SLAM)
- Behavior nodes (walking, grasping, speech)

### Topics
Topics enable asynchronous communication between nodes through a publish-subscribe model. This decoupled architecture allows nodes to operate independently while sharing information. Common topics in humanoid robots include:
- `/joint_states`: Current positions, velocities, and efforts of all joints
- `/sensor_msgs/LaserScan`: LiDAR data for navigation
- `/image_raw`: Camera feeds for computer vision
- `/cmd_vel`: Velocity commands for locomotion

### Services
Services provide synchronous request-response communication for operations that require confirmation or return specific results. Examples in humanoid robotics:
- `/get_joint_state`: Request current joint positions
- `/set_trajectory`: Send trajectory commands to controllers
- `/save_map`: Save a map in SLAM operations

## Bridging Python Agents to ROS Controllers using rclpy

Python agents often serve as high-level decision makers in robotic systems, while ROS controllers handle low-level actuator commands. The `rclpy` library enables seamless integration between Python-based AI agents and ROS 2 systems.

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import JointState
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint

class AIAgentNode(Node):
    def __init__(self):
        super().__init__("ai_agent_node")

        # Subscribe to sensor data
        self.sensor_subscription = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_state_callback,
            10)

        # Publisher for high-level commands
        self.command_publisher = self.create_publisher(
            String,
            '/ai_commands',
            10)

        # Publisher for trajectory commands
        self.trajectory_publisher = self.create_publisher(
            JointTrajectory,
            '/joint_trajectory_controller/joint_trajectory',
            10)

    def joint_state_callback(self, msg):
        # Process joint states and make AI decisions
        self.get_logger().info(f"Received joint states: {len(msg.position)} joints")

    def send_trajectory_command(self, joint_names, positions, velocities=None):
        traj_msg = JointTrajectory()
        traj_msg.joint_names = joint_names

        point = JointTrajectoryPoint()
        point.positions = positions
        if velocities:
            point.velocities = velocities

        # Set execution time
        point.time_from_start.sec = 2
        point.time_from_start.nanosec = 0

        traj_msg.points.append(point)
        self.trajectory_publisher.publish(traj_msg)
```

This bridge allows sophisticated Python-based AI algorithms to interact with real-time ROS 2 controllers, enabling complex behaviors in humanoid robots.

## Understanding URDF for Humanoid Robots

Unified Robot Description Format (URDF) defines the physical and kinematic properties of robots. For humanoid robots, URDF describes:
- Link geometry and mass properties
- Joint types and limits
- Kinematic chains (legs, arms, torso, head)
- Sensor placements
- Collision models

```xml
<?xml version="1.0"?>
<robot name="humanoid_robot">
  <!-- Base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.3 0.2 0.1"/>
      </geometry>
    </visual>
    <collision>
      <geometry>
        <box size="0.3 0.2 0.1"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="5.0"/>
      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>
    </inertial>
  </link>

  <!-- Hip joint and link -->
  <joint name="hip_joint" type="revolute">
    <parent link="base_link"/>
    <child link="left_leg"/>
    <axis xyz="0 0 1"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1.0"/>
  </joint>

  <link name="left_leg">
    <visual>
      <geometry>
        <capsule length="0.5" radius="0.05"/>
      </geometry>
    </visual>
    <collision>
      <geometry>
        <capsule length="0.5" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="2.0"/>
      <inertia ixx="0.05" ixy="0.0" ixz="0.0" iyy="0.05" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>
</robot>
```

URDF enables simulation environments, kinematic solvers, and motion planning algorithms to understand the robot's physical structure.

## ROS 2 Humanoid Architecture Overview

A typical ROS 2 architecture for humanoid robots includes multiple layers:

### Hardware Abstraction Layer
- Real-time controllers for joint servos
- Sensor drivers for IMUs, cameras, LiDAR
- Actuator interfaces

### Perception Layer
- Computer vision nodes
- SLAM and localization
- Object recognition
- Environment mapping

### Planning Layer
- Motion planners
- Trajectory generators
- Path planning algorithms
- Balance controllers

### Behavior Layer
- High-level AI agents
- State machines
- Task planners
- Learning algorithms

<figure>
  <img src="/img/ros2-humanoid-architecture.png" alt="ROS 2 Humanoid Architecture Diagram" />
  <figcaption>Diagram showing the layered architecture of ROS 2 for humanoid robots</figcaption>
</figure>

This architecture enables modular development and easy integration of new capabilities while maintaining system stability and performance.